{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E1 Test & Evaluation\n",
    "\n",
    "**Purpose:** This notebook is for conducting E1 Experiment and processing the experiment's results.\n",
    "\n",
    "This notebook: \n",
    "\n",
    "1. Downloads dataset (Roboflow)\n",
    "2. Downloads COCO pre-trained models (YOLOv8m and RT-DETER-L)\n",
    "2. Builds evaluation indices\n",
    "3. Conducting the experiment\n",
    "4. Generates predictions on test set\n",
    "5. Runs all 3 evaluation metrics\n",
    "6. Generates plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Clone Repo to Colab /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if in Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab\")\n",
    "    # Clone repo if not already cloned\n",
    "    import os\n",
    "    if not os.path.exists('Deep_Learning_Gil_Alon'):\n",
    "        !git clone https://github.com/gil-attar/Deep_Learning_Project_Gil_Alon.git Deep_Learning_Gil_Alon\n",
    "    %cd Deep_Learning_Gil_Alon\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    # Navigate to project root if in notebooks/\n",
    "    if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "        os.chdir('..')\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Mounting Google Drive & Setting up Folder Structure for Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for saving run results\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_NAME = \"Deep_Learning_Project_Gil_Alon\"\n",
    "\n",
    "# IMPORTANT:\n",
    "# - For a fresh folder each time, keep the timestamp.\n",
    "# - To continue the SAME sweep after a disconnect, set RUN_ID to a fixed string\n",
    "#   (e.g., RUN_ID=\"E1_fullsweep_50ep_v1\") and reuse it after reconnect.\n",
    "RUN_ID = time.strftime(\"E1_%Y%m%d_%H%M%S\")\n",
    "\n",
    "DRIVE_ROOT = Path(\"/content/drive/MyDrive/Colab_Outputs\") / PROJECT_NAME / RUN_ID\n",
    "\n",
    "PERSIST_E1_RUNS = DRIVE_ROOT / \"E1_runs\"       # where Experiment 1 outputs will live\n",
    "PERSIST_WEIGHTS = DRIVE_ROOT / \"pretrained\"    # optional cache for pretrained weights\n",
    "\n",
    "PERSIST_E1_RUNS.mkdir(parents=True, exist_ok=True)\n",
    "PERSIST_WEIGHTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Drive root:\", DRIVE_ROOT)\n",
    "print(\"E1 runs:\", PERSIST_E1_RUNS)\n",
    "print(\"Weights cache:\", PERSIST_WEIGHTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Symlink the repo’s runs/ to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "REPO = Path.cwd()  \n",
    "print(\"REPO =\", REPO)\n",
    "\n",
    "E1_RUNS_IN_REPO = REPO / \"experiments\" / \"Experiment_1\" / \"runs\"\n",
    "\n",
    "# Safety check\n",
    "assert REPO.exists(), f\"Repo path does not exist: {REPO}\"\n",
    "\n",
    "# Remove local runs dir if it exists, then link to Drive\n",
    "!rm -rf \"{E1_RUNS_IN_REPO}\"\n",
    "!ln -s \"{PERSIST_E1_RUNS}\" \"{E1_RUNS_IN_REPO}\"\n",
    "\n",
    "print(\"Symlink created:\")\n",
    "!ls -la \"{E1_RUNS_IN_REPO}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.4 Re-routing Model Weights to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_IN_REPO = REPO / \"artifacts\" / \"weights\"\n",
    "\n",
    "!rm -rf \"{WEIGHTS_IN_REPO}\"\n",
    "!ln -s \"{PERSIST_WEIGHTS}\" \"{WEIGHTS_IN_REPO}\"\n",
    "\n",
    "print(\"Weights dir now points to Drive:\")\n",
    "!ls -la \"{WEIGHTS_IN_REPO}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q ultralytics roboflow pyyaml pillow numpy matplotlib pandas tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Roboflow API key\n",
    "import os\n",
    "os.environ[\"ROBOFLOW_API_KEY\"] = \"zEF9icmDY2oTcPkaDcQY\"  # Your API key\n",
    "\n",
    "# Download dataset\n",
    "!python scripts/download_dataset.py --output_dir data/raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset downloaded\n",
    "!echo \"Train images: $(ls data/raw/train/images/ 2>/dev/null | wc -l)\"\n",
    "!echo \"Valid images: $(ls data/raw/valid/images/ 2>/dev/null | wc -l)\"\n",
    "!echo \"Test images: $(ls data/raw/test/images/ 2>/dev/null | wc -l)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fetch COCO-Pretrained Weights (YOLOv8m + RT-DETR-L)\n",
    "\n",
    "Will be stored under `artifacts/weights/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch pretrained weights (idempotent)\n",
    "!bash scripts/fetch_weights.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Evaluation Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train/val/test indices (with ACTUAL image dimensions - this is critical!)\n",
    "# Remove old indices first to ensure fresh rebuild\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "if Path(\"data/processed/evaluation\").exists():\n",
    "    shutil.rmtree(\"data/processed/evaluation\")\n",
    "    print(\"✓ Removed old indices\")\n",
    "\n",
    "!python scripts/build_evaluation_indices.py \\\n",
    "    --dataset_root data/raw \\\n",
    "    --output_dir data/processed/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify indices created\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "test_index_path = \"data/processed/evaluation/test_index.json\"\n",
    "\n",
    "if Path(test_index_path).exists():\n",
    "    with open(test_index_path) as f:\n",
    "        test_data = json.load(f)\n",
    "    print(f\"✓ Test index: {test_data['metadata']['num_images']} images\")\n",
    "    print(f\"  Total objects: {test_data['metadata']['total_objects']}\")\n",
    "    print(f\"  Classes: {test_data['metadata']['num_classes']}\")\n",
    "else:\n",
    "    print(f\"❌ Test index not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Create data.yaml for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data.yaml with absolute paths for Colab\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Get absolute path to dataset\n",
    "dataset_root = Path('data/raw').resolve()\n",
    "\n",
    "# Read original data.yaml to get class names\n",
    "with open(dataset_root / 'data.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Create config with ABSOLUTE paths\n",
    "train_config = {\n",
    "    'path': str(dataset_root),  # Absolute base path\n",
    "    'train': 'train/images',\n",
    "    'val': 'valid/images', \n",
    "    'test': 'test/images',\n",
    "    'names': config['names'],\n",
    "    'nc': len(config['names'])\n",
    "}\n",
    "\n",
    "# Save to data/processed/\n",
    "output_path = Path('data/processed/data.yaml')\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    yaml.dump(train_config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"✓ Created data.yaml with absolute paths\")\n",
    "print(f\"  Path: {train_config['path']}\")\n",
    "print(f\"  Classes: {train_config['nc']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Experiment 1 (Freeze Ladder: YOLOv8m vs RT-DETR-L)\n",
    "\n",
    "This section runs the full E1 sweep using the experiments runner scripts. \n",
    "then, per run train, export predictions, and run the custom evaluator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E1 sweep configuration\n",
    "DRY_RUN = True   # True for quick testing: 1 epoch for each of the 8 runs. False for the real sweep.\n",
    "EPOCHS  = 1 if DRY_RUN else 50\n",
    "IMGSZ   = 640\n",
    "SEED    = 42\n",
    "\n",
    "print(f\"DRY_RUN={DRY_RUN} | EPOCHS={EPOCHS} | IMGSZ={IMGSZ} | SEED={SEED}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full E1 matrix (8 runs)\n",
    "# This calls experiments/Experiment_1/runOneTest.py for each (model, freeze_id).\n",
    "!EPOCHS={EPOCHS} IMGSZ={IMGSZ} SEED={SEED} bash experiments/Experiment_1/run_experiment1.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Run Outputs\n",
    "\n",
    "Sanity checks: verify that each run directory contains manifests, predictions, and evaluation outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "runs_root = Path(\"experiments/Experiment_1/runs\")\n",
    "assert runs_root.exists(), f\"Missing runs root: {runs_root}\"\n",
    "\n",
    "expected_files = [\n",
    "    \"run_manifest.json\",\n",
    "    \"train_summary.json\",\n",
    "    \"predictions/val_predictions.json\",\n",
    "    \"predictions/test_predictions.json\",\n",
    "    \"eval/val/metrics.json\",\n",
    "    \"eval/test/metrics.json\",\n",
    "]\n",
    "\n",
    "missing = []\n",
    "run_dirs = sorted([p for p in runs_root.glob(\"**/F[0-3]\") if p.is_dir()])\n",
    "print(f\"Found {len(run_dirs)} run directories\")\n",
    "for rd in run_dirs:\n",
    "    for ef in expected_files:\n",
    "        if not (rd / ef).exists():\n",
    "            missing.append((str(rd), ef))\n",
    "\n",
    "if missing:\n",
    "    print(\"WARNING: missing expected artifacts in some runs:\")\n",
    "    for rd, ef in missing[:40]:\n",
    "        print(f\"  - {rd} :: {ef}\")\n",
    "else:\n",
    "    print(\"✓ All runs contain the expected artifacts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Aggregate Metrics Across Runs\n",
    "\n",
    "Build a single table across the 8 runs, reading `run_manifest.json`, `eval/test/metrics.json`, and the predictions timing metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS CODE CELL IS AFTER THE EDIT ###\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def find_results_csv(run_dir: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Ultralytics typically writes: <run_dir>/results.csv\n",
    "    We also support fallback searches.\n",
    "    \"\"\"\n",
    "    direct = run_dir / \"results.csv\"\n",
    "    if direct.exists():\n",
    "        return direct\n",
    "\n",
    "    # fallback: sometimes results*.csv exists\n",
    "    cands = list(run_dir.glob(\"results*.csv\")) + list(run_dir.rglob(\"results*.csv\"))\n",
    "    cands = [p for p in cands if p.is_file()]\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"No results.csv found under: {run_dir}\")\n",
    "\n",
    "    # pick the shortest path (usually the main one), then newest\n",
    "    cands.sort(key=lambda p: (len(str(p)), -p.stat().st_mtime))\n",
    "    return cands[0]\n",
    "\n",
    "def load_epoch_log(run_dir: Path) -> pd.DataFrame:\n",
    "    csv_path = find_results_csv(run_dir)\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Normalize column names (strip whitespace)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    # Ensure epoch exists\n",
    "    if \"epoch\" not in df.columns:\n",
    "        # sometimes epoch is implicit index\n",
    "        df.insert(0, \"epoch\", np.arange(len(df)))\n",
    "\n",
    "    return df\n",
    "\n",
    "def pick_first_existing_col(df: pd.DataFrame, candidates: list[str]) -> str | None:\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def get_map50_col(df: pd.DataFrame) -> str | None:\n",
    "    # Common Ultralytics naming variants\n",
    "    candidates = [\n",
    "        \"metrics/mAP50(B)\", \"metrics/mAP50\", \"metrics/mAP_0.5\",\n",
    "        \"metrics/mAP50-95(B)\",  # not map50, but used as fallback\n",
    "        \"metrics/mAP50(M)\",     # if masks exist (unlikely here)\n",
    "    ]\n",
    "    c = pick_first_existing_col(df, candidates)\n",
    "    return c\n",
    "\n",
    "def get_lr_cols(df: pd.DataFrame) -> list[str]:\n",
    "    # Ultralytics often logs lr/pg0, lr/pg1, lr/pg2\n",
    "    return [c for c in df.columns if c.startswith(\"lr/\")]\n",
    "\n",
    "def get_train_loss_cols(df: pd.DataFrame) -> list[str]:\n",
    "    return [c for c in df.columns if c.startswith(\"train/\") and \"loss\" in c]\n",
    "\n",
    "def get_val_loss_cols(df: pd.DataFrame) -> list[str]:\n",
    "    return [c for c in df.columns if c.startswith(\"val/\") and \"loss\" in c]\n",
    "\n",
    "# Load per-epoch logs for all discovered runs\n",
    "epoch_logs = {}  # key: (model, freeze_id) -> DataFrame\n",
    "for rd in run_dirs:\n",
    "    model = rd.parts[-2]\n",
    "    freeze_id = rd.parts[-1]\n",
    "    try:\n",
    "        epoch_logs[(model, freeze_id)] = load_epoch_log(rd)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not load epoch log for {model}/{freeze_id}: {e}\")\n",
    "\n",
    "print(f\"Loaded epoch logs: {len(epoch_logs)} / {len(run_dirs)} runs\")\n",
    "list(epoch_logs.keys())[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _safe_get(d, keys, default=None):\n",
    "    cur = d\n",
    "    for k in keys:\n",
    "        if not isinstance(cur, dict) or k not in cur:\n",
    "            return default\n",
    "        cur = cur[k]\n",
    "    return cur\n",
    "\n",
    "def auc_trapz(x, y):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    if len(x) < 2:\n",
    "        return np.nan\n",
    "    return float(np.trapz(y, x))\n",
    "\n",
    "rows = []\n",
    "for rd in run_dirs:\n",
    "    parts = rd.parts\n",
    "    model = parts[-2]\n",
    "    freeze_id = parts[-1]\n",
    "\n",
    "    manifest = json.loads((rd / \"run_manifest.json\").read_text())\n",
    "    test_metrics = json.loads((rd / \"eval/test/metrics.json\").read_text())\n",
    "    preds_test = json.loads((rd / \"predictions/test_predictions.json\").read_text())\n",
    "\n",
    "    # Per-epoch log (Ultralytics)\n",
    "    ep = epoch_logs.get((model, freeze_id))\n",
    "    map50_col = None\n",
    "    best_val_map50 = np.nan\n",
    "    best_epoch = np.nan\n",
    "    epoch_to_90pct = np.nan\n",
    "    auc_map50 = np.nan\n",
    "\n",
    "    train_loss_cols, val_loss_cols = [], []\n",
    "    best_train_loss = np.nan\n",
    "    best_val_loss = np.nan\n",
    "    gen_gap_at_best = np.nan\n",
    "\n",
    "    if ep is not None and len(ep) > 0:\n",
    "        map50_col = get_map50_col(ep)\n",
    "        if map50_col is not None:\n",
    "            y = ep[map50_col].astype(float).to_numpy()\n",
    "            x = ep[\"epoch\"].astype(float).to_numpy()\n",
    "\n",
    "            best_epoch = int(ep.loc[np.nanargmax(y), \"epoch\"])\n",
    "            best_val_map50 = float(np.nanmax(y))\n",
    "            auc_map50 = auc_trapz(x, y)\n",
    "\n",
    "            target = 0.9 * best_val_map50\n",
    "            idxs = np.where(y >= target)[0]\n",
    "            if len(idxs) > 0:\n",
    "                epoch_to_90pct = int(ep.loc[idxs[0], \"epoch\"])\n",
    "\n",
    "        # Loss summaries + generalization gap (if val losses exist)\n",
    "        train_loss_cols = get_train_loss_cols(ep)\n",
    "        val_loss_cols = get_val_loss_cols(ep)\n",
    "\n",
    "        if train_loss_cols:\n",
    "            ep[\"_train_total_loss\"] = ep[train_loss_cols].sum(axis=1)\n",
    "        if val_loss_cols:\n",
    "            ep[\"_val_total_loss\"] = ep[val_loss_cols].sum(axis=1)\n",
    "\n",
    "        if (train_loss_cols and val_loss_cols and not math.isnan(best_epoch)):\n",
    "            # pick the row at best_epoch if exists, else closest\n",
    "            row = ep[ep[\"epoch\"] == best_epoch]\n",
    "            if len(row) == 0:\n",
    "                row = ep.iloc[[int(np.nanargmax(ep[map50_col].astype(float).to_numpy()))]]\n",
    "            best_train_loss = float(row[\"_train_total_loss\"].iloc[0])\n",
    "            best_val_loss = float(row[\"_val_total_loss\"].iloc[0])\n",
    "            gen_gap_at_best = float(best_val_loss - best_train_loss)\n",
    "\n",
    "    # Evaluator primary metric (kept; mostly for final test reporting)\n",
    "    # (Your extract_primary_metric is fine; reuse it)\n",
    "    def extract_primary_metric(metrics_json: dict):\n",
    "        candidates = [\n",
    "            (\"overall_f1\", [\"overall\", \"f1\"]),\n",
    "            (\"overall_precision\", [\"overall\", \"precision\"]),\n",
    "            (\"overall_recall\", [\"overall\", \"recall\"]),\n",
    "            (\"f1\", [\"f1\"]),\n",
    "            (\"precision\", [\"precision\"]),\n",
    "            (\"recall\", [\"recall\"]),\n",
    "            (\"map50\", [\"map50\"]),\n",
    "            (\"map\", [\"map\"]),\n",
    "        ]\n",
    "        for name, path in candidates:\n",
    "            val = _safe_get(metrics_json, path)\n",
    "            if isinstance(val, (int, float)) and not (isinstance(val, float) and math.isnan(val)):\n",
    "                return name, float(val)\n",
    "        for k, v in metrics_json.items():\n",
    "            if isinstance(v, (int, float)):\n",
    "                return str(k), float(v)\n",
    "        return None, None\n",
    "\n",
    "    primary_metric_name, primary_metric_val = extract_primary_metric(test_metrics)\n",
    "\n",
    "    rows.append({\n",
    "        \"model\": model,\n",
    "        \"freeze_id\": freeze_id,\n",
    "\n",
    "        \"trainable_params\": _safe_get(manifest, [\"param_counts\", \"trainable_params\"]),\n",
    "        \"total_params\": _safe_get(manifest, [\"param_counts\", \"total_params\"]),\n",
    "\n",
    "        # Per-epoch (Ultralytics)\n",
    "        \"map50_col\": map50_col,\n",
    "        \"best_val_map50\": best_val_map50,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"epoch_to_90pct_best\": epoch_to_90pct,\n",
    "        \"auc_val_map50\": auc_map50,\n",
    "\n",
    "        \"best_train_loss_at_best_epoch\": best_train_loss,\n",
    "        \"best_val_loss_at_best_epoch\": best_val_loss,\n",
    "        \"gen_gap_val_minus_train_at_best\": gen_gap_at_best,\n",
    "\n",
    "        # Evaluator final (test-side; depends on your evaluation contract)\n",
    "        \"primary_metric_name\": primary_metric_name,\n",
    "        \"primary_metric_test\": primary_metric_val,\n",
    "\n",
    "        \"avg_inference_time_ms\": _safe_get(preds_test, [\"inference_time_ms\", \"avg_inference_time_ms\"]),\n",
    "        \"num_images\": _safe_get(preds_test, [\"inference_time_ms\", \"num_images\"]),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values([\"model\", \"freeze_id\"])\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plots\n",
    "\n",
    "Key plots for E1:\n",
    "1. Best Validation mAP@0.5 vs. Trainable Parameters (Freeze Ladder Curve)\n",
    "2. Speed–Accuracy Tradeoff: mAP@0.5 vs. Avg Inference Time (ms/image)\n",
    "3. Per-Model Training Loss vs. Epoch (F0–F3 Overlay)\n",
    "4. Per-Model Validation Loss vs. Epoch (F0–F3 Overlay)\n",
    "5. Per-Model Validation mAP@0.5 vs. Epoch (F0–F3 Overlay)\n",
    "6. Cross-Model Validation mAP@0.5 vs. Epoch (YOLO vs RT-DETR, per Freeze Regime)\n",
    "7. Generalization Gap vs. Freeze Regime (Val Loss − Train Loss at Best Epoch)\n",
    "8. Convergence Speed vs. Freeze Regime (Epoch to 90% of Best Val mAP@0.5)\n",
    "9. Learning Rate Schedule vs. Epoch (Representative Runs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "PLOTS_DIR = Path(\"experiments/Experiment_1/runs\") / \"_plots\"\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def savefig(name: str):\n",
    "    png = PLOTS_DIR / f\"{name}.png\"\n",
    "    pdf = PLOTS_DIR / f\"{name}.pdf\"\n",
    "    plt.savefig(png, bbox_inches=\"tight\", dpi=200)\n",
    "    plt.savefig(pdf, bbox_inches=\"tight\")\n",
    "    print(f\"Saved: {png} and {pdf}\")\n",
    "\n",
    "def get_run_df(model: str, freeze_id: str):\n",
    "    return epoch_logs.get((model, freeze_id))\n",
    "\n",
    "def plot_loss_overlays_for_model(model: str):\n",
    "    # Plot (1): loss curves vs epoch, overlay F0-F3\n",
    "    freezes = sorted([f for (m, f) in epoch_logs.keys() if m == model])\n",
    "    if not freezes:\n",
    "        print(f\"[SKIP] No epoch logs for model={model}\")\n",
    "        return\n",
    "\n",
    "    # Total train loss\n",
    "    plt.figure(figsize=(8,4))\n",
    "    for fz in freezes:\n",
    "        ep = get_run_df(model, fz).copy()\n",
    "        train_cols = get_train_loss_cols(ep)\n",
    "        if not train_cols:\n",
    "            continue\n",
    "        ep[\"_train_total_loss\"] = ep[train_cols].sum(axis=1)\n",
    "        plt.plot(ep[\"epoch\"], ep[\"_train_total_loss\"], label=fz)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Total Train Loss (sum of train/*loss cols)\")\n",
    "    plt.title(f\"{model}: Train Loss vs Epoch (F0–F3)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    savefig(f\"{model}_train_loss_overlay\")\n",
    "    plt.show()\n",
    "\n",
    "    # Total val loss (if exists)\n",
    "    plt.figure(figsize=(8,4))\n",
    "    any_val = False\n",
    "    for fz in freezes:\n",
    "        ep = get_run_df(model, fz).copy()\n",
    "        val_cols = get_val_loss_cols(ep)\n",
    "        if not val_cols:\n",
    "            continue\n",
    "        any_val = True\n",
    "        ep[\"_val_total_loss\"] = ep[val_cols].sum(axis=1)\n",
    "        plt.plot(ep[\"epoch\"], ep[\"_val_total_loss\"], label=fz)\n",
    "    if any_val:\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Total Val Loss (sum of val/*loss cols)\")\n",
    "        plt.title(f\"{model}: Val Loss vs Epoch (F0–F3)\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        savefig(f\"{model}_val_loss_overlay\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "        print(f\"[INFO] No val/*loss columns logged for model={model}.\")\n",
    "\n",
    "def plot_map_overlays_for_model(model: str):\n",
    "    # helper for plots (2) and (6)\n",
    "    freezes = sorted([f for (m, f) in epoch_logs.keys() if m == model])\n",
    "    if not freezes:\n",
    "        return\n",
    "    plt.figure(figsize=(8,4))\n",
    "    any_map = False\n",
    "    for fz in freezes:\n",
    "        ep = get_run_df(model, fz)\n",
    "        map_col = get_map50_col(ep)\n",
    "        if map_col is None:\n",
    "            continue\n",
    "        any_map = True\n",
    "        plt.plot(ep[\"epoch\"], ep[map_col], label=fz)\n",
    "    if any_map:\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Val mAP@0.5 (from results.csv)\")\n",
    "        plt.title(f\"{model}: Val mAP@0.5 vs Epoch (F0–F3)\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        savefig(f\"{model}_map50_overlay\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "        print(f\"[INFO] No mAP column found for model={model}.\")\n",
    "\n",
    "def plot_cross_model_map_per_freeze():\n",
    "    # Plot (2): compare YOLO vs RT-DETR per freeze regime over epochs\n",
    "    models = sorted(set(m for (m, _) in epoch_logs.keys()))\n",
    "    if len(models) < 2:\n",
    "        print(\"[SKIP] Need both models present for cross-model plots.\")\n",
    "        return\n",
    "\n",
    "    freezes = sorted(set(f for (_, f) in epoch_logs.keys()))\n",
    "    for fz in freezes:\n",
    "        plt.figure(figsize=(8,4))\n",
    "        any_curve = False\n",
    "        for model in models:\n",
    "            ep = epoch_logs.get((model, fz))\n",
    "            if ep is None:\n",
    "                continue\n",
    "            map_col = get_map50_col(ep)\n",
    "            if map_col is None:\n",
    "                continue\n",
    "            any_curve = True\n",
    "            plt.plot(ep[\"epoch\"], ep[map_col], label=model)\n",
    "        if any_curve:\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Val mAP@0.5\")\n",
    "            plt.title(f\"Cross-Model: Val mAP@0.5 vs Epoch ({fz})\")\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            savefig(f\"cross_model_map50_{fz}\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "\n",
    "def plot_perf_vs_trainable_params():\n",
    "    # Plot (3): best val mAP vs trainable parameters (log scale)\n",
    "    d = df.copy()\n",
    "    d = d.dropna(subset=[\"trainable_params\", \"best_val_map50\"])\n",
    "    if len(d) == 0:\n",
    "        print(\"[SKIP] Missing trainable_params or best_val_map50.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    for model, g in d.groupby(\"model\"):\n",
    "        g = g.sort_values(\"trainable_params\")\n",
    "        plt.plot(g[\"trainable_params\"], g[\"best_val_map50\"], marker=\"o\", label=model)\n",
    "        for _, r in g.iterrows():\n",
    "            plt.annotate(r[\"freeze_id\"], (r[\"trainable_params\"], r[\"best_val_map50\"]))\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"Trainable parameters (log scale)\")\n",
    "    plt.ylabel(\"Best Val mAP@0.5\")\n",
    "    plt.title(\"E1: Best Val mAP@0.5 vs Trainable Parameters\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    savefig(\"best_map50_vs_trainable_params\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_generalization_gap():\n",
    "    # Plot (4): generalization gap at best epoch (val_loss - train_loss)\n",
    "    d = df.copy()\n",
    "    d = d.dropna(subset=[\"gen_gap_val_minus_train_at_best\"])\n",
    "    if len(d) == 0:\n",
    "        print(\"[SKIP] No gen gap computed (need train/*loss and val/*loss in results.csv).\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    for model, g in d.groupby(\"model\"):\n",
    "        # order F0-F3 nicely\n",
    "        g = g.sort_values(\"freeze_id\")\n",
    "        plt.plot(g[\"freeze_id\"], g[\"gen_gap_val_minus_train_at_best\"], marker=\"o\", label=model)\n",
    "    plt.xlabel(\"Freeze regime\")\n",
    "    plt.ylabel(\"Val Loss - Train Loss (at best-mAP epoch)\")\n",
    "    plt.title(\"E1: Generalization Gap vs Freeze Regime\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    savefig(\"generalization_gap_vs_freeze\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_convergence_speed():\n",
    "    # Plot (5): epoch to reach 90% of best val mAP\n",
    "    d = df.copy()\n",
    "    d = d.dropna(subset=[\"epoch_to_90pct_best\"])\n",
    "    if len(d) == 0:\n",
    "        print(\"[SKIP] No convergence speed computed (need mAP column in results.csv).\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    for model, g in d.groupby(\"model\"):\n",
    "        g = g.sort_values(\"freeze_id\")\n",
    "        plt.plot(g[\"freeze_id\"], g[\"epoch_to_90pct_best\"], marker=\"o\", label=model)\n",
    "    plt.xlabel(\"Freeze regime\")\n",
    "    plt.ylabel(\"Epoch to reach 90% of best Val mAP@0.5\")\n",
    "    plt.title(\"E1: Convergence Speed vs Freeze Regime\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    savefig(\"convergence_speed_vs_freeze\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_lr_and_map_example(model: str, freeze_id: str):\n",
    "    # Plot (6): LR curve + mAP curve (same run) — shown as two figures (cleaner than dual axis)\n",
    "    ep = epoch_logs.get((model, freeze_id))\n",
    "    if ep is None:\n",
    "        print(f\"[SKIP] Missing epoch log for {model}/{freeze_id}\")\n",
    "        return\n",
    "\n",
    "    lr_cols = get_lr_cols(ep)\n",
    "    if not lr_cols:\n",
    "        print(f\"[INFO] No lr/* columns found for {model}/{freeze_id}\")\n",
    "    else:\n",
    "        plt.figure(figsize=(8,4))\n",
    "        for c in lr_cols:\n",
    "            plt.plot(ep[\"epoch\"], ep[c], label=c)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Learning rate\")\n",
    "        plt.title(f\"{model}/{freeze_id}: LR Schedule\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        savefig(f\"{model}_{freeze_id}_lr\")\n",
    "        plt.show()\n",
    "\n",
    "    map_col = get_map50_col(ep)\n",
    "    if map_col is None:\n",
    "        print(f\"[INFO] No mAP column found for {model}/{freeze_id}\")\n",
    "    else:\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.plot(ep[\"epoch\"], ep[map_col], label=map_col)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Val mAP@0.5\")\n",
    "        plt.title(f\"{model}/{freeze_id}: Val mAP@0.5\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        savefig(f\"{model}_{freeze_id}_map50\")\n",
    "        plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Produce plots (1)–(6)\n",
    "# -----------------------------\n",
    "models_present = sorted(set(m for (m, _) in epoch_logs.keys()))\n",
    "\n",
    "# (1) per model loss overlays\n",
    "for m in models_present:\n",
    "    plot_loss_overlays_for_model(m)\n",
    "\n",
    "# (2) cross-model mAP per freeze\n",
    "plot_cross_model_map_per_freeze()\n",
    "\n",
    "# (3) best mAP vs trainable params\n",
    "plot_perf_vs_trainable_params()\n",
    "\n",
    "# (4) generalization gap\n",
    "plot_generalization_gap()\n",
    "\n",
    "# (5) convergence speed\n",
    "plot_convergence_speed()\n",
    "\n",
    "# (6) LR + mAP for one representative run (edit selection if you want)\n",
    "if models_present:\n",
    "    # choose first available run in epoch_logs\n",
    "    m0, f0 = list(epoch_logs.keys())[0]\n",
    "    plot_lr_and_map_example(m0, f0)\n",
    "\n",
    "print(\"All plots saved under:\", PLOTS_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inspect One Run (Optional)\n",
    "\n",
    "Display a few evaluator plots for a selected run directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Choose a run to inspect\n",
    "inspect_run = run_dirs[0] if run_dirs else None\n",
    "print(f\"Inspecting: {inspect_run}\")\n",
    "\n",
    "if inspect_run:\n",
    "    for rel in [\n",
    "        \"eval/test/threshold_sweep.png\",\n",
    "        \"eval/test/per_class_f1.png\",\n",
    "        \"eval/test/confusion_matrix.png\",\n",
    "        \"eval/test/count_mae_comparison.png\",\n",
    "    ]:\n",
    "        p = inspect_run / rel\n",
    "        if p.exists():\n",
    "            print(f\"\\n{rel}:\")\n",
    "            display(Image(filename=str(p)))\n",
    "        else:\n",
    "            print(f\"Missing: {rel}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Stages 1–3 prepare the dataset, build ground-truth indices, and generate `data/processed/data.yaml`.\n",
    "- Stage 4 runs E1 (8 runs) using your experiment runner scripts.\n",
    "- Stages 5–8 verify artifacts and aggregate results into tables and plots suitable for reporting.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
