{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Evaluation System\n",
    "\n",
    "**Purpose:** This notebook is for conducting E1 Experiment and processing the experiment's results.\n",
    "\n",
    "This notebook: \n",
    "\n",
    "1. Downloads dataset (Roboflow)\n",
    "2. Downloads COCO pre-trained models (YOLOv8m and RT-DETER-L)\n",
    "2. Builds evaluation indices\n",
    "3. Conducting the experiment\n",
    "4. Generates predictions on test set\n",
    "5. Runs all 3 evaluation metrics\n",
    "6. Generates plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup & Clone Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if in Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab\")\n",
    "    # Clone repo if not already cloned\n",
    "    import os\n",
    "    if not os.path.exists('Deep_Learning_Gil_Alon'):\n",
    "        !git clone https://github.com/gil-attar/Deep_Learning_Project_Gil_Alon.git Deep_Learning_Gil_Alon\n",
    "    %cd Deep_Learning_Gil_Alon\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    # Navigate to project root if in notebooks/\n",
    "    if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "        os.chdir('..')\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q ultralytics roboflow pyyaml pillow numpy matplotlib pandas tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Roboflow API key\n",
    "import os\n",
    "os.environ[\"ROBOFLOW_API_KEY\"] = \"zEF9icmDY2oTcPkaDcQY\"  # Your API key\n",
    "\n",
    "# Download dataset\n",
    "!python scripts/download_dataset.py --output_dir data/raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset downloaded\n",
    "!echo \"Train images: $(ls data/raw/train/images/ 2>/dev/null | wc -l)\"\n",
    "!echo \"Valid images: $(ls data/raw/valid/images/ 2>/dev/null | wc -l)\"\n",
    "!echo \"Test images: $(ls data/raw/test/images/ 2>/dev/null | wc -l)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fetch COCO-Pretrained Weights (YOLOv8m + RT-DETR-L)\n",
    "\n",
    "Will be stored under `artifacts/weights/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch pretrained weights (idempotent)\n",
    "!bash scripts/fetch_weights.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Evaluation Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train/val/test indices (with ACTUAL image dimensions - this is critical!)\n",
    "# Remove old indices first to ensure fresh rebuild\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "if Path(\"data/processed/evaluation\").exists():\n",
    "    shutil.rmtree(\"data/processed/evaluation\")\n",
    "    print(\"✓ Removed old indices\")\n",
    "\n",
    "!python scripts/build_evaluation_indices.py \\\n",
    "    --dataset_root data/raw \\\n",
    "    --output_dir data/processed/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify indices created\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "test_index_path = \"data/processed/evaluation/test_index.json\"\n",
    "\n",
    "if Path(test_index_path).exists():\n",
    "    with open(test_index_path) as f:\n",
    "        test_data = json.load(f)\n",
    "    print(f\"✓ Test index: {test_data['metadata']['num_images']} images\")\n",
    "    print(f\"  Total objects: {test_data['metadata']['total_objects']}\")\n",
    "    print(f\"  Classes: {test_data['metadata']['num_classes']}\")\n",
    "else:\n",
    "    print(f\"❌ Test index not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Create data.yaml for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data.yaml with absolute paths for Colab\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Get absolute path to dataset\n",
    "dataset_root = Path('data/raw').resolve()\n",
    "\n",
    "# Read original data.yaml to get class names\n",
    "with open(dataset_root / 'data.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Create config with ABSOLUTE paths\n",
    "train_config = {\n",
    "    'path': str(dataset_root),  # Absolute base path\n",
    "    'train': 'train/images',\n",
    "    'val': 'valid/images', \n",
    "    'test': 'test/images',\n",
    "    'names': config['names'],\n",
    "    'nc': len(config['names'])\n",
    "}\n",
    "\n",
    "# Save to data/processed/\n",
    "output_path = Path('data/processed/data.yaml')\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    yaml.dump(train_config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"✓ Created data.yaml with absolute paths\")\n",
    "print(f\"  Path: {train_config['path']}\")\n",
    "print(f\"  Classes: {train_config['nc']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Experiment 1 (Freeze Ladder: YOLOv8m vs RT-DETR-L)\n",
    "\n",
    "This section runs the full E1 sweep using the experiments runner scripts. \n",
    "then, per run train, export predictions, and run the custom evaluator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E1 sweep configuration\n",
    "DRY_RUN = True   # True for quick testing: 1 epoch for each of the 8 runs. False for the real sweep.\n",
    "EPOCHS  = 1 if DRY_RUN else 50\n",
    "IMGSZ   = 640\n",
    "SEED    = 42\n",
    "\n",
    "print(f\"DRY_RUN={DRY_RUN} | EPOCHS={EPOCHS} | IMGSZ={IMGSZ} | SEED={SEED}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full E1 matrix (8 runs)\n",
    "# This calls experiments/Experiment_1/runOneTest.py for each (model, freeze_id).\n",
    "!EPOCHS={EPOCHS} IMGSZ={IMGSZ} SEED={SEED} bash experiments/Experiment_1/run_experiment1.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Run Outputs\n",
    "\n",
    "Sanity checks: verify that each run directory contains manifests, predictions, and evaluation outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "runs_root = Path(\"experiments/Experiment_1/runs\")\n",
    "assert runs_root.exists(), f\"Missing runs root: {runs_root}\"\n",
    "\n",
    "expected_files = [\n",
    "    \"run_manifest.json\",\n",
    "    \"train_summary.json\",\n",
    "    \"predictions/val_predictions.json\",\n",
    "    \"predictions/test_predictions.json\",\n",
    "    \"eval/val/metrics.json\",\n",
    "    \"eval/test/metrics.json\",\n",
    "]\n",
    "\n",
    "missing = []\n",
    "run_dirs = sorted([p for p in runs_root.glob(\"**/F[0-3]\") if p.is_dir()])\n",
    "print(f\"Found {len(run_dirs)} run directories\")\n",
    "for rd in run_dirs:\n",
    "    for ef in expected_files:\n",
    "        if not (rd / ef).exists():\n",
    "            missing.append((str(rd), ef))\n",
    "\n",
    "if missing:\n",
    "    print(\"WARNING: missing expected artifacts in some runs:\")\n",
    "    for rd, ef in missing[:40]:\n",
    "        print(f\"  - {rd} :: {ef}\")\n",
    "else:\n",
    "    print(\"✓ All runs contain the expected artifacts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Aggregate Metrics Across Runs\n",
    "\n",
    "Build a single table across the 8 runs, reading `run_manifest.json`, `eval/test/metrics.json`, and the predictions timing metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def _safe_get(d, keys, default=None):\n",
    "    cur = d\n",
    "    for k in keys:\n",
    "        if not isinstance(cur, dict) or k not in cur:\n",
    "            return default\n",
    "        cur = cur[k]\n",
    "    return cur\n",
    "\n",
    "def extract_primary_metric(metrics_json: dict):\n",
    "    \"\"\"Robust extraction across evaluator versions. Tries common locations/keys.\"\"\"\n",
    "    # Common patterns we might produce (depending on evaluator implementation)\n",
    "    candidates = [\n",
    "        (\"overall_f1\", [\"overall\", \"f1\"]),\n",
    "        (\"overall_precision\", [\"overall\", \"precision\"]),\n",
    "        (\"overall_recall\", [\"overall\", \"recall\"]),\n",
    "        (\"f1\", [\"f1\"]),\n",
    "        (\"precision\", [\"precision\"]),\n",
    "        (\"recall\", [\"recall\"]),\n",
    "        (\"map50\", [\"map50\"]),\n",
    "        (\"map\", [\"map\"]),\n",
    "    ]\n",
    "    for name, path in candidates:\n",
    "        val = _safe_get(metrics_json, path)\n",
    "        if isinstance(val, (int, float)) and not math.isnan(val):\n",
    "            return name, float(val)\n",
    "    # Fallback: search top-level numeric fields\n",
    "    for k,v in metrics_json.items():\n",
    "        if isinstance(v, (int, float)):\n",
    "            return str(k), float(v)\n",
    "    return None, None\n",
    "\n",
    "rows = []\n",
    "for rd in run_dirs:\n",
    "    # Parse run identifiers from path: .../runs/<model>/<F#>\n",
    "    parts = rd.parts\n",
    "    model = parts[-2]\n",
    "    freeze_id = parts[-1]\n",
    "    manifest = json.loads((rd / \"run_manifest.json\").read_text())\n",
    "    test_metrics = json.loads((rd / \"eval/test/metrics.json\").read_text())\n",
    "    preds_test = json.loads((rd / \"predictions/test_predictions.json\").read_text())\n",
    "\n",
    "    metric_name, metric_val = extract_primary_metric(test_metrics)\n",
    "\n",
    "    rows.append({\n",
    "        \"model\": model,\n",
    "        \"freeze_id\": freeze_id,\n",
    "        \"trainable_params\": _safe_get(manifest, [\"trainable_params\"]),\n",
    "        \"total_params\": _safe_get(manifest, [\"total_params\"]),\n",
    "        \"primary_metric_name\": metric_name,\n",
    "        \"primary_metric\": metric_val,\n",
    "        \"avg_inference_time_ms\": _safe_get(preds_test, [\"inference_time_ms\", \"avg_inference_time_ms\"]),\n",
    "        \"num_images\": _safe_get(preds_test, [\"inference_time_ms\", \"num_images\"])\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values([\"model\",\"freeze_id\"])\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plots\n",
    "\n",
    "Key plots for E1:\n",
    "1. Performance vs. trainable parameters (transfer learning / fine-tuning tradeoff).\n",
    "2. Speed–accuracy tradeoff using average inference time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot 1: primary metric vs trainable parameters\n",
    "plt.figure(figsize=(7,4))\n",
    "for model, g in df.groupby(\"model\"):\n",
    "    plt.plot(g[\"trainable_params\"], g[\"primary_metric\"], marker=\"o\", label=model)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Trainable parameters (log scale)\")\n",
    "plt.ylabel(df[\"primary_metric_name\"].dropna().iloc[0] if df[\"primary_metric_name\"].notna().any() else \"primary_metric\")\n",
    "plt.title(\"E1: Performance vs Trainable Parameters\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: speed–accuracy (if timing is available)\n",
    "if df[\"avg_inference_time_ms\"].notna().any():\n",
    "    plt.figure(figsize=(7,4))\n",
    "    for model, g in df.groupby(\"model\"):\n",
    "        plt.scatter(g[\"avg_inference_time_ms\"], g[\"primary_metric\"], label=model)\n",
    "        for _, r in g.iterrows():\n",
    "            plt.annotate(r[\"freeze_id\"], (r[\"avg_inference_time_ms\"], r[\"primary_metric\"]))\n",
    "    plt.xlabel(\"Avg inference time per image (ms)\")\n",
    "    plt.ylabel(df[\"primary_metric_name\"].dropna().iloc[0] if df[\"primary_metric_name\"].notna().any() else \"primary_metric\")\n",
    "    plt.title(\"E1: Speed–Accuracy Tradeoff\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No inference timing found in predictions JSON (avg_inference_time_ms).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inspect One Run (Optional)\n",
    "\n",
    "Display a few evaluator plots for a selected run directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Choose a run to inspect\n",
    "inspect_run = run_dirs[0] if run_dirs else None\n",
    "print(f\"Inspecting: {inspect_run}\")\n",
    "\n",
    "if inspect_run:\n",
    "    for rel in [\n",
    "        \"eval/test/threshold_sweep.png\",\n",
    "        \"eval/test/per_class_f1.png\",\n",
    "        \"eval/test/confusion_matrix.png\",\n",
    "        \"eval/test/count_mae_comparison.png\",\n",
    "    ]:\n",
    "        p = inspect_run / rel\n",
    "        if p.exists():\n",
    "            print(f\"\\n{rel}:\")\n",
    "            display(Image(filename=str(p)))\n",
    "        else:\n",
    "            print(f\"Missing: {rel}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Stages 1–3 prepare the dataset, build ground-truth indices, and generate `data/processed/data.yaml`.\n",
    "- Stage 4 runs E1 (8 runs) using your experiment runner scripts.\n",
    "- Stages 5–8 verify artifacts and aggregate results into tables and plots suitable for reporting.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
