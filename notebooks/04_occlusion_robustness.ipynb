{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Occlusion Robustness Evaluation\n",
    "\n",
    "**Objective:** Compare YOLOv8 vs RT-DETR performance degradation under synthetic occlusions.\n",
    "\n",
    "**Hypothesis:** RT-DETR (Transformer) should degrade more gracefully than YOLOv8 (CNN) as occlusion increases, because global attention can reason about partial objects better than local convolutions.\n",
    "\n",
    "**Experiment Design:**\n",
    "- Fixed weights (no retraining!)\n",
    "- Same 400 test images with different occlusion levels\n",
    "- 4 test sets: 0%, 20%, 40%, 60% occlusion\n",
    "- Same occlusion patterns for both models (seed=42)\n",
    "- Generate 24 JSON files (2 models × 4 levels × 3 files)\n",
    "\n",
    "**Runtime:** ~30-40 minutes total on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Colab\n",
    "import os\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab\")\n",
    "    \n",
    "    # Check if repository already exists\n",
    "    if not os.path.exists('/content/Deep_Learning_Gil_Alon'):\n",
    "        print(\"Cloning repository...\")\n",
    "        # Clone without authentication prompt\n",
    "        !git clone https://github.com/gil-attar/Deep_Learning_Gil_Alon.git 2>&1 | grep -v \"Username\"\n",
    "        \n",
    "        # If clone failed, try alternative\n",
    "        if not os.path.exists('/content/Deep_Learning_Gil_Alon'):\n",
    "            print(\"\\n⚠️ Git clone failed. Using Google Drive instead...\")\n",
    "            print(\"Make sure you've uploaded the project to Google Drive first.\")\n",
    "            from google.colab import drive\n",
    "            drive.mount('/content/drive')\n",
    "            !cp -r /content/drive/MyDrive/Deep_Learning_Project_Gil_Alon /content/Deep_Learning_Gil_Alon 2>/dev/null || cp -r /content/drive/MyDrive/Deep_Learning_Gil_Alon /content/Deep_Learning_Gil_Alon\n",
    "    else:\n",
    "        print(\"✓ Repository already exists\")\n",
    "    \n",
    "    os.chdir('/content/Deep_Learning_Gil_Alon')\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    # Assume we're in notebooks/ directory\n",
    "    if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "        os.chdir('..')\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab only)\n",
    "if IN_COLAB:\n",
    "    !pip install -q ultralytics roboflow pyyaml pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU availability\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"⚠️ WARNING: No GPU detected! Evaluation will be very slow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Dataset (Colab Only)\n",
    "\n",
    "**NOTE:** If running locally, ensure dataset already exists in `data/raw/test/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Roboflow API key here\n",
    "ROBOFLOW_API_KEY = \"zEF9icmDY2oTcPkaDcQY\"  # ← REPLACE THIS!\n",
    "\n",
    "# Dataset version\n",
    "DATASET_VERSION = 1  # Use version 1 (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from roboflow import Roboflow\n",
    "    \n",
    "    # Download dataset using correct workspace and project\n",
    "    rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
    "    project = rf.workspace(\"gaworkspace-utcbg\").project(\"food-ingredients-dataset-2-rewtd\")\n",
    "    \n",
    "    # Download to temporary location first\n",
    "    dataset = project.version(DATASET_VERSION).download(\"yolov8\")\n",
    "    \n",
    "    print(f\"Downloaded to: {dataset.location}\")\n",
    "    \n",
    "    # Move to data/raw using the download_dataset.py logic\n",
    "    import shutil\n",
    "    from pathlib import Path\n",
    "    \n",
    "    src = Path(dataset.location)\n",
    "    dest = Path(\"data/raw\")\n",
    "    dest.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Move train, valid, test folders\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        split_dest = dest / split\n",
    "        if split_dest.exists():\n",
    "            shutil.rmtree(split_dest)\n",
    "        if (src / split).exists():\n",
    "            shutil.move(str(src / split), str(dest / split))\n",
    "            print(f\"  Moved {split}/\")\n",
    "    \n",
    "    # Copy data.yaml\n",
    "    if (src / \"data.yaml\").exists():\n",
    "        shutil.copy(str(src / \"data.yaml\"), str(dest / \"data.yaml\"))\n",
    "        print(\"  Copied data.yaml\")\n",
    "    \n",
    "    # Clean up\n",
    "    shutil.rmtree(src, ignore_errors=True)\n",
    "    \n",
    "    print(f\"\\n✓ Dataset ready in: {dest}\")\n",
    "else:\n",
    "    print(\"⚠️ Running locally - assuming dataset already exists\")\n",
    "    \n",
    "# Verify test set exists\n",
    "from pathlib import Path\n",
    "test_images = list(Path(\"data/raw/test/images\").glob(\"*.jpg\"))\n",
    "print(f\"\\n✓ Found {len(test_images)} test images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Prerequisites\n",
    "\n",
    "Before generating occlusions, ensure:\n",
    "- ✅ Model weights exist (from Step 3.2 training)\n",
    "- ✅ Test index exists (from Step 2)\n",
    "- ✅ Scripts are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Check required files\n",
    "required_files = {\n",
    "    \"YOLOv8 weights\": \"models/yolov8n_baseline.pt\",\n",
    "    \"RT-DETR weights\": \"models/rtdetr_baseline.pt\",\n",
    "    \"Test index\": \"data/processed/evaluation/test_index.json\",\n",
    "    \"Occlusion script\": \"scripts/generate_synthetic_occlusions.py\",\n",
    "    \"Evaluation script\": \"scripts/evaluate_baseline.py\",\n",
    "    \"Data YAML helper\": \"scripts/create_data_yaml.py\"\n",
    "}\n",
    "\n",
    "all_exist = True\n",
    "for name, path in required_files.items():\n",
    "    exists = Path(path).exists()\n",
    "    status = \"✓\" if exists else \"✗\"\n",
    "    print(f\"{status} {name}: {path}\")\n",
    "    if not exists:\n",
    "        all_exist = False\n",
    "\n",
    "if not all_exist:\n",
    "    print(\"\\n❌ Some required files are missing!\")\n",
    "    print(\"\\nPlease ensure:\")\n",
    "    print(\"  1. You've trained models in 02_train_models.ipynb\")\n",
    "    print(\"  2. You've run build_evaluation_index.py (Step 2)\")\n",
    "    print(\"  3. All scripts are present in scripts/ directory\")\n",
    "else:\n",
    "    print(\"\\n✓ All prerequisites satisfied!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Synthetic Occlusion Test Sets\n",
    "\n",
    "Create 4 test sets from the same 400 images:\n",
    "- `level_000/` - Original (0% occlusion) - baseline\n",
    "- `level_020/` - 20% occlusion per bbox\n",
    "- `level_040/` - 40% occlusion per bbox\n",
    "- `level_060/` - 60% occlusion per bbox\n",
    "\n",
    "**Time:** ~2-5 minutes depending on image sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate occluded test sets\n",
    "!python scripts/generate_synthetic_occlusions.py \\\n",
    "    --test_index data/processed/evaluation/test_index.json \\\n",
    "    --images_dir data/raw/test/images \\\n",
    "    --labels_dir data/raw/test/labels \\\n",
    "    --output_dir data/synthetic_occlusion \\\n",
    "    --levels 0.0,0.2,0.4,0.6 \\\n",
    "    --seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify occlusion test sets created\n",
    "occlusion_levels = [0, 20, 40, 60]\n",
    "occlusion_manifest_path = Path(\"data/synthetic_occlusion/occlusion_manifest.json\")\n",
    "\n",
    "if occlusion_manifest_path.exists():\n",
    "    with open(occlusion_manifest_path, 'r') as f:\n",
    "        manifest = json.load(f)\n",
    "    \n",
    "    print(\"✓ Synthetic occlusion test sets created!\\n\")\n",
    "    print(\"Levels generated:\")\n",
    "    for level_name, description in manifest['occlusion_levels'].items():\n",
    "        print(f\"  - {level_name}: {description}\")\n",
    "    \n",
    "    print(\"\\nStatistics:\")\n",
    "    for level_name, stats in manifest['statistics'].items():\n",
    "        print(f\"  {level_name}:\")\n",
    "        print(f\"    - Images: {stats['total_images']}\")\n",
    "        print(f\"    - Boxes occluded: {stats['total_boxes_occluded']}\")\n",
    "else:\n",
    "    print(\"❌ Manifest not found! Occlusion generation may have failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create data.yaml for Each Occlusion Level\n",
    "\n",
    "Each test set needs its own `data.yaml` for Ultralytics validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data.yaml for original test set (0% occlusion)\n",
    "!python scripts/create_data_yaml.py \\\n",
    "    --dataset_root data/raw \\\n",
    "    --output data/synthetic_occlusion/level_000/data.yaml \\\n",
    "    --absolute\n",
    "\n",
    "# Copy test images to level_000 (original, no occlusions)\n",
    "import shutil\n",
    "level_000_dir = Path(\"data/synthetic_occlusion/level_000\")\n",
    "level_000_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy original test images and labels\n",
    "for subdir in ['images', 'labels']:\n",
    "    src = Path(f\"data/raw/test/{subdir}\")\n",
    "    dst = level_000_dir / subdir\n",
    "    if dst.exists():\n",
    "        shutil.rmtree(dst)\n",
    "    shutil.copytree(src, dst)\n",
    "\n",
    "print(\"✓ Created level_000 (original test set)\")\n",
    "\n",
    "# Verify all data.yaml files exist\n",
    "print(\"\\nVerifying data.yaml files:\")\n",
    "for level in [0, 20, 40, 60]:\n",
    "    level_name = f\"level_{level:03d}\"\n",
    "    yaml_path = Path(f\"data/synthetic_occlusion/{level_name}/data.yaml\")\n",
    "    status = \"✓\" if yaml_path.exists() else \"✗\"\n",
    "    print(f\"{status} {level_name}/data.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function for evaluation\n",
    "import json\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO, RTDETR\n",
    "\n",
    "def evaluate_and_save(model_type, model_path, data_yaml, run_id, occlusion_level):\n",
    "    \"\"\"\n",
    "    Evaluate a model and save metrics to JSON.\n",
    "    \n",
    "    Args:\n",
    "        model_type: 'yolo' or 'rtdetr'\n",
    "        model_path: Path to model weights\n",
    "        data_yaml: Path to data.yaml for this test set\n",
    "        run_id: Unique run identifier (e.g., 'e3_yolo_020')\n",
    "        occlusion_level: Occlusion percentage (0.0, 0.2, 0.4, 0.6)\n",
    "    \"\"\"\n",
    "    print(f\"\\nEvaluating {run_id}...\")\n",
    "    \n",
    "    # Load model\n",
    "    if model_type == 'yolo':\n",
    "        model = YOLO(model_path)\n",
    "    else:\n",
    "        model = RTDETR(model_path)\n",
    "    \n",
    "    # Run validation\n",
    "    results = model.val(\n",
    "        data=data_yaml,\n",
    "        imgsz=640,\n",
    "        conf=0.25,\n",
    "        iou=0.50,\n",
    "        device=0,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = Path(\"evaluation/metrics\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Extract metrics\n",
    "    metrics_data = {\n",
    "        \"run_id\": run_id,\n",
    "        \"model\": model_type,\n",
    "        \"occlusion_level\": float(occlusion_level),\n",
    "        \"aggregate_metrics\": {\n",
    "            \"map50\": float(results.results_dict.get('metrics/mAP50(B)', 0)),\n",
    "            \"map50_95\": float(results.results_dict.get('metrics/mAP50-95(B)', 0)),\n",
    "            \"precision\": float(results.results_dict.get('metrics/precision(B)', 0)),\n",
    "            \"recall\": float(results.results_dict.get('metrics/recall(B)', 0)),\n",
    "            \"fps\": float(results.speed.get('inference', 0))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save metrics JSON\n",
    "    with open(output_dir / f\"{run_id}_metrics.json\", 'w') as f:\n",
    "        json.dump(metrics_data, f, indent=2)\n",
    "    \n",
    "    # Create minimal run and predictions JSONs (for compatibility)\n",
    "    run_data = {\n",
    "        \"run_id\": run_id,\n",
    "        \"model\": model_type,\n",
    "        \"occlusion_level\": float(occlusion_level),\n",
    "        \"data_yaml\": str(data_yaml)\n",
    "    }\n",
    "    \n",
    "    with open(output_dir / f\"{run_id}_run.json\", 'w') as f:\n",
    "        json.dump(run_data, f, indent=2)\n",
    "    \n",
    "    predictions_data = {\n",
    "        \"run_id\": run_id,\n",
    "        \"predictions\": []  # Placeholder - full predictions not needed for degradation analysis\n",
    "    }\n",
    "    \n",
    "    with open(output_dir / f\"{run_id}_predictions.json\", 'w') as f:\n",
    "        json.dump(predictions_data, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ {run_id}: mAP@50={metrics_data['aggregate_metrics']['map50']:.3f}, \"\n",
    "          f\"mAP@50-95={metrics_data['aggregate_metrics']['map50_95']:.3f}\")\n",
    "    \n",
    "    return metrics_data\n",
    "\n",
    "print(\"✓ Helper function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate YOLOv8 on all occlusion levels\n",
    "print(\"=\" * 60)\n",
    "print(\"YOLOv8 Evaluations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "yolo_results = {}\n",
    "for level, level_pct in [(0, '000'), (0.2, '020'), (0.4, '040'), (0.6, '060')]:\n",
    "    result = evaluate_and_save(\n",
    "        model_type='yolo',\n",
    "        model_path='models/yolov8n_baseline.pt',\n",
    "        data_yaml=f'data/synthetic_occlusion/level_{level_pct}/data.yaml',\n",
    "        run_id=f'e3_yolo_{level_pct}',\n",
    "        occlusion_level=level\n",
    "    )\n",
    "    yolo_results[level] = result\n",
    "\n",
    "print(f\"\\n✓ Completed all YOLO evaluations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate YOLOv8 on 20% occlusion\n",
    "print(\"=\" * 60)\n",
    "print(\"YOLOv8 Evaluation - 20% Occlusion\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "!python scripts/evaluate_baseline.py \\\n",
    "    --model yolo \\\n",
    "    --yolo_weights models/yolov8n_baseline.pt \\\n",
    "    --data_yaml data/synthetic_occlusion/level_020/data.yaml \\\n",
    "    --test_index data/processed/evaluation/test_index.json \\\n",
    "    --output_dir evaluation/metrics \\\n",
    "    --run_id e3_yolo_020 \\\n",
    "    --device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate YOLOv8 on 40% occlusion\n",
    "print(\"=\" * 60)\n",
    "print(\"YOLOv8 Evaluation - 40% Occlusion\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "!python scripts/evaluate_baseline.py \\\n",
    "    --model yolo \\\n",
    "    --yolo_weights models/yolov8n_baseline.pt \\\n",
    "    --data_yaml data/synthetic_occlusion/level_040/data.yaml \\\n",
    "    --test_index data/processed/evaluation/test_index.json \\\n",
    "    --output_dir evaluation/metrics \\\n",
    "    --run_id e3_yolo_040 \\\n",
    "    --device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate YOLOv8 on 60% occlusion\n",
    "print(\"=\" * 60)\n",
    "print(\"YOLOv8 Evaluation - 60% Occlusion\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "!python scripts/evaluate_baseline.py \\\n",
    "    --model yolo \\\n",
    "    --yolo_weights models/yolov8n_baseline.pt \\\n",
    "    --data_yaml data/synthetic_occlusion/level_060/data.yaml \\\n",
    "    --test_index data/processed/evaluation/test_index.json \\\n",
    "    --output_dir evaluation/metrics \\\n",
    "    --run_id e3_yolo_060 \\\n",
    "    --device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate RT-DETR on All Occlusion Levels\n",
    "\n",
    "Same procedure for RT-DETR.\n",
    "\n",
    "Generates 12 more JSON files:\n",
    "- `rtdetr_000_run.json`, `rtdetr_000_metrics.json`, `rtdetr_000_predictions.json`\n",
    "- `rtdetr_020_run.json`, `rtdetr_020_metrics.json`, `rtdetr_020_predictions.json`\n",
    "- `rtdetr_040_run.json`, `rtdetr_040_metrics.json`, `rtdetr_040_predictions.json`\n",
    "- `rtdetr_060_run.json`, `rtdetr_060_metrics.json`, `rtdetr_060_predictions.json`\n",
    "\n",
    "**Time:** ~10-15 minutes total (RT-DETR is slower than YOLO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate RT-DETR on all occlusion levels\n",
    "print(\"=\" * 60)\n",
    "print(\"RT-DETR Evaluations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "rtdetr_results = {}\n",
    "for level, level_pct in [(0, '000'), (0.2, '020'), (0.4, '040'), (0.6, '060')]:\n",
    "    result = evaluate_and_save(\n",
    "        model_type='rtdetr',\n",
    "        model_path='models/rtdetr_baseline.pt',\n",
    "        data_yaml=f'data/synthetic_occlusion/level_{level_pct}/data.yaml',\n",
    "        run_id=f'e3_rtdetr_{level_pct}',\n",
    "        occlusion_level=level\n",
    "    )\n",
    "    rtdetr_results[level] = result\n",
    "\n",
    "print(f\"\\n✓ Completed all RT-DETR evaluations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate RT-DETR on 20% occlusion\n",
    "print(\"=\" * 60)\n",
    "print(\"RT-DETR Evaluation - 20% Occlusion\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "!python scripts/evaluate_baseline.py \\\n",
    "    --model rtdetr \\\n",
    "    --rtdetr_weights models/rtdetr_baseline.pt \\\n",
    "    --data_yaml data/synthetic_occlusion/level_020/data.yaml \\\n",
    "    --test_index data/processed/evaluation/test_index.json \\\n",
    "    --output_dir evaluation/metrics \\\n",
    "    --run_id e3_rtdetr_020 \\\n",
    "    --device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate RT-DETR on 40% occlusion\n",
    "print(\"=\" * 60)\n",
    "print(\"RT-DETR Evaluation - 40% Occlusion\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "!python scripts/evaluate_baseline.py \\\n",
    "    --model rtdetr \\\n",
    "    --rtdetr_weights models/rtdetr_baseline.pt \\\n",
    "    --data_yaml data/synthetic_occlusion/level_040/data.yaml \\\n",
    "    --test_index data/processed/evaluation/test_index.json \\\n",
    "    --output_dir evaluation/metrics \\\n",
    "    --run_id e3_rtdetr_040 \\\n",
    "    --device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate RT-DETR on 60% occlusion\n",
    "print(\"=\" * 60)\n",
    "print(\"RT-DETR Evaluation - 60% Occlusion\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "!python scripts/evaluate_baseline.py \\\n",
    "    --model rtdetr \\\n",
    "    --rtdetr_weights models/rtdetr_baseline.pt \\\n",
    "    --data_yaml data/synthetic_occlusion/level_060/data.yaml \\\n",
    "    --test_index data/processed/evaluation/test_index.json \\\n",
    "    --output_dir evaluation/metrics \\\n",
    "    --run_id e3_rtdetr_060 \\\n",
    "    --device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Verify All JSON Files Generated\n",
    "\n",
    "Should have 24 JSON files total:\n",
    "- 8 `*_run.json` files\n",
    "- 8 `*_metrics.json` files\n",
    "- 8 `*_predictions.json` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all JSON files exist\n",
    "output_dir = Path(\"evaluation/metrics\")\n",
    "models = ['yolo', 'rtdetr']\n",
    "levels = ['000', '020', '040', '060']\n",
    "file_types = ['run', 'metrics', 'predictions']\n",
    "\n",
    "print(\"Checking generated JSON files:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_files_exist = True\n",
    "total_files = 0\n",
    "\n",
    "for model in models:\n",
    "    print(f\"\\n{model.upper()} Evaluations:\")\n",
    "    for level in levels:\n",
    "        print(f\"\\n  Level {level} ({int(level)}% occlusion):\")\n",
    "        for file_type in file_types:\n",
    "            filename = f\"e3_{model}_{level}_{file_type}.json\"\n",
    "            filepath = output_dir / filename\n",
    "            exists = filepath.exists()\n",
    "            status = \"✓\" if exists else \"✗\"\n",
    "            \n",
    "            # Get file size if exists\n",
    "            size_str = \"\"\n",
    "            if exists:\n",
    "                size_kb = filepath.stat().st_size / 1024\n",
    "                size_str = f\"({size_kb:.1f} KB)\"\n",
    "                total_files += 1\n",
    "            \n",
    "            print(f\"    {status} {filename} {size_str}\")\n",
    "            \n",
    "            if not exists:\n",
    "                all_files_exist = False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\nTotal files created: {total_files} / 24\")\n",
    "\n",
    "if all_files_exist:\n",
    "    print(\"✓ All 24 JSON files successfully generated!\")\n",
    "else:\n",
    "    print(\"\\n❌ Some JSON files are missing!\")\n",
    "    print(\"Check the evaluation outputs above for errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare Performance Across Occlusion Levels\n",
    "\n",
    "Quick preview of the degradation curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load all metrics\n",
    "results = []\n",
    "\n",
    "for model in models:\n",
    "    for level in levels:\n",
    "        metrics_file = output_dir / f\"e3_{model}_{level}_metrics.json\"\n",
    "        if metrics_file.exists():\n",
    "            with open(metrics_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            agg = data['aggregate_metrics']\n",
    "            results.append({\n",
    "                'Model': model.upper(),\n",
    "                'Occlusion': f\"{int(level)}%\",\n",
    "                'mAP@50': round(agg['map50'], 3),\n",
    "                'mAP@50-95': round(agg['map50_95'], 3),\n",
    "                'Precision': round(agg['precision'], 3),\n",
    "                'Recall': round(agg['recall'], 3),\n",
    "                'FPS': round(agg['fps'], 1)\n",
    "            })\n",
    "\n",
    "# Create comparison table\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OCCLUSION ROBUSTNESS COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate degradation percentages\n",
    "print(\"\\nPerformance Degradation (compared to 0% baseline):\\n\")\n",
    "\n",
    "for model in ['YOLO', 'RTDETR']:\n",
    "    model_results = df[df['Model'] == model]\n",
    "    baseline = model_results[model_results['Occlusion'] == '0%']['mAP@50'].values[0]\n",
    "    \n",
    "    print(f\"{model}:\")\n",
    "    for _, row in model_results.iterrows():\n",
    "        if row['Occlusion'] == '0%':\n",
    "            continue\n",
    "        degradation = ((baseline - row['mAP@50']) / baseline) * 100\n",
    "        print(f\"  {row['Occlusion']:>4} occlusion: mAP@50 = {row['mAP@50']:.3f} ({degradation:+.1f}% vs baseline)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Download Results to Google Drive (Colab Only)\n",
    "\n",
    "Save all JSON files for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Create backup directory\n",
    "    backup_dir = Path('/content/drive/MyDrive/Deep_Learning_Occlusion_Results')\n",
    "    backup_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Copy all JSON files\n",
    "    import shutil\n",
    "    for json_file in output_dir.glob('*.json'):\n",
    "        shutil.copy(json_file, backup_dir / json_file.name)\n",
    "    \n",
    "    print(f\"✓ Saved {len(list(output_dir.glob('*.json')))} JSON files to Google Drive\")\n",
    "    print(f\"Location: {backup_dir}\")\n",
    "else:\n",
    "    print(\"Running locally - JSON files already saved to evaluation/occlusion_metrics/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary & Next Steps\n",
    "\n",
    "**What We Just Did:**\n",
    "1. ✅ Generated 4 synthetic occlusion test sets (0%, 20%, 40%, 60%)\n",
    "2. ✅ Evaluated YOLOv8 with fixed weights on all 4 levels\n",
    "3. ✅ Evaluated RT-DETR with fixed weights on all 4 levels\n",
    "4. ✅ Generated 24 JSON files (2 models × 4 levels × 3 file types)\n",
    "\n",
    "**Expected Results:**\n",
    "- Both models should degrade as occlusion increases\n",
    "- **RT-DETR should degrade LESS** than YOLOv8 at higher occlusion levels\n",
    "- This validates the hypothesis: Transformers handle occlusion better\n",
    "\n",
    "**Next Steps:**\n",
    "1. Analyze degradation curves in detail\n",
    "2. Compute per-class robustness (which ingredients are most affected?)\n",
    "3. Visualize predictions on occluded images\n",
    "4. Write up findings for project report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OCCLUSION ROBUSTNESS EVALUATION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nGenerated {total_files} JSON files in: {output_dir}\")\n",
    "print(\"\\nJSON files ready for analysis:\")\n",
    "print(\"  - Run metadata: Reproducibility info\")\n",
    "print(\"  - Metrics: mAP, precision, recall, FPS\")\n",
    "print(\"  - Predictions: Per-image detections for detailed analysis\")\n",
    "print(\"\\nYou can now:\")\n",
    "print(\"  1. Commit JSON files to GitHub\")\n",
    "print(\"  2. Analyze degradation curves\")\n",
    "print(\"  3. Compare CNN vs Transformer robustness\")\n",
    "print(\"  4. Generate visualizations for report\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
