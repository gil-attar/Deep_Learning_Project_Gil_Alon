{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Evaluation System\n",
    "\n",
    "**Purpose:** Quick test of the new evaluation module with minimal training.\n",
    "\n",
    "This notebook:\n",
    "1. Downloads dataset (Roboflow)\n",
    "2. Builds evaluation indices\n",
    "3. Trains a tiny model for 50 epochs (just for testing)\n",
    "4. Generates predictions on test set\n",
    "5. Runs all 3 evaluation metrics\n",
    "6. Generates plots\n",
    "\n",
    "**NOT for actual experiments** - just for debugging the evaluation pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup & Clone Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if in Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab\")\n",
    "    # Clone repo if not already cloned\n",
    "    import os\n",
    "    if not os.path.exists('Deep_Learning_Gil_Alon'):\n",
    "        !git clone https://github.com/gil-attar/Deep_Learning_Project_Gil_Alon.git Deep_Learning_Gil_Alon\n",
    "    %cd Deep_Learning_Gil_Alon\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    # Navigate to project root if in notebooks/\n",
    "    if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "        os.chdir('..')\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q ultralytics roboflow pyyaml pillow numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Roboflow API key\n",
    "import os\n",
    "os.environ[\"ROBOFLOW_API_KEY\"] = \"zEF9icmDY2oTcPkaDcQY\"  # Your API key\n",
    "\n",
    "# Download dataset\n",
    "!python scripts/download_dataset.py --output_dir data/raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset downloaded\n",
    "!echo \"Train images: $(ls data/raw/train/images/ 2>/dev/null | wc -l)\"\n",
    "!echo \"Valid images: $(ls data/raw/valid/images/ 2>/dev/null | wc -l)\"\n",
    "!echo \"Test images: $(ls data/raw/test/images/ 2>/dev/null | wc -l)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Evaluation Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train/val/test indices\n",
    "!python scripts/build_evaluation_indices.py \\\n",
    "    --dataset_root data/raw \\\n",
    "    --output_dir data/processed/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify indices created\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "test_index_path = \"data/processed/evaluation/test_index.json\"\n",
    "\n",
    "if Path(test_index_path).exists():\n",
    "    with open(test_index_path) as f:\n",
    "        test_data = json.load(f)\n",
    "    print(f\"✓ Test index: {test_data['metadata']['num_images']} images\")\n",
    "    print(f\"  Total objects: {test_data['metadata']['total_objects']}\")\n",
    "    print(f\"  Classes: {test_data['metadata']['num_classes']}\")\n",
    "else:\n",
    "    print(f\"❌ Test index not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create data.yaml for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create data.yaml with absolute paths for Colab\nimport yaml\nfrom pathlib import Path\nimport os\n\n# Get absolute path to dataset\ndataset_root = Path('data/raw').resolve()\n\n# Read original data.yaml to get class names\nwith open(dataset_root / 'data.yaml', 'r') as f:\n    config = yaml.safe_load(f)\n\n# Create config with ABSOLUTE paths\ntrain_config = {\n    'path': str(dataset_root),  # Absolute base path\n    'train': 'train/images',\n    'val': 'valid/images', \n    'test': 'test/images',\n    'names': config['names'],\n    'nc': len(config['names'])\n}\n\n# Save to data/processed/\noutput_path = Path('data/processed/data.yaml')\noutput_path.parent.mkdir(parents=True, exist_ok=True)\n\nwith open(output_path, 'w') as f:\n    yaml.dump(train_config, f, default_flow_style=False, sort_keys=False)\n\nprint(f\"✓ Created data.yaml with absolute paths\")\nprint(f\"  Path: {train_config['path']}\")\nprint(f\"  Classes: {train_config['nc']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quick Training (50 epochs - just for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from ultralytics import YOLO\n\n# Load pretrained model\nmodel = YOLO('yolov8n.pt')  # Nano model (starts with COCO pretrained weights)\n\nprint(\"Training for 50 epochs (just for testing)...\")\nresults = model.train(\n    data='data/processed/data.yaml',\n    epochs=50,\n    imgsz=640,\n    batch=-1,  # Auto batch size (like legacy)\n    patience=10,  # Early stopping after 10 epochs without improvement\n    save=True,\n    project='runs/test_eval',\n    name='quick_test',\n    exist_ok=True,\n    pretrained=True,  # Use pretrained weights\n    optimizer='auto',  # Auto optimizer selection\n    verbose=True,\n    seed=42  # Reproducibility\n)\n\nprint(\"\\n✓ Training complete (this model is NOT for actual experiments!)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model - find the weights automatically\n",
    "import glob\n",
    "\n",
    "# Ultralytics adds 'runs/detect/' prefix in Colab\n",
    "possible_paths = [\n",
    "    \"runs/test_eval/quick_test/weights/best.pt\",\n",
    "    \"runs/detect/runs/test_eval/quick_test/weights/best.pt\"\n",
    "]\n",
    "\n",
    "weights_path = None\n",
    "for path in possible_paths:\n",
    "    if Path(path).exists():\n",
    "        weights_path = path\n",
    "        break\n",
    "\n",
    "if not weights_path:\n",
    "    # Search for any best.pt in runs/\n",
    "    found = list(Path(\"runs\").rglob(\"quick_test/weights/best.pt\"))\n",
    "    if found:\n",
    "        weights_path = str(found[0])\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Could not find trained weights!\")\n",
    "\n",
    "model_eval = YOLO(weights_path)\n",
    "print(f\"✓ Loaded model from {weights_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test index\n",
    "with open(test_index_path) as f:\n",
    "    test_index = json.load(f)\n",
    "\n",
    "test_images = test_index['images'][:50]  # Use only first 50 images for quick test\n",
    "print(f\"Running inference on {len(test_images)} test images...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference and collect predictions\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for img_data in tqdm(test_images, desc=\"Inference\"):\n",
    "    image_id = img_data['image_id']\n",
    "    image_filename = img_data['image_filename']\n",
    "    image_path = Path(\"data/raw/test/images\") / image_filename\n",
    "    \n",
    "    if not image_path.exists():\n",
    "        print(f\"Warning: {image_path} not found\")\n",
    "        continue\n",
    "    \n",
    "    # Run inference with LOW confidence threshold (save almost everything)\n",
    "    start = time.time()\n",
    "    results = model_eval.predict(\n",
    "        source=str(image_path),\n",
    "        conf=0.01,  # Very low threshold to save all predictions\n",
    "        imgsz=640,\n",
    "        verbose=False\n",
    "    )[0]\n",
    "    inference_time = time.time() - start\n",
    "    \n",
    "    # Extract detections\n",
    "    detections = []\n",
    "    if len(results.boxes) > 0:\n",
    "        boxes = results.boxes\n",
    "        for i in range(len(boxes)):\n",
    "            detections.append({\n",
    "                \"class_id\": int(boxes.cls[i].item()),\n",
    "                \"class_name\": results.names[int(boxes.cls[i].item())],\n",
    "                \"confidence\": float(boxes.conf[i].item()),\n",
    "                \"bbox\": boxes.xyxy[i].tolist(),\n",
    "                \"bbox_format\": \"xyxy\"\n",
    "            })\n",
    "    \n",
    "    predictions.append({\n",
    "        \"image_id\": image_id,\n",
    "        \"detections\": detections\n",
    "    })\n",
    "\n",
    "print(f\"\\n✓ Generated predictions for {len(predictions)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions in new format\n",
    "pred_output_path = \"evaluation/metrics/test_quick_predictions.json\"\n",
    "Path(pred_output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pred_json = {\n",
    "    \"run_id\": \"test_quick_10epochs\",\n",
    "    \"split\": \"test\",\n",
    "    \"model_family\": \"yolo\",\n",
    "    \"model_name\": \"yolov8n\",\n",
    "    \"inference_settings\": {\n",
    "        \"conf_threshold\": 0.01,\n",
    "        \"iou_threshold\": 0.50,\n",
    "        \"imgsz\": 640\n",
    "    },\n",
    "    \"predictions\": predictions\n",
    "}\n",
    "\n",
    "with open(pred_output_path, 'w') as f:\n",
    "    json.dump(pred_json, f, indent=2)\n",
    "\n",
    "print(f\"✓ Saved predictions to {pred_output_path}\")\n",
    "\n",
    "# DIAGNOSTIC: Check how many detections we got\n",
    "total_detections = sum(len(p['detections']) for p in predictions)\n",
    "images_with_detections = sum(1 for p in predictions if len(p['detections']) > 0)\n",
    "print(f\"\\nDIAGNOSTIC INFO:\")\n",
    "print(f\"  Total predictions across {len(predictions)} images: {total_detections}\")\n",
    "print(f\"  Images with at least 1 detection: {images_with_detections}\")\n",
    "if total_detections > 0:\n",
    "    # Show confidence range\n",
    "    all_confs = [d['confidence'] for p in predictions for d in p['detections']]\n",
    "    print(f\"  Confidence range: {min(all_confs):.3f} - {max(all_confs):.3f}\")\n",
    "    print(f\"  Detections above 0.1: {sum(1 for c in all_confs if c >= 0.1)}\")\n",
    "    print(f\"  Detections above 0.5: {sum(1 for c in all_confs if c >= 0.5)}\")\n",
    "else:\n",
    "    print(\"  ⚠️ WARNING: Model made ZERO detections! Try training longer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Evaluation\n",
    "\n",
    "Test all 3 evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation module\n",
    "from evaluation.io import load_predictions, load_ground_truth, load_class_names\n",
    "from evaluation.metrics import (\n",
    "    eval_detection_prf_at_iou,\n",
    "    eval_per_class_metrics_and_confusions,\n",
    "    eval_counting_quality\n",
    ")\n",
    "from evaluation.plots import plot_all_metrics\n",
    "\n",
    "print(\"✓ Evaluation module imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions and ground truth\n",
    "preds = load_predictions(pred_output_path, split=\"test\")\n",
    "gts = load_ground_truth(test_index_path, split=\"test\")\n",
    "class_names = load_class_names(test_index_path)\n",
    "\n",
    "# Filter GTs to match predictions (first 50 images)\n",
    "pred_image_ids = {p['image_id'] for p in preds}\n",
    "gts = [g for g in gts if g['image_id'] in pred_image_ids]\n",
    "\n",
    "print(f\"✓ Loaded {len(preds)} predictions\")\n",
    "print(f\"✓ Loaded {len(gts)} ground truths\")\n",
    "print(f\"✓ Loaded {len(class_names)} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. P/R/F1 at multiple thresholds\n",
    "print(\"1. Running detection P/R/F1 evaluation...\")\n",
    "threshold_sweep = eval_detection_prf_at_iou(\n",
    "    preds, gts,\n",
    "    iou_threshold=0.5,\n",
    "    conf_thresholds=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    ")\n",
    "\n",
    "print(\"\\nResults by confidence threshold:\")\n",
    "for conf_thr, metrics in threshold_sweep.items():\n",
    "    print(f\"  conf={conf_thr}: P={metrics['precision']:.3f}, R={metrics['recall']:.3f}, F1={metrics['f1']:.3f}\")\n",
    "\n",
    "best_thr = max(threshold_sweep.keys(), key=lambda k: threshold_sweep[k]['f1'])\n",
    "print(f\"\\n✓ Best threshold: {best_thr} (F1={threshold_sweep[best_thr]['f1']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Per-class metrics\n",
    "print(\"\\n2. Running per-class evaluation...\")\n",
    "per_class_results = eval_per_class_metrics_and_confusions(\n",
    "    preds, gts,\n",
    "    iou_threshold=0.5,\n",
    "    conf_threshold=float(best_thr),\n",
    "    class_names=class_names\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Evaluated {len(per_class_results['per_class'])} classes\")\n",
    "print(f\"✓ Found {len(per_class_results['top_confusions'][:5])} top confusions\")\n",
    "\n",
    "# Show top 3 classes by F1\n",
    "sorted_classes = sorted(\n",
    "    per_class_results['per_class'].items(),\n",
    "    key=lambda x: x[1]['f1'],\n",
    "    reverse=True\n",
    ")\n",
    "print(\"\\nTop 3 classes by F1:\")\n",
    "for class_name, metrics in sorted_classes[:3]:\n",
    "    print(f\"  {class_name}: F1={metrics['f1']:.3f} (support={metrics['support']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Counting quality\n",
    "print(\"\\n3. Running counting quality evaluation...\")\n",
    "counting_results = eval_counting_quality(\n",
    "    preds, gts,\n",
    "    iou_threshold=0.5,\n",
    "    conf_threshold=float(best_thr),\n",
    "    class_names=class_names\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Matched-only MAE: {counting_results['matched_only']['global_mae']:.4f}\")\n",
    "print(f\"✓ All-predictions MAE: {counting_results['all_predictions']['global_mae']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all plots\n",
    "output_dir = \"evaluation/results/test_quick/\"\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "plot_all_metrics(\n",
    "    threshold_sweep=threshold_sweep,\n",
    "    per_class_results=per_class_results['per_class'],\n",
    "    confusion_data=per_class_results,\n",
    "    counting_results=counting_results,\n",
    "    output_dir=output_dir,\n",
    "    run_name=\"Quick Test (50 epochs)\"\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ All plots saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test CLI Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the standalone evaluation script\n",
    "!python scripts/evaluate_run.py \\\n",
    "    --predictions evaluation/metrics/test_quick_predictions.json \\\n",
    "    --ground_truth data/processed/evaluation/test_index.json \\\n",
    "    --output_dir evaluation/results/test_quick_cli/ \\\n",
    "    --run_name \"Quick Test CLI\" \\\n",
    "    --conf_thresholds 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "# Show all plots from the notebook results\n",
    "plot_dir = \"evaluation/results/test_quick/\"\n",
    "\n",
    "plots = [\n",
    "    \"threshold_sweep.png\",\n",
    "    \"per_class_f1.png\", \n",
    "    \"confusion_matrix.png\",\n",
    "    \"count_mae_comparison.png\"\n",
    "]\n",
    "\n",
    "for plot_name in plots:\n",
    "    plot_path = os.path.join(plot_dir, plot_name)\n",
    "    if os.path.exists(plot_path):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"{plot_name}\")\n",
    "        print('='*60)\n",
    "        display(Image(plot_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "If you see this without errors, the evaluation system is working!\n",
    "\n",
    "**What was tested:**\n",
    "- ✅ Dataset download\n",
    "- ✅ Evaluation indices generation\n",
    "- ✅ Model training (10 epochs)\n",
    "- ✅ Prediction generation and saving\n",
    "- ✅ Loading predictions and ground truth\n",
    "- ✅ Detection P/R/F1 at multiple thresholds\n",
    "- ✅ Per-class metrics and confusion matrix\n",
    "- ✅ Counting quality (both methods)\n",
    "- ✅ Plot generation\n",
    "- ✅ CLI evaluation script\n",
    "\n",
    "**Next steps:**\n",
    "1. Run actual experiments with proper training\n",
    "2. Use the evaluation system on train/val/test splits\n",
    "3. Compare models (YOLO vs RT-DETR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}