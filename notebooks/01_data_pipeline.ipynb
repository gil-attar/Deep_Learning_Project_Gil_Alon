{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a5c29b",
   "metadata": {},
   "source": [
    "## 1. Setup - Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01ccedba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "Cloning into 'Deep_Learning_Project_Gil_Alon'...\n",
      "remote: Enumerating objects: 77, done.\u001b[K\n",
      "remote: Counting objects: 100% (77/77), done.\u001b[K\n",
      "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
      "remote: Total 77 (delta 34), reused 53 (delta 17), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (77/77), 1021.65 KiB | 3.97 MiB/s, done.\n",
      "Resolving deltas: 100% (34/34), done.\n",
      "/content/Deep_Learning_Project_Gil_Alon\n"
     ]
    }
   ],
   "source": [
    "# Go to /content first, clean previous clone, then clone fresh\n",
    "%cd /content\n",
    "!rm -rf Deep_Learning_Project_Gil_Alon\n",
    "!git clone https://github.com/gil-attar/Deep_Learning_Project_Gil_Alon.git\n",
    "%cd Deep_Learning_Project_Gil_Alon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1015db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e9ae38",
   "metadata": {},
   "source": [
    "## 2. Set API Keys\n",
    "\n",
    "Replace with your actual keys below, or use Colab Secrets (ðŸ”‘ icon in sidebar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32241106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Option A: Set directly (replace with your keys)\n",
    "os.environ[\"ROBOFLOW_API_KEY\"] = \"zEF9icmDY2oTcPkaDcQY\"\n",
    "\n",
    "# Option B: Use Colab Secrets (recommended)\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"ROBOFLOW_API_KEY\"] = userdata.get('ROBOFLOW_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce638f4",
   "metadata": {},
   "source": [
    "## 3. Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29f6a95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Downloading Food Ingredients Dataset\n",
      "==================================================\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Downloading version 1...\n",
      "Downloading Dataset Version Zip in FOOD-INGREDIENTS-dataset-2-1 to yolov8:: 100% 141695/141695 [00:03<00:00, 43408.25it/s]\n",
      "\n",
      "Extracting Dataset Version Zip to FOOD-INGREDIENTS-dataset-2-1 in yolov8:: 100% 3980/3980 [00:01<00:00, 3281.87it/s]\n",
      "Downloaded to: /content/Deep_Learning_Project_Gil_Alon/FOOD-INGREDIENTS-dataset-2-1\n",
      "Moved train/\n",
      "Moved valid/\n",
      "Moved test/\n",
      "Copied data.yaml\n",
      "\n",
      "==================================================\n",
      "Download Complete!\n",
      "==================================================\n",
      "  train: 1384 images\n",
      "  valid: 200 images\n",
      "  test: 400 images\n",
      "\n",
      "âœ“ Dataset ready in /content/Deep_Learning_Project_Gil_Alon/data/processed\n"
     ]
    }
   ],
   "source": [
    "!python scripts/download_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cd5f11",
   "metadata": {},
   "source": [
    "## 4. Build Evaluation Index\n",
    "\n",
    "(Roboflow already split the data - no need to re-split!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c33eb903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BUILD EVALUATION INDEX\n",
      "============================================================\n",
      "\n",
      "1. Finding dataset...\n",
      "   Found: /content/Deep_Learning_Project_Gil_Alon/data/processed\n",
      "\n",
      "2. Loading class names...\n",
      "   Found 26 classes\n",
      "\n",
      "3. Processing test images...\n",
      "Found 400 test images\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "  Total images: 400\n",
      "  Total objects: 856\n",
      "  Classes: 26\n",
      "\n",
      "  Difficulty distribution:\n",
      "    EASY    :  324 (81.0%)\n",
      "    MEDIUM  :   53 (13.2%)\n",
      "    HARD    :   23 (5.8%)\n",
      "\n",
      "âœ… Saved to: /content/Deep_Learning_Project_Gil_Alon/data/processed/test_index.json\n",
      "\n",
      "Next steps:\n",
      "  1. Train models (scripts/train_models.py)\n",
      "  2. Run evaluation (scripts/evaluate_models.py)\n"
     ]
    }
   ],
   "source": [
    "!python scripts/build_evaluation_index.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2884d5b",
   "metadata": {},
   "source": [
    "## 6. Verify Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a16b9f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Structure ===\n",
      "total 540\n",
      "drwxr-xr-x 5 root root   4096 Jan  8 13:52 .\n",
      "drwxr-xr-x 4 root root   4096 Jan  8 13:51 ..\n",
      "-rw-r--r-- 1 root root    544 Jan  8 13:52 data.yaml\n",
      "-rw-r--r-- 1 root root      0 Jan  8 13:51 .gitkeep\n",
      "drwxr-xr-x 4 root root   4096 Jan  8 13:52 test\n",
      "-rw-r--r-- 1 root root 525299 Jan  8 13:52 test_index.json\n",
      "drwxr-xr-x 4 root root   4096 Jan  8 13:52 train\n",
      "drwxr-xr-x 4 root root   4096 Jan  8 13:52 valid\n",
      "\n",
      "=== Train/Valid/Test Counts ===\n",
      "Train images: 1384\n",
      "Valid images: 200\n",
      "Test images: 400\n"
     ]
    }
   ],
   "source": [
    "# Check directory structure\n",
    "!echo \"=== Data Structure ===\"\n",
    "!ls -la data/processed/\n",
    "\n",
    "!echo \"\"\n",
    "!echo \"=== Train/Valid/Test Counts ===\"\n",
    "!echo \"Train images: $(ls data/processed/train/images/ | wc -l)\"\n",
    "!echo \"Valid images: $(ls data/processed/valid/images/ | wc -l)\"\n",
    "!echo \"Test images: $(ls data/processed/test/images/ | wc -l)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "885574f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluation Index Summary ===\n",
      "Total test images: 400\n",
      "Total objects: 856\n",
      "Classes: 26\n",
      "\n",
      "Difficulty distribution:\n",
      "  easy: 324\n",
      "  medium: 53\n",
      "  hard: 23\n"
     ]
    }
   ],
   "source": [
    "# Check the evaluation index\n",
    "import json\n",
    "\n",
    "with open('data/processed/test_index.json', 'r') as f:\n",
    "    index = json.load(f)\n",
    "\n",
    "print(\"=== Evaluation Index Summary ===\")\n",
    "print(f\"Total test images: {index['metadata']['num_images']}\")\n",
    "print(f\"Total objects: {index['metadata']['total_objects']}\")\n",
    "print(f\"Classes: {index['metadata']['num_classes']}\")\n",
    "print(f\"\\nDifficulty distribution:\")\n",
    "for diff, count in index['metadata']['difficulty_distribution'].items():\n",
    "    print(f\"  {diff}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9830201",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1162800198.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Get 5 sample images from test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtest_images_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/processed/test/images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mtest_labels_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/processed/test/labels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0msample_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "# Show 5 sample images with bounding boxes\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# Load class names\n",
    "with open('data/processed/data.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "class_names = config['names']\n",
    "\n",
    "# Get 5 sample images from test set\n",
    "test_images_dir = Path('data/processed/test/images')\n",
    "test_labels_dir = Path('data/processed/test/labels')\n",
    "sample_images = list(test_images_dir.glob('*.jpg'))[:5]\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "for ax, img_path in zip(axes, sample_images):\n",
    "    # Load image\n",
    "    img = Image.open(img_path)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(img_path.name[:20] + '...', fontsize=8)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Load labels and draw boxes\n",
    "    label_path = test_labels_dir / (img_path.stem + '.txt')\n",
    "    if label_path.exists():\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    class_id = int(parts[0])\n",
    "                    x_center, y_center, w, h = [float(p) for p in parts[1:5]]\n",
    "                    \n",
    "                    # Convert YOLO format to pixel coordinates\n",
    "                    img_w, img_h = img.size\n",
    "                    x1 = (x_center - w/2) * img_w\n",
    "                    y1 = (y_center - h/2) * img_h\n",
    "                    box_w = w * img_w\n",
    "                    box_h = h * img_h\n",
    "                    \n",
    "                    # Draw rectangle\n",
    "                    rect = patches.Rectangle((x1, y1), box_w, box_h, \n",
    "                                            linewidth=2, edgecolor='lime', facecolor='none')\n",
    "                    ax.add_patch(rect)\n",
    "                    \n",
    "                    # Add label\n",
    "                    label = class_names[class_id] if class_id < len(class_names) else f'class_{class_id}'\n",
    "                    ax.text(x1, y1-5, label, fontsize=6, color='lime', \n",
    "                           bbox=dict(boxstyle='round', facecolor='black', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da0880e",
   "metadata": {},
   "source": [
    "## âœ… Data Pipeline Complete!\n",
    "\n",
    "Next steps:\n",
    "- Train models (Step 3)\n",
    "- Evaluate and compare (Step 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
