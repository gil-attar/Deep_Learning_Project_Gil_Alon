{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I92MZyE2Mlzh"
      },
      "source": [
        "# Occlusion Robustness Evaluation\n",
        "\n",
        "**Objective:** Compare YOLOv8 vs RT-DETR performance degradation under synthetic occlusions.\n",
        "\n",
        "**Hypothesis:** RT-DETR (Transformer) should degrade more gracefully than YOLOv8 (CNN) as occlusion increases, because global attention can reason about partial objects better than local convolutions.\n",
        "\n",
        "**Experiment Design:**\n",
        "- Fixed weights (no retraining!)\n",
        "- Same 400 test images with different occlusion levels\n",
        "- 4 test sets: 0%, 20%, 40%, 60% occlusion\n",
        "- Same occlusion patterns for both models (seed=42)\n",
        "- Generate 24 JSON files (2 models √ó 4 levels √ó 3 files)\n",
        "\n",
        "**Runtime:** ~30-40 minutes total on GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RfP9KpqMlzj"
      },
      "source": [
        "## 1. Setup & Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt3LdY6DMlzj",
        "outputId": "93d09e49-c920-426a-aedd-ec8a22f903e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab\n",
            "‚úì Repository already exists\n",
            "Working directory: /content/Deep_Learning_Gil_Alon\n"
          ]
        }
      ],
      "source": [
        "# Check if running in Colab\n",
        "import os\n",
        "import sys\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Running in Google Colab\")\n",
        "\n",
        "    # Check if repository already exists\n",
        "    if not os.path.exists('/content/Deep_Learning_Gil_Alon'):\n",
        "        print(\"Cloning repository...\")\n",
        "        # Clone without authentication prompt\n",
        "        !git clone https://github.com/gil-attar/Deep_Learning_Gil_Alon.git 2>&1 | grep -v \"Username\"\n",
        "\n",
        "        # If clone failed, try alternative\n",
        "        if not os.path.exists('/content/Deep_Learning_Gil_Alon'):\n",
        "            print(\"\\n‚ö†Ô∏è Git clone failed. Using Google Drive instead...\")\n",
        "            print(\"Make sure you've uploaded the project to Google Drive first.\")\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive')\n",
        "            !cp -r /content/drive/MyDrive/Deep_Learning_Project_Gil_Alon /content/Deep_Learning_Gil_Alon 2>/dev/null || cp -r /content/drive/MyDrive/Deep_Learning_Gil_Alon /content/Deep_Learning_Gil_Alon\n",
        "    else:\n",
        "        print(\"‚úì Repository already exists\")\n",
        "\n",
        "    os.chdir('/content/Deep_Learning_Gil_Alon')\n",
        "else:\n",
        "    print(\"Running locally\")\n",
        "    # Assume we're in notebooks/ directory\n",
        "    if os.path.basename(os.getcwd()) == 'notebooks':\n",
        "        os.chdir('..')\n",
        "\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "D9b2NMnjMlzk"
      },
      "outputs": [],
      "source": [
        "# Install dependencies (Colab only)\n",
        "if IN_COLAB:\n",
        "    !pip install -q ultralytics roboflow pyyaml pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFqRu6L7Mlzl",
        "outputId": "69610f6e-9418-4b91-9362-4cfac2c59605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU: NVIDIA L4\n",
            "CUDA version: 12.6\n"
          ]
        }
      ],
      "source": [
        "# Verify GPU availability\n",
        "import torch\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è WARNING: No GPU detected! Evaluation will be very slow.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhW5SDGdMlzl"
      },
      "source": [
        "## 2. Download Dataset (Colab Only)\n",
        "\n",
        "**NOTE:** If running locally, ensure dataset already exists in `data/raw/test/`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "1onwk6uZMlzl"
      },
      "outputs": [],
      "source": [
        "# Set your Roboflow API key here\n",
        "ROBOFLOW_API_KEY = \"zEF9icmDY2oTcPkaDcQY\"  # ‚Üê REPLACE THIS!\n",
        "\n",
        "# Dataset version\n",
        "DATASET_VERSION = 1  # Use version 1 (default)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kk_HO0raMlzm",
        "outputId": "11a7e2fa-d320-4064-b412-3687f217a948"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in FOOD-INGREDIENTS-dataset-2-1 to yolov8:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 141695/141695 [00:01<00:00, 73796.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to FOOD-INGREDIENTS-dataset-2-1 in yolov8:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3980/3980 [00:00<00:00, 6846.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded to: /content/Deep_Learning_Gil_Alon/FOOD-INGREDIENTS-dataset-2-1\n",
            "  Moved train/\n",
            "  Moved valid/\n",
            "  Moved test/\n",
            "  Copied data.yaml\n",
            "\n",
            "‚úì Dataset ready in: data/raw\n",
            "\n",
            "‚úì Found 400 test images\n"
          ]
        }
      ],
      "source": [
        "if IN_COLAB:\n",
        "    from roboflow import Roboflow\n",
        "\n",
        "    # Download dataset using correct workspace and project\n",
        "    rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
        "    project = rf.workspace(\"gaworkspace-utcbg\").project(\"food-ingredients-dataset-2-rewtd\")\n",
        "\n",
        "    # Download to temporary location first\n",
        "    dataset = project.version(DATASET_VERSION).download(\"yolov8\")\n",
        "\n",
        "    print(f\"Downloaded to: {dataset.location}\")\n",
        "\n",
        "    # Move to data/raw using the download_dataset.py logic\n",
        "    import shutil\n",
        "    from pathlib import Path\n",
        "\n",
        "    src = Path(dataset.location)\n",
        "    dest = Path(\"data/raw\")\n",
        "    dest.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Move train, valid, test folders\n",
        "    for split in [\"train\", \"valid\", \"test\"]:\n",
        "        split_dest = dest / split\n",
        "        if split_dest.exists():\n",
        "            shutil.rmtree(split_dest)\n",
        "        if (src / split).exists():\n",
        "            shutil.move(str(src / split), str(dest / split))\n",
        "            print(f\"  Moved {split}/\")\n",
        "\n",
        "    # Copy data.yaml\n",
        "    if (src / \"data.yaml\").exists():\n",
        "        shutil.copy(str(src / \"data.yaml\"), str(dest / \"data.yaml\"))\n",
        "        print(\"  Copied data.yaml\")\n",
        "\n",
        "    # Clean up\n",
        "    shutil.rmtree(src, ignore_errors=True)\n",
        "\n",
        "    print(f\"\\n‚úì Dataset ready in: {dest}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Running locally - assuming dataset already exists\")\n",
        "\n",
        "# Verify test set exists\n",
        "from pathlib import Path\n",
        "test_images = list(Path(\"data/raw/test/images\").glob(\"*.jpg\"))\n",
        "print(f\"\\n‚úì Found {len(test_images)} test images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EsdIL5tMlzm"
      },
      "source": [
        "## 3. Verify Prerequisites\n",
        "\n",
        "Before generating occlusions, ensure:\n",
        "- ‚úÖ Model weights exist (from Step 3.2 training)\n",
        "- ‚úÖ Test index exists (from Step 2)\n",
        "- ‚úÖ Scripts are present"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKIF8hctMlzm",
        "outputId": "985852b6-9c19-4d85-e1df-bf432933631c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì YOLOv8 weights: models/yolov8n_baseline.pt\n",
            "‚úì RT-DETR weights: models/rtdetr_baseline.pt\n",
            "‚úì Test index: data/processed/evaluation/test_index.json\n",
            "‚úì Occlusion script: scripts/generate_synthetic_occlusions.py\n",
            "‚úì Evaluation script: scripts/evaluate_baseline.py\n",
            "‚úì Data YAML helper: scripts/create_data_yaml.py\n",
            "\n",
            "‚úì All prerequisites satisfied!\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# Check required files\n",
        "required_files = {\n",
        "    \"YOLOv8 weights\": \"models/yolov8n_baseline.pt\",\n",
        "    \"RT-DETR weights\": \"models/rtdetr_baseline.pt\",\n",
        "    \"Test index\": \"data/processed/evaluation/test_index.json\",\n",
        "    \"Occlusion script\": \"scripts/generate_synthetic_occlusions.py\",\n",
        "    \"Evaluation script\": \"scripts/evaluate_baseline.py\",\n",
        "    \"Data YAML helper\": \"scripts/create_data_yaml.py\"\n",
        "}\n",
        "\n",
        "all_exist = True\n",
        "for name, path in required_files.items():\n",
        "    exists = Path(path).exists()\n",
        "    status = \"‚úì\" if exists else \"‚úó\"\n",
        "    print(f\"{status} {name}: {path}\")\n",
        "    if not exists:\n",
        "        all_exist = False\n",
        "\n",
        "if not all_exist:\n",
        "    print(\"\\n‚ùå Some required files are missing!\")\n",
        "    print(\"\\nPlease ensure:\")\n",
        "    print(\"  1. You've trained models in 02_train_models.ipynb\")\n",
        "    print(\"  2. You've run build_evaluation_index.py (Step 2)\")\n",
        "    print(\"  3. All scripts are present in scripts/ directory\")\n",
        "else:\n",
        "    print(\"\\n‚úì All prerequisites satisfied!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXJK41wUMlzm"
      },
      "source": [
        "## 4. Generate Synthetic Occlusion Test Sets\n",
        "\n",
        "Create 4 test sets from the same 400 images:\n",
        "- `level_000/` - Original (0% occlusion) - baseline\n",
        "- `level_020/` - 20% occlusion per bbox\n",
        "- `level_040/` - 40% occlusion per bbox\n",
        "- `level_060/` - 60% occlusion per bbox\n",
        "\n",
        "**Time:** ~2-5 minutes depending on image sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKV4ZnwCMlzm",
        "outputId": "624f2842-be51-44c2-f405-d010228af0f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "GENERATE SYNTHETIC OCCLUSIONS\n",
            "============================================================\n",
            "  Test index:  data/processed/evaluation/test_index.json\n",
            "  Images dir:  data/raw/test/images\n",
            "  Output dir:  data/synthetic_occlusion\n",
            "  Levels:      [0.0, 0.2, 0.4, 0.6]\n",
            "  Seed:        42\n",
            "============================================================\n",
            "\n",
            "Loaded 400 test images from index\n",
            "\n",
            "Processing level_000 (0% occlusion)...\n",
            "  ‚úì Created 400 images, 856 boxes occluded\n",
            "\n",
            "Processing level_020 (20% occlusion)...\n",
            "  ‚úì Created 400 images, 856 boxes occluded\n",
            "\n",
            "Processing level_040 (40% occlusion)...\n",
            "  ‚úì Created 400 images, 856 boxes occluded\n",
            "\n",
            "Processing level_060 (60% occlusion)...\n",
            "  ‚úì Created 400 images, 856 boxes occluded\n",
            "\n",
            "============================================================\n",
            "SYNTHETIC OCCLUSION TEST SETS COMPLETE\n",
            "============================================================\n",
            "\n",
            "Created test sets:\n",
            "  ‚úì data/synthetic_occlusion/level_000\n",
            "  ‚úì data/synthetic_occlusion/level_020\n",
            "  ‚úì data/synthetic_occlusion/level_040\n",
            "  ‚úì data/synthetic_occlusion/level_060\n",
            "\n",
            "Manifest: data/synthetic_occlusion/occlusion_manifest.json\n",
            "\n",
            "============================================================\n",
            "NEXT: Run evaluation on each test set\n",
            "============================================================\n",
            "\n",
            "Example usage in evaluation:\n",
            "\n",
            "    # For each occlusion level:\n",
            "    model = YOLO('models/yolov8n_baseline.pt')\n",
            "    results = model.val(data='data/synthetic_occlusion/level_020/data.yaml')\n",
            "    \n",
            "    # Compare mAP across levels to see degradation curve\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate occluded test sets\n",
        "!python scripts/generate_synthetic_occlusions.py \\\n",
        "    --test_index data/processed/evaluation/test_index.json \\\n",
        "    --images_dir data/raw/test/images \\\n",
        "    --labels_dir data/raw/test/labels \\\n",
        "    --output_dir data/synthetic_occlusion \\\n",
        "    --levels 0.0,0.2,0.4,0.6 \\\n",
        "    --seed 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr2pqzMSMlzn",
        "outputId": "31d36501-8052-4dd0-8639-4794673c2993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Synthetic occlusion test sets created!\n",
            "\n",
            "Levels generated:\n",
            "  - level_000: 0% of each bbox covered\n",
            "  - level_020: 20% of each bbox covered\n",
            "  - level_040: 40% of each bbox covered\n",
            "  - level_060: 60% of each bbox covered\n",
            "\n",
            "Statistics:\n",
            "  level_000:\n",
            "    - Images: 400\n",
            "    - Boxes occluded: 856\n",
            "  level_020:\n",
            "    - Images: 400\n",
            "    - Boxes occluded: 856\n",
            "  level_040:\n",
            "    - Images: 400\n",
            "    - Boxes occluded: 856\n",
            "  level_060:\n",
            "    - Images: 400\n",
            "    - Boxes occluded: 856\n"
          ]
        }
      ],
      "source": [
        "# Verify occlusion test sets created\n",
        "occlusion_levels = [0, 20, 40, 60]\n",
        "occlusion_manifest_path = Path(\"data/synthetic_occlusion/occlusion_manifest.json\")\n",
        "\n",
        "if occlusion_manifest_path.exists():\n",
        "    with open(occlusion_manifest_path, 'r') as f:\n",
        "        manifest = json.load(f)\n",
        "\n",
        "    print(\"‚úì Synthetic occlusion test sets created!\\n\")\n",
        "    print(\"Levels generated:\")\n",
        "    for level_name, description in manifest['occlusion_levels'].items():\n",
        "        print(f\"  - {level_name}: {description}\")\n",
        "\n",
        "    print(\"\\nStatistics:\")\n",
        "    for level_name, stats in manifest['statistics'].items():\n",
        "        print(f\"  {level_name}:\")\n",
        "        print(f\"    - Images: {stats['total_images']}\")\n",
        "        print(f\"    - Boxes occluded: {stats['total_boxes_occluded']}\")\n",
        "else:\n",
        "    print(\"‚ùå Manifest not found! Occlusion generation may have failed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10rGgRN-Mlzn"
      },
      "source": [
        "## 5. Create data.yaml for Each Occlusion Level\n",
        "\n",
        "Each test set needs its own `data.yaml` for Ultralytics validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heOzO8QOMlzn",
        "outputId": "13851144-557b-405b-e22e-bcff93b86495"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating data.yaml files for each occlusion level...\n",
            "\n",
            "‚úì Created level_000/data.yaml\n",
            "‚úì Created level_020/data.yaml\n",
            "‚úì Created level_040/data.yaml\n",
            "‚úì Created level_060/data.yaml\n",
            "\n",
            "‚úì All data.yaml files created\n"
          ]
        }
      ],
      "source": [
        "# Create data.yaml for each occlusion level\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "# Load original data.yaml to get class names\n",
        "with open('data/raw/data.yaml', 'r') as f:\n",
        "    original_config = yaml.safe_load(f)\n",
        "\n",
        "print(\"Creating data.yaml files for each occlusion level...\\n\")\n",
        "\n",
        "for level in [0, 20, 40, 60]:\n",
        "    level_name = f\"level_{level:03d}\"\n",
        "    level_dir = Path(f\"data/synthetic_occlusion/{level_name}\")\n",
        "\n",
        "    # Create data.yaml with train/val pointing to original, test to occluded\n",
        "    config = {\n",
        "        'path': str(level_dir.absolute()),\n",
        "        'train': '../../../raw/train/images',  # Use original training data\n",
        "        'val': '../../../raw/valid/images',    # Use original validation data\n",
        "        'test': 'images',                      # Use occluded test images\n",
        "        'names': original_config['names'],\n",
        "        'nc': len(original_config['names'])\n",
        "    }\n",
        "\n",
        "    yaml_path = level_dir / \"data.yaml\"\n",
        "    yaml_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with open(yaml_path, 'w') as f:\n",
        "        yaml.dump(config, f, default_flow_style=False)\n",
        "\n",
        "    print(f\"‚úì Created {level_name}/data.yaml\")\n",
        "\n",
        "print(f\"\\n‚úì All data.yaml files created\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for evaluation\n",
        "import json\n",
        "from pathlib import Path\n",
        "from ultralytics import YOLO, RTDETR\n",
        "\n",
        "def evaluate_and_save(model_type, model_path, data_yaml, run_id, occlusion_level):\n",
        "    \"\"\"\n",
        "    Evaluate a model and save metrics to JSON.\n",
        "\n",
        "    Args:\n",
        "        model_type: 'yolo' or 'rtdetr'\n",
        "        model_path: Path to model weights\n",
        "        data_yaml: Path to data.yaml for this test set\n",
        "        run_id: Unique run identifier (e.g., 'e3_yolo_020')\n",
        "        occlusion_level: Occlusion percentage (0.0, 0.2, 0.4, 0.6)\n",
        "    \"\"\"\n",
        "    print(f\"\\nEvaluating {run_id}...\")\n",
        "\n",
        "    # Load model\n",
        "    if model_type == 'yolo':\n",
        "        model = YOLO(model_path)\n",
        "    else:\n",
        "        model = RTDETR(model_path)\n",
        "\n",
        "    # Run validation on TEST split (not val split)\n",
        "    results = model.val(\n",
        "        data=data_yaml,\n",
        "        split='test',  # Use test split, not validation\n",
        "        imgsz=640,\n",
        "        conf=0.25,\n",
        "        iou=0.50,\n",
        "        device=0,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # Create output directory\n",
        "    output_dir = Path(\"evaluation/metrics\")\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Extract metrics\n",
        "    metrics_data = {\n",
        "        \"run_id\": run_id,\n",
        "        \"model\": model_type,\n",
        "        \"occlusion_level\": float(occlusion_level),\n",
        "        \"aggregate_metrics\": {\n",
        "            \"map50\": float(results.results_dict.get('metrics/mAP50(B)', 0)),\n",
        "            \"map50_95\": float(results.results_dict.get('metrics/mAP50-95(B)', 0)),\n",
        "            \"precision\": float(results.results_dict.get('metrics/precision(B)', 0)),\n",
        "            \"recall\": float(results.results_dict.get('metrics/recall(B)', 0)),\n",
        "            \"fps\": float(results.speed.get('inference', 0))\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Save metrics JSON\n",
        "    with open(output_dir / f\"{run_id}_metrics.json\", 'w') as f:\n",
        "        json.dump(metrics_data, f, indent=2)\n",
        "\n",
        "    # Create minimal run and predictions JSONs (for compatibility)\n",
        "    run_data = {\n",
        "        \"run_id\": run_id,\n",
        "        \"model\": model_type,\n",
        "        \"occlusion_level\": float(occlusion_level),\n",
        "        \"data_yaml\": str(data_yaml)\n",
        "    }\n",
        "\n",
        "    with open(output_dir / f\"{run_id}_run.json\", 'w') as f:\n",
        "        json.dump(run_data, f, indent=2)\n",
        "\n",
        "    predictions_data = {\n",
        "        \"run_id\": run_id,\n",
        "        \"predictions\": []  # Placeholder - full predictions not needed for degradation analysis\n",
        "    }\n",
        "\n",
        "    with open(output_dir / f\"{run_id}_predictions.json\", 'w') as f:\n",
        "        json.dump(predictions_data, f, indent=2)\n",
        "\n",
        "    print(f\"‚úì {run_id}: mAP@50={metrics_data['aggregate_metrics']['map50']:.3f}, \"\n",
        "          f\"mAP@50-95={metrics_data['aggregate_metrics']['map50_95']:.3f}\")\n",
        "\n",
        "    return metrics_data\n",
        "\n",
        "print(\"‚úì Helper function loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWbT0RXlNce3",
        "outputId": "534b7c17-ce60-4b97-acf4-3bc68a3a5483"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Helper function loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oInZdsktMlzo",
        "outputId": "b81c0b8c-fffc-44c1-a3d7-8810af04be11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "YOLOv8 Evaluations\n",
            "============================================================\n",
            "\n",
            "Evaluating e3_yolo_000...\n",
            "Ultralytics 8.3.251 üöÄ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "Model summary (fused): 72 layers, 3,010,718 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1391.2¬±857.1 MB/s, size: 78.4 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Deep_Learning_Gil_Alon/data/synthetic_occlusion/level_000/labels... 400 images, 26 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 400/400 1.6Kit/s 0.3s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/Deep_Learning_Gil_Alon/data/synthetic_occlusion/level_000/labels.cache\n",
            "WARNING ‚ö†Ô∏è Box and segment counts should be equal, but got len(segments) = 11, len(boxes) = 856. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 25/25 8.1it/s 3.1s\n",
            "                   all        400        856       0.73      0.564      0.671      0.447\n",
            "Speed: 0.9ms preprocess, 2.9ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
            "Results saved to \u001b[1m/content/Deep_Learning_Gil_Alon/runs/detect/val3\u001b[0m\n",
            "‚úì e3_yolo_000: mAP@50=0.671, mAP@50-95=0.447\n",
            "\n",
            "Evaluating e3_yolo_020...\n",
            "Ultralytics 8.3.251 üöÄ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "Model summary (fused): 72 layers, 3,010,718 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1623.6¬±860.8 MB/s, size: 58.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Deep_Learning_Gil_Alon/data/synthetic_occlusion/level_020/labels... 400 images, 26 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 400/400 1.6Kit/s 0.3s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/Deep_Learning_Gil_Alon/data/synthetic_occlusion/level_020/labels.cache\n",
            "WARNING ‚ö†Ô∏è Box and segment counts should be equal, but got len(segments) = 11, len(boxes) = 856. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 25/25 10.9it/s 2.3s\n",
            "                   all        400        856       0.18      0.121      0.138     0.0597\n",
            "Speed: 0.9ms preprocess, 1.7ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
            "Results saved to \u001b[1m/content/Deep_Learning_Gil_Alon/runs/detect/val4\u001b[0m\n",
            "‚úì e3_yolo_020: mAP@50=0.138, mAP@50-95=0.060\n",
            "\n",
            "Evaluating e3_yolo_040...\n",
            "Ultralytics 8.3.251 üöÄ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "Model summary (fused): 72 layers, 3,010,718 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1541.6¬±686.6 MB/s, size: 55.3 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Deep_Learning_Gil_Alon/data/synthetic_occlusion/level_040/labels... 400 images, 26 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 400/400 1.6Kit/s 0.3s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/Deep_Learning_Gil_Alon/data/synthetic_occlusion/level_040/labels.cache\n",
            "WARNING ‚ö†Ô∏è Box and segment counts should be equal, but got len(segments) = 11, len(boxes) = 856. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 25/25 11.3it/s 2.2s\n",
            "                   all        400        856       0.08     0.0188     0.0486     0.0256\n",
            "Speed: 1.0ms preprocess, 1.6ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
            "Results saved to \u001b[1m/content/Deep_Learning_Gil_Alon/runs/detect/val5\u001b[0m\n",
            "‚úì e3_yolo_040: mAP@50=0.049, mAP@50-95=0.026\n",
            "\n",
            "Evaluating e3_yolo_060...\n",
            "Ultralytics 8.3.251 üöÄ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "Model summary (fused): 72 layers, 3,010,718 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1330.7¬±1125.4 MB/s, size: 78.1 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Deep_Learning_Gil_Alon/data/synthetic_occlusion/level_060/labels... 400 images, 26 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 400/400 1.5Kit/s 0.3s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/Deep_Learning_Gil_Alon/data/synthetic_occlusion/level_060/labels.cache\n",
            "WARNING ‚ö†Ô∏è Box and segment counts should be equal, but got len(segments) = 11, len(boxes) = 856. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 25/25 11.5it/s 2.2s\n",
            "                   all        400        856     0.0744     0.0151     0.0443     0.0231\n",
            "Speed: 1.0ms preprocess, 2.0ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
            "Results saved to \u001b[1m/content/Deep_Learning_Gil_Alon/runs/detect/val6\u001b[0m\n",
            "‚úì e3_yolo_060: mAP@50=0.044, mAP@50-95=0.023\n",
            "\n",
            "‚úì Completed all YOLO evaluations\n"
          ]
        }
      ],
      "source": [
        "# Evaluate YOLOv8 on all occlusion levels\n",
        "print(\"=\" * 60)\n",
        "print(\"YOLOv8 Evaluations\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "yolo_results = {}\n",
        "for level, level_pct in [(0, '000'), (0.2, '020'), (0.4, '040'), (0.6, '060')]:\n",
        "    result = evaluate_and_save(\n",
        "        model_type='yolo',\n",
        "        model_path='models/yolov8n_baseline.pt',\n",
        "        data_yaml=f'data/synthetic_occlusion/level_{level_pct}/data.yaml',\n",
        "        run_id=f'e3_yolo_{level_pct}',\n",
        "        occlusion_level=level\n",
        "    )\n",
        "    yolo_results[level] = result\n",
        "\n",
        "print(f\"\\n‚úì Completed all YOLO evaluations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KihvCcqDMlzo"
      },
      "source": [
        "## 7. Evaluate RT-DETR on All Occlusion Levels\n",
        "\n",
        "Same procedure for RT-DETR.\n",
        "\n",
        "Generates 12 more JSON files:\n",
        "- `rtdetr_000_run.json`, `rtdetr_000_metrics.json`, `rtdetr_000_predictions.json`\n",
        "- `rtdetr_020_run.json`, `rtdetr_020_metrics.json`, `rtdetr_020_predictions.json`\n",
        "- `rtdetr_040_run.json`, `rtdetr_040_metrics.json`, `rtdetr_040_predictions.json`\n",
        "- `rtdetr_060_run.json`, `rtdetr_060_metrics.json`, `rtdetr_060_predictions.json`\n",
        "\n",
        "**Time:** ~10-15 minutes total (RT-DETR is slower than YOLO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmTsFts3Mlzp",
        "outputId": "5dade870-2bf1-4a75-f696-5e587c9793ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "RT-DETR Evaluations\n",
            "============================================================\n",
            "\n",
            "Evaluating e3_rtdetr_000...\n",
            "Ultralytics 8.3.251 üöÄ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "rt-detr-l summary: 310 layers, 32,037,170 parameters, 0 gradients, 103.5 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1650.3¬±243.4 MB/s, size: 60.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Deep_Learning_Gil_Alon/data/synthetic_occlusion/level_000/labels.cache... 400 images, 26 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 400/400 612.8Kit/s 0.0s\n",
            "WARNING ‚ö†Ô∏è Box and segment counts should be equal, but got len(segments) = 11, len(boxes) = 856. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 25/25 3.1it/s 8.1s\n",
            "                   all        400        856      0.713      0.677      0.731      0.466\n",
            "Speed: 1.0ms preprocess, 16.3ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
            "Results saved to \u001b[1m/content/Deep_Learning_Gil_Alon/runs/detect/val7\u001b[0m\n",
            "‚úì e3_rtdetr_000: mAP@50=0.731, mAP@50-95=0.466\n",
            "\n",
            "Evaluating e3_rtdetr_020...\n",
            "Ultralytics 8.3.251 üöÄ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "rt-detr-l summary: 310 layers, 32,037,170 parameters, 0 gradients, 103.5 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1496.8¬±632.9 MB/s, size: 46.7 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Deep_Learning_Gil_Alon/data/synthetic_occlusion/level_020/labels.cache... 400 images, 26 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 400/400 856.0Kit/s 0.0s\n",
            "WARNING ‚ö†Ô∏è Box and segment counts should be equal, but got len(segments) = 11, len(boxes) = 856. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 25/25 3.2it/s 7.9s\n",
            "                   all        400        856      0.318      0.224      0.236      0.134\n",
            "Speed: 1.0ms preprocess, 16.0ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
            "Results saved to \u001b[1m/content/Deep_Learning_Gil_Alon/runs/detect/val8\u001b[0m\n",
            "‚úì e3_rtdetr_020: mAP@50=0.236, mAP@50-95=0.134\n",
            "\n",
            "Evaluating e3_rtdetr_040...\n",
            "Ultralytics 8.3.251 üöÄ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "rt-detr-l summary: 310 layers, 32,037,170 parameters, 0 gradients, 103.5 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1124.5¬±538.2 MB/s, size: 35.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Deep_Learning_Gil_Alon/data/synthetic_occlusion/level_040/labels.cache... 400 images, 26 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 400/400 854.2Kit/s 0.0s\n",
            "WARNING ‚ö†Ô∏è Box and segment counts should be equal, but got len(segments) = 11, len(boxes) = 856. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 25/25 3.2it/s 7.9s\n",
            "                   all        400        856     0.0914      0.108     0.0686     0.0408\n",
            "Speed: 1.0ms preprocess, 16.2ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
            "Results saved to \u001b[1m/content/Deep_Learning_Gil_Alon/runs/detect/val9\u001b[0m\n",
            "‚úì e3_rtdetr_040: mAP@50=0.069, mAP@50-95=0.041\n",
            "\n",
            "Evaluating e3_rtdetr_060...\n",
            "Ultralytics 8.3.251 üöÄ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "rt-detr-l summary: 310 layers, 32,037,170 parameters, 0 gradients, 103.5 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1993.8¬±897.4 MB/s, size: 190.9 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Deep_Learning_Gil_Alon/data/synthetic_occlusion/level_060/labels.cache... 400 images, 26 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 400/400 885.8Kit/s 0.0s\n",
            "WARNING ‚ö†Ô∏è Box and segment counts should be equal, but got len(segments) = 11, len(boxes) = 856. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 25/25 3.2it/s 7.8s\n",
            "                   all        400        856     0.0288     0.0521     0.0236     0.0137\n",
            "Speed: 1.0ms preprocess, 16.1ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
            "Results saved to \u001b[1m/content/Deep_Learning_Gil_Alon/runs/detect/val10\u001b[0m\n",
            "‚úì e3_rtdetr_060: mAP@50=0.024, mAP@50-95=0.014\n",
            "\n",
            "‚úì Completed all RT-DETR evaluations\n"
          ]
        }
      ],
      "source": [
        "# Evaluate RT-DETR on all occlusion levels\n",
        "print(\"=\" * 60)\n",
        "print(\"RT-DETR Evaluations\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "rtdetr_results = {}\n",
        "for level, level_pct in [(0, '000'), (0.2, '020'), (0.4, '040'), (0.6, '060')]:\n",
        "    result = evaluate_and_save(\n",
        "        model_type='rtdetr',\n",
        "        model_path='models/rtdetr_baseline.pt',\n",
        "        data_yaml=f'data/synthetic_occlusion/level_{level_pct}/data.yaml',\n",
        "        run_id=f'e3_rtdetr_{level_pct}',\n",
        "        occlusion_level=level\n",
        "    )\n",
        "    rtdetr_results[level] = result\n",
        "\n",
        "print(f\"\\n‚úì Completed all RT-DETR evaluations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycUBjAR-Mlzp"
      },
      "source": [
        "## 8. Verify All JSON Files Generated\n",
        "\n",
        "Should have 24 JSON files total:\n",
        "- 8 `*_run.json` files\n",
        "- 8 `*_metrics.json` files\n",
        "- 8 `*_predictions.json` files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbjLr94vMlzp",
        "outputId": "ad0976e9-ee3a-4684-e537-973001af903a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking generated JSON files:\n",
            "\n",
            "================================================================================\n",
            "\n",
            "YOLO Evaluations:\n",
            "\n",
            "  Level 000 (0% occlusion):\n",
            "    ‚úì e3_yolo_000_run.json (0.1 KB)\n",
            "    ‚úì e3_yolo_000_metrics.json (0.3 KB)\n",
            "    ‚úì e3_yolo_000_predictions.json (0.0 KB)\n",
            "\n",
            "  Level 020 (20% occlusion):\n",
            "    ‚úì e3_yolo_020_run.json (0.1 KB)\n",
            "    ‚úì e3_yolo_020_metrics.json (0.3 KB)\n",
            "    ‚úì e3_yolo_020_predictions.json (0.0 KB)\n",
            "\n",
            "  Level 040 (40% occlusion):\n",
            "    ‚úì e3_yolo_040_run.json (0.1 KB)\n",
            "    ‚úì e3_yolo_040_metrics.json (0.3 KB)\n",
            "    ‚úì e3_yolo_040_predictions.json (0.0 KB)\n",
            "\n",
            "  Level 060 (60% occlusion):\n",
            "    ‚úì e3_yolo_060_run.json (0.1 KB)\n",
            "    ‚úì e3_yolo_060_metrics.json (0.3 KB)\n",
            "    ‚úì e3_yolo_060_predictions.json (0.0 KB)\n",
            "\n",
            "RTDETR Evaluations:\n",
            "\n",
            "  Level 000 (0% occlusion):\n",
            "    ‚úì e3_rtdetr_000_run.json (0.1 KB)\n",
            "    ‚úì e3_rtdetr_000_metrics.json (0.3 KB)\n",
            "    ‚úì e3_rtdetr_000_predictions.json (0.1 KB)\n",
            "\n",
            "  Level 020 (20% occlusion):\n",
            "    ‚úì e3_rtdetr_020_run.json (0.1 KB)\n",
            "    ‚úì e3_rtdetr_020_metrics.json (0.3 KB)\n",
            "    ‚úì e3_rtdetr_020_predictions.json (0.1 KB)\n",
            "\n",
            "  Level 040 (40% occlusion):\n",
            "    ‚úì e3_rtdetr_040_run.json (0.1 KB)\n",
            "    ‚úì e3_rtdetr_040_metrics.json (0.3 KB)\n",
            "    ‚úì e3_rtdetr_040_predictions.json (0.1 KB)\n",
            "\n",
            "  Level 060 (60% occlusion):\n",
            "    ‚úì e3_rtdetr_060_run.json (0.1 KB)\n",
            "    ‚úì e3_rtdetr_060_metrics.json (0.3 KB)\n",
            "    ‚úì e3_rtdetr_060_predictions.json (0.1 KB)\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Total files created: 24 / 24\n",
            "‚úì All 24 JSON files successfully generated!\n"
          ]
        }
      ],
      "source": [
        "# Verify all JSON files exist\n",
        "output_dir = Path(\"evaluation/metrics\")\n",
        "models = ['yolo', 'rtdetr']\n",
        "levels = ['000', '020', '040', '060']\n",
        "file_types = ['run', 'metrics', 'predictions']\n",
        "\n",
        "print(\"Checking generated JSON files:\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "all_files_exist = True\n",
        "total_files = 0\n",
        "\n",
        "for model in models:\n",
        "    print(f\"\\n{model.upper()} Evaluations:\")\n",
        "    for level in levels:\n",
        "        print(f\"\\n  Level {level} ({int(level)}% occlusion):\")\n",
        "        for file_type in file_types:\n",
        "            filename = f\"e3_{model}_{level}_{file_type}.json\"\n",
        "            filepath = output_dir / filename\n",
        "            exists = filepath.exists()\n",
        "            status = \"‚úì\" if exists else \"‚úó\"\n",
        "\n",
        "            # Get file size if exists\n",
        "            size_str = \"\"\n",
        "            if exists:\n",
        "                size_kb = filepath.stat().st_size / 1024\n",
        "                size_str = f\"({size_kb:.1f} KB)\"\n",
        "                total_files += 1\n",
        "\n",
        "            print(f\"    {status} {filename} {size_str}\")\n",
        "\n",
        "            if not exists:\n",
        "                all_files_exist = False\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"\\nTotal files created: {total_files} / 24\")\n",
        "\n",
        "if all_files_exist:\n",
        "    print(\"‚úì All 24 JSON files successfully generated!\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Some JSON files are missing!\")\n",
        "    print(\"Check the evaluation outputs above for errors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9SXWSY5Mlzq"
      },
      "source": [
        "## 9. Compare Performance Across Occlusion Levels\n",
        "\n",
        "Quick preview of the degradation curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWhb62COMlzq",
        "outputId": "4da4a2aa-712d-429b-d088-11e62cbc389c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "OCCLUSION ROBUSTNESS COMPARISON\n",
            "================================================================================\n",
            " Model Occlusion  mAP@50  mAP@50-95  Precision  Recall  FPS\n",
            "  YOLO        0%   0.671      0.447      0.730   0.564  2.9\n",
            "  YOLO       20%   0.138      0.060      0.180   0.121  1.7\n",
            "  YOLO       40%   0.049      0.026      0.080   0.019  1.6\n",
            "  YOLO       60%   0.044      0.023      0.074   0.015  2.0\n",
            "RTDETR        0%   0.731      0.466      0.713   0.677 16.3\n",
            "RTDETR       20%   0.236      0.134      0.318   0.224 16.0\n",
            "RTDETR       40%   0.069      0.041      0.091   0.108 16.2\n",
            "RTDETR       60%   0.024      0.014      0.029   0.052 16.1\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Load all metrics\n",
        "results = []\n",
        "\n",
        "for model in models:\n",
        "    for level in levels:\n",
        "        metrics_file = output_dir / f\"e3_{model}_{level}_metrics.json\"\n",
        "        if metrics_file.exists():\n",
        "            with open(metrics_file, 'r') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            agg = data['aggregate_metrics']\n",
        "            results.append({\n",
        "                'Model': model.upper(),\n",
        "                'Occlusion': f\"{int(level)}%\",\n",
        "                'mAP@50': round(agg['map50'], 3),\n",
        "                'mAP@50-95': round(agg['map50_95'], 3),\n",
        "                'Precision': round(agg['precision'], 3),\n",
        "                'Recall': round(agg['recall'], 3),\n",
        "                'FPS': round(agg['fps'], 1)\n",
        "            })\n",
        "\n",
        "# Create comparison table\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"OCCLUSION ROBUSTNESS COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "print(df.to_string(index=False))\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8F9VPmSMlzr",
        "outputId": "8ed389ed-ca69-4627-8de0-e3a500bb4eef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Performance Degradation (compared to 0% baseline):\n",
            "\n",
            "YOLO:\n",
            "   20% occlusion: mAP@50 = 0.138 (+79.4% vs baseline)\n",
            "   40% occlusion: mAP@50 = 0.049 (+92.7% vs baseline)\n",
            "   60% occlusion: mAP@50 = 0.044 (+93.4% vs baseline)\n",
            "\n",
            "RTDETR:\n",
            "   20% occlusion: mAP@50 = 0.236 (+67.7% vs baseline)\n",
            "   40% occlusion: mAP@50 = 0.069 (+90.6% vs baseline)\n",
            "   60% occlusion: mAP@50 = 0.024 (+96.7% vs baseline)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate degradation percentages\n",
        "print(\"\\nPerformance Degradation (compared to 0% baseline):\\n\")\n",
        "\n",
        "for model in ['YOLO', 'RTDETR']:\n",
        "    model_results = df[df['Model'] == model]\n",
        "    baseline = model_results[model_results['Occlusion'] == '0%']['mAP@50'].values[0]\n",
        "\n",
        "    print(f\"{model}:\")\n",
        "    for _, row in model_results.iterrows():\n",
        "        if row['Occlusion'] == '0%':\n",
        "            continue\n",
        "        degradation = ((baseline - row['mAP@50']) / baseline) * 100\n",
        "        print(f\"  {row['Occlusion']:>4} occlusion: mAP@50 = {row['mAP@50']:.3f} ({degradation:+.1f}% vs baseline)\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EDfY9GOMlzr"
      },
      "source": [
        "## 10. Download Results to Google Drive (Colab Only)\n",
        "\n",
        "Save all JSON files for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju6Pe3lPMlzr",
        "outputId": "90da6296-b780-4760-e80d-0934eefd3c1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úì Saved 41 JSON files to Google Drive\n",
            "Location: /content/drive/MyDrive/Deep_Learning_Occlusion_Results\n"
          ]
        }
      ],
      "source": [
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Create backup directory\n",
        "    backup_dir = Path('/content/drive/MyDrive/Deep_Learning_Occlusion_Results')\n",
        "    backup_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Copy all JSON files\n",
        "    import shutil\n",
        "    for json_file in output_dir.glob('*.json'):\n",
        "        shutil.copy(json_file, backup_dir / json_file.name)\n",
        "\n",
        "    print(f\"‚úì Saved {len(list(output_dir.glob('*.json')))} JSON files to Google Drive\")\n",
        "    print(f\"Location: {backup_dir}\")\n",
        "else:\n",
        "    print(\"Running locally - JSON files already saved to evaluation/occlusion_metrics/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMzW4JSBMlzr"
      },
      "source": [
        "## 11. Summary & Next Steps\n",
        "\n",
        "**What We Just Did:**\n",
        "1. ‚úÖ Generated 4 synthetic occlusion test sets (0%, 20%, 40%, 60%)\n",
        "2. ‚úÖ Evaluated YOLOv8 with fixed weights on all 4 levels\n",
        "3. ‚úÖ Evaluated RT-DETR with fixed weights on all 4 levels\n",
        "4. ‚úÖ Generated 24 JSON files (2 models √ó 4 levels √ó 3 file types)\n",
        "\n",
        "**Expected Results:**\n",
        "- Both models should degrade as occlusion increases\n",
        "- **RT-DETR should degrade LESS** than YOLOv8 at higher occlusion levels\n",
        "- This validates the hypothesis: Transformers handle occlusion better\n",
        "\n",
        "**Next Steps:**\n",
        "1. Analyze degradation curves in detail\n",
        "2. Compute per-class robustness (which ingredients are most affected?)\n",
        "3. Visualize predictions on occluded images\n",
        "4. Write up findings for project report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96_Oz-lkMlzs",
        "outputId": "8b42ac28-bea3-4ace-d185-2b31db3961b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "OCCLUSION ROBUSTNESS EVALUATION COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "Generated 24 JSON files in: evaluation/metrics\n",
            "\n",
            "JSON files ready for analysis:\n",
            "  - Run metadata: Reproducibility info\n",
            "  - Metrics: mAP, precision, recall, FPS\n",
            "  - Predictions: Per-image detections for detailed analysis\n",
            "\n",
            "You can now:\n",
            "  1. Commit JSON files to GitHub\n",
            "  2. Analyze degradation curves\n",
            "  3. Compare CNN vs Transformer robustness\n",
            "  4. Generate visualizations for report\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"OCCLUSION ROBUSTNESS EVALUATION COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nGenerated {total_files} JSON files in: {output_dir}\")\n",
        "print(\"\\nJSON files ready for analysis:\")\n",
        "print(\"  - Run metadata: Reproducibility info\")\n",
        "print(\"  - Metrics: mAP, precision, recall, FPS\")\n",
        "print(\"  - Predictions: Per-image detections for detailed analysis\")\n",
        "print(\"\\nYou can now:\")\n",
        "print(\"  1. Commit JSON files to GitHub\")\n",
        "print(\"  2. Analyze degradation curves\")\n",
        "print(\"  3. Compare CNN vs Transformer robustness\")\n",
        "print(\"  4. Generate visualizations for report\")\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}