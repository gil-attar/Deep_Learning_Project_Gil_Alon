{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E2 Test & Evaluation\n",
    "\n",
    "Experiment 2 (Option A): **Training time (epochs) vs performance** (no early stopping)\n",
    "\n",
    "We sweep epoch budgets **E \u2208 {5, 10, 20, 40, 80}** with a **fixed freezing configuration** (default: F2) for each model (YOLOv8m vs RT-DETR-L).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Clone Repo to Colab /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if in Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab\")\n",
    "    # Clone repo if not already cloned\n",
    "    import os\n",
    "    if not os.path.exists('Deep_Learning_Gil_Alon'):\n",
    "        !git clone https://github.com/gil-attar/Deep_Learning_Project_Gil_Alon.git Deep_Learning_Gil_Alon\n",
    "    %cd Deep_Learning_Gil_Alon\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    # Navigate to project root if in notebooks/\n",
    "    if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "        os.chdir('..')\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Mounting Google Drive & Setting up Folder Structure for Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for saving run results\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_NAME = \"Deep_Learning_Project_Gil_Alon\"\n",
    "\n",
    "# IMPORTANT:\n",
    "# - For a fresh folder each time, keep the timestamp.\n",
    "# - To continue the SAME sweep after a disconnect, set RUN_ID to a fixed string\n",
    "#   (e.g., RUN_ID=\"E2_fullsweep_v1\") and reuse it after reconnect.\n",
    "RUN_ID = time.strftime(\"E2_%Y%m%d_%H%M%S\")\n",
    "\n",
    "DRIVE_ROOT = Path(\"/content/drive/MyDrive/Colab_Outputs\") / PROJECT_NAME / RUN_ID\n",
    "\n",
    "PERSIST_E2_RUNS = DRIVE_ROOT / \"E2_runs\"       # where Experiment 2 outputs will live\n",
    "PERSIST_WEIGHTS = DRIVE_ROOT / \"pretrained\"    # optional cache for pretrained weights\n",
    "\n",
    "PERSIST_E2_RUNS.mkdir(parents=True, exist_ok=True)\n",
    "PERSIST_WEIGHTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Drive root:\", DRIVE_ROOT)\n",
    "print(\"E2 runs:\", PERSIST_E2_RUNS)\n",
    "print(\"Weights cache:\", PERSIST_WEIGHTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Symlink the repo\u2019s runs/ to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "REPO = Path.cwd()\n",
    "print(\"REPO =\", REPO)\n",
    "\n",
    "E2_RUNS_IN_REPO = REPO / \"experiments\" / \"Experiment_2\" / \"runs\"\n",
    "\n",
    "# Safety check\n",
    "assert REPO.exists(), f\"Repo path does not exist: {REPO}\"\n",
    "\n",
    "# Remove local runs dir if it exists, then link to Drive\n",
    "!rm -rf \"{E2_RUNS_IN_REPO}\"\n",
    "!ln -s \"{PERSIST_E2_RUNS}\" \"{E2_RUNS_IN_REPO}\"\n",
    "\n",
    "print(\"Symlink created:\")\n",
    "!ls -la \"{E2_RUNS_IN_REPO}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.4 Re-routing Model Weights to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_IN_REPO = REPO / \"artifacts\" / \"weights\"\n",
    "\n",
    "!rm -rf \"{WEIGHTS_IN_REPO}\"\n",
    "!ln -s \"{PERSIST_WEIGHTS}\" \"{WEIGHTS_IN_REPO}\"\n",
    "\n",
    "print(\"Weights dir now points to Drive:\")\n",
    "!ls -la \"{WEIGHTS_IN_REPO}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q ultralytics roboflow pyyaml pillow numpy matplotlib pandas tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Roboflow API key\n",
    "import os\n",
    "os.environ[\"ROBOFLOW_API_KEY\"] = \"zEF9icmDY2oTcPkaDcQY\"  # Your API key\n",
    "\n",
    "# Download dataset\n",
    "!python scripts/download_dataset.py --output_dir data/raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset downloaded\n",
    "!echo \"Train images: $(ls data/raw/train/images/ 2>/dev/null | wc -l)\"\n",
    "!echo \"Valid images: $(ls data/raw/valid/images/ 2>/dev/null | wc -l)\"\n",
    "!echo \"Test images: $(ls data/raw/test/images/ 2>/dev/null | wc -l)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fetch COCO-Pretrained Weights (YOLOv8m + RT-DETR-L)\n",
    "\n",
    "Will be stored under `artifacts/weights/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch pretrained weights (idempotent)\n",
    "!bash scripts/fetch_weights.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Evaluation Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train/val/test indices (with ACTUAL image dimensions - this is critical!)\n",
    "# Remove old indices first to ensure fresh rebuild\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "if Path(\"data/processed/evaluation\").exists():\n",
    "    shutil.rmtree(\"data/processed/evaluation\")\n",
    "    print(\"\u2713 Removed old indices\")\n",
    "\n",
    "!python scripts/build_evaluation_indices.py \\\n",
    "    --dataset_root data/raw \\\n",
    "    --output_dir data/processed/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify indices created\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "test_index_path = \"data/processed/evaluation/test_index.json\"\n",
    "\n",
    "if Path(test_index_path).exists():\n",
    "    with open(test_index_path) as f:\n",
    "        test_data = json.load(f)\n",
    "    print(f\"\u2713 Test index: {test_data['metadata']['num_images']} images\")\n",
    "    print(f\"  Total objects: {test_data['metadata']['total_objects']}\")\n",
    "    print(f\"  Classes: {test_data['metadata']['num_classes']}\")\n",
    "else:\n",
    "    print(f\"\u274c Test index not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Create data.yaml for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data.yaml with absolute paths for Colab\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Get absolute path to dataset\n",
    "dataset_root = Path('data/raw').resolve()\n",
    "\n",
    "# Read original data.yaml to get class names\n",
    "with open(dataset_root / 'data.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Create config with ABSOLUTE paths\n",
    "train_config = {\n",
    "    'path': str(dataset_root),  # Absolute base path\n",
    "    'train': 'train/images',\n",
    "    'val': 'valid/images', \n",
    "    'test': 'test/images',\n",
    "    'names': config['names'],\n",
    "    'nc': len(config['names'])\n",
    "}\n",
    "\n",
    "# Save to data/processed/\n",
    "output_path = Path('data/processed/data.yaml')\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    yaml.dump(train_config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"\u2713 Created data.yaml with absolute paths\")\n",
    "print(f\"  Path: {train_config['path']}\")\n",
    "print(f\"  Classes: {train_config['nc']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Experiment 2 (Epoch Budget Sweep: YOLOv8m vs RT-DETR-L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E2 sweep configuration\n",
    "DRY_RUN = True   # True: quick smoke test (E=1). False: full sweep (E in {5,10,20,40,80}).\n",
    "\n",
    "FREEZE = \"F2\"            # fixed freeze preset (see experiments/Experiment_1/freezing/freeze_presets.py)\n",
    "EPOCHS_LIST = \"1\" if DRY_RUN else \"5 10 20 40 80\"\n",
    "IMGSZ = 640\n",
    "SEED = 42\n",
    "\n",
    "# Reporting threshold used inside the E2 runner for per-class/counting metrics.\n",
    "# NOTE: the bash script must pass this through to the runner if you change it.\n",
    "REPORT_CONF = 0.25\n",
    "\n",
    "print(f\"DRY_RUN={DRY_RUN} | FREEZE={FREEZE} | EPOCHS_LIST={EPOCHS_LIST} | IMGSZ={IMGSZ} | SEED={SEED} | REPORT_CONF={REPORT_CONF}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Experiment 2\n",
    "# This calls experiments/Experiment_2/runOneTest.py for each (model, epochs) with fixed FREEZE.\n",
    "\n",
    "# If you later update run_experiment2.sh to forward REPORT_CONF, add: REPORT_CONF={REPORT_CONF}\n",
    "!FREEZE={FREEZE} EPOCHS_LIST=\"{EPOCHS_LIST}\" IMGSZ={IMGSZ} SEED={SEED} bash experiments/Experiment_2/run_experiment2.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Run Outputs (E2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "runs_root = Path(\"experiments/Experiment_2/runs\")\n",
    "assert runs_root.exists(), f\"Missing runs root: {runs_root}\"\n",
    "\n",
    "expected_files = [\n",
    "    \"run_manifest.json\",\n",
    "    \"train_summary.json\",\n",
    "    \"predictions/test_predictions.json\",\n",
    "    \"eval/test/metrics.json\",\n",
    "    \"eval/test/summary.csv\",\n",
    "    \"run_summary.json\",\n",
    "]\n",
    "\n",
    "missing = []\n",
    "run_dirs = sorted([p for p in runs_root.glob(\"**/E*\") if p.is_dir()])\n",
    "print(f\"Found {len(run_dirs)} epoch-budget run directories\")\n",
    "\n",
    "for rd in run_dirs:\n",
    "    for ef in expected_files:\n",
    "        if not (rd / ef).exists():\n",
    "            missing.append((str(rd), ef))\n",
    "\n",
    "if missing:\n",
    "    print(\"WARNING: missing expected artifacts in some runs:\")\n",
    "    for rd, ef in missing[:60]:\n",
    "        print(f\"  - {rd} :: {ef}\")\n",
    "else:\n",
    "    print(\"\u2713 All runs contain the expected artifacts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Aggregate Metrics Across Runs (E2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "runs_root = Path(\"experiments/Experiment_2/runs\")\n",
    "\n",
    "rows = []\n",
    "for summary_path in runs_root.glob(\"**/run_summary.json\"):\n",
    "    j = json.loads(summary_path.read_text())\n",
    "    m = j.get(\"manifest\", {})\n",
    "    model = m.get(\"model\")\n",
    "    freeze_id = m.get(\"freeze_id\")\n",
    "    epochs = int(m.get(\"epochs\"))\n",
    "\n",
    "    timing = (j.get(\"train\", {}) or {}).get(\"timing\", {})\n",
    "    wall = timing.get(\"train_wall_time_seconds\")\n",
    "    spe = timing.get(\"train_seconds_per_epoch\")\n",
    "\n",
    "    # Prefer evaluator summary.csv for a single reported score (fixed threshold)\n",
    "    eval_dir = Path((j.get(\"eval\", {}) or {}).get(\"test\", {}).get(\"eval_dir\", \"\"))\n",
    "    summary_csv = eval_dir / \"summary.csv\" if eval_dir else None\n",
    "\n",
    "    score = None\n",
    "    score_col = None\n",
    "    if summary_csv and summary_csv.exists():\n",
    "        df = pd.read_csv(summary_csv)\n",
    "        # Try common column candidates in your evaluator outputs\n",
    "        candidates = [\n",
    "            \"mAP50\", \"map50\", \"mAP@0.5\", \"mAP_0.5\",\n",
    "            \"f1\", \"F1\", \"precision\", \"recall\",\n",
    "        ]\n",
    "        for c in candidates:\n",
    "            if c in df.columns:\n",
    "                score_col = c\n",
    "                score = float(df[c].max())\n",
    "                break\n",
    "        if score is None:\n",
    "            # fallback: pick first numeric column (excluding ids)\n",
    "            numeric_cols = [c for c in df.columns if c.lower() not in {\"class\", \"class_id\", \"name\"}]\n",
    "            for c in numeric_cols:\n",
    "                try:\n",
    "                    score = float(pd.to_numeric(df[c], errors=\"coerce\").dropna().max())\n",
    "                    score_col = c\n",
    "                    break\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    # Secondary fallback: Ultralytics results.csv last-epoch val mAP50 if present\n",
    "    if score is None:\n",
    "        results_csv = summary_path.parent / \"results.csv\"\n",
    "        if results_csv.exists():\n",
    "            rdf = pd.read_csv(results_csv)\n",
    "            rdf.columns = [c.strip() for c in rdf.columns]\n",
    "            for c in [\"metrics/mAP50(B)\", \"metrics/mAP50\", \"metrics/mAP_0.5\"]:\n",
    "                if c in rdf.columns:\n",
    "                    score = float(rdf[c].iloc[-1])\n",
    "                    score_col = c\n",
    "                    break\n",
    "\n",
    "    rows.append({\n",
    "        \"model\": model,\n",
    "        \"freeze_id\": freeze_id,\n",
    "        \"epochs\": epochs,\n",
    "        \"train_wall_s\": wall,\n",
    "        \"sec_per_epoch\": spe,\n",
    "        \"score\": score,\n",
    "        \"score_col\": score_col,\n",
    "        \"run_dir\": str(summary_path.parent),\n",
    "    })\n",
    "\n",
    "agg = pd.DataFrame(rows)\n",
    "agg = agg.sort_values([\"model\", \"epochs\"]).reset_index(drop=True)\n",
    "print(\"Runs found:\", len(agg))\n",
    "display(agg)\n",
    "\n",
    "out_csv = Path(\"experiments/Experiment_2/runs/_aggregate_e2.csv\")\n",
    "out_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "agg.to_csv(out_csv, index=False)\n",
    "print(\"Saved:\", out_csv.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# This cell summarizes each E* run directory using:\n",
    "# - run_manifest.json (trainable params, settings)\n",
    "# - eval/test/metrics.json (test-side evaluator metrics)\n",
    "# - predictions/test_predictions.json (inference timing)\n",
    "# - results.csv (per-epoch Ultralytics log, if present)\n",
    "\n",
    "def _safe_get(d, keys, default=None):\n",
    "    cur = d\n",
    "    for k in keys:\n",
    "        if not isinstance(cur, dict) or k not in cur:\n",
    "            return default\n",
    "        cur = cur[k]\n",
    "    return cur\n",
    "\n",
    "def auc_trapz(x, y):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    if len(x) < 2:\n",
    "        return np.nan\n",
    "    return float(np.trapz(y, x))\n",
    "\n",
    "def get_map50_col(ep_df: pd.DataFrame):\n",
    "    # Common Ultralytics mAP50 column names\n",
    "    candidates = [\n",
    "        \"metrics/mAP50(B)\",\n",
    "        \"metrics/mAP50\",\n",
    "        \"metrics/mAP_0.5\",\n",
    "        \"metrics/mAP@0.5\",\n",
    "        \"metrics/mAP50-95(B)\",  # fallback (not mAP50 but better than nothing)\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if c in ep_df.columns:\n",
    "            return c\n",
    "    # Last resort: any column containing 'mAP50'\n",
    "    for c in ep_df.columns:\n",
    "        if \"map50\" in c.lower():\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def get_train_loss_cols(ep_df: pd.DataFrame):\n",
    "    # YOLO typically: train/box_loss, train/cls_loss, train/dfl_loss\n",
    "    return [c for c in ep_df.columns if c.startswith('train/') and 'loss' in c.lower()]\n",
    "\n",
    "def get_val_loss_cols(ep_df: pd.DataFrame):\n",
    "    # YOLO typically: val/box_loss, val/cls_loss, val/dfl_loss\n",
    "    return [c for c in ep_df.columns if c.startswith('val/') and 'loss' in c.lower()]\n",
    "\n",
    "rows = []\n",
    "for rd in run_dirs:  # run_dirs should be E* directories (see prior cell)\n",
    "    rd = Path(rd)\n",
    "\n",
    "    manifest = json.loads((rd / \"run_manifest.json\").read_text())\n",
    "    test_metrics = json.loads((rd / \"eval/test/metrics.json\").read_text())\n",
    "    preds_test = json.loads((rd / \"predictions/test_predictions.json\").read_text())\n",
    "\n",
    "    model = manifest.get('model')\n",
    "    freeze_id = manifest.get('freeze_id')\n",
    "    epochs = manifest.get('epochs')\n",
    "\n",
    "    # Per-epoch log (Ultralytics)\n",
    "    ep = None\n",
    "    results_csv = rd / \"results.csv\"\n",
    "    if results_csv.exists():\n",
    "        ep = pd.read_csv(results_csv)\n",
    "        ep.columns = [c.strip() for c in ep.columns]\n",
    "        if \"epoch\" not in ep.columns:\n",
    "            ep = ep.reset_index().rename(columns={\"index\": \"epoch\"})\n",
    "\n",
    "    map50_col = None\n",
    "    best_val_map50 = np.nan\n",
    "    best_epoch = np.nan\n",
    "    epoch_to_90pct = np.nan\n",
    "    auc_map50 = np.nan\n",
    "\n",
    "    train_loss_cols, val_loss_cols = [], []\n",
    "    best_train_loss = np.nan\n",
    "    best_val_loss = np.nan\n",
    "    gen_gap_at_best = np.nan\n",
    "\n",
    "    if ep is not None and len(ep) > 0:\n",
    "        map50_col = get_map50_col(ep)\n",
    "        if map50_col is not None and map50_col in ep.columns and \"epoch\" in ep.columns:\n",
    "            y = pd.to_numeric(ep[map50_col], errors='coerce').to_numpy(dtype=float)\n",
    "            x = pd.to_numeric(ep[\"epoch\"], errors='coerce').to_numpy(dtype=float)\n",
    "\n",
    "            if np.isfinite(y).any():\n",
    "                best_idx = int(np.nanargmax(y))\n",
    "                best_epoch = int(ep.loc[best_idx, \"epoch\"]) if np.isfinite(ep.loc[best_idx, \"epoch\"]) else np.nan\n",
    "                best_val_map50 = float(np.nanmax(y))\n",
    "                auc_map50 = auc_trapz(x, y)\n",
    "\n",
    "                target = 0.9 * best_val_map50\n",
    "                idxs = np.where(y >= target)[0]\n",
    "                if len(idxs) > 0:\n",
    "                    epoch_to_90pct = int(ep.loc[int(idxs[0]), \"epoch\"])\n",
    "\n",
    "        # Loss summaries + generalization gap (if val losses exist)\n",
    "        train_loss_cols = get_train_loss_cols(ep)\n",
    "        val_loss_cols = get_val_loss_cols(ep)\n",
    "\n",
    "        if train_loss_cols:\n",
    "            ep[\"_train_total_loss\"] = ep[train_loss_cols].apply(pd.to_numeric, errors='coerce').sum(axis=1)\n",
    "        if val_loss_cols:\n",
    "            ep[\"_val_total_loss\"] = ep[val_loss_cols].apply(pd.to_numeric, errors='coerce').sum(axis=1)\n",
    "\n",
    "        if (train_loss_cols and val_loss_cols and not (isinstance(best_epoch, float) and math.isnan(best_epoch))):\n",
    "            row = ep[ep[\"epoch\"] == best_epoch]\n",
    "            if len(row) == 0 and map50_col is not None and map50_col in ep.columns:\n",
    "                y = pd.to_numeric(ep[map50_col], errors='coerce').to_numpy(dtype=float)\n",
    "                row = ep.iloc[[int(np.nanargmax(y))]]\n",
    "\n",
    "            if len(row) > 0:\n",
    "                best_train_loss = float(row[\"_train_total_loss\"].iloc[0])\n",
    "                best_val_loss = float(row[\"_val_total_loss\"].iloc[0])\n",
    "                gen_gap_at_best = float(best_val_loss - best_train_loss)\n",
    "\n",
    "    # Evaluator primary metric (kept; mostly for final test reporting)\n",
    "    def extract_primary_metric(metrics_json: dict):\n",
    "        candidates = [\n",
    "            (\"overall_f1\", [\"overall\", \"f1\"]),\n",
    "            (\"overall_precision\", [\"overall\", \"precision\"]),\n",
    "            (\"overall_recall\", [\"overall\", \"recall\"]),\n",
    "            (\"f1\", [\"f1\"]),\n",
    "            (\"precision\", [\"precision\"]),\n",
    "            (\"recall\", [\"recall\"]),\n",
    "            (\"map50\", [\"map50\"]),\n",
    "            (\"map\", [\"map\"]),\n",
    "        ]\n",
    "        for name, path in candidates:\n",
    "            val = _safe_get(metrics_json, path)\n",
    "            if isinstance(val, (int, float)) and not (isinstance(val, float) and math.isnan(val)):\n",
    "                return name, float(val)\n",
    "        for k, v in metrics_json.items():\n",
    "            if isinstance(v, (int, float)):\n",
    "                return str(k), float(v)\n",
    "        return None, None\n",
    "\n",
    "    primary_metric_name, primary_metric_val = extract_primary_metric(test_metrics)\n",
    "\n",
    "    rows.append({\n",
    "        \"model\": model,\n",
    "        \"freeze_id\": freeze_id,\n",
    "        \"epochs\": epochs,\n",
    "\n",
    "        \"trainable_params\": _safe_get(manifest, [\"param_counts\", \"trainable\"]),\n",
    "        \"total_params\": _safe_get(manifest, [\"param_counts\", \"total\"]),\n",
    "\n",
    "        # Per-epoch (Ultralytics)\n",
    "        \"map50_col\": map50_col,\n",
    "        \"best_val_map50\": best_val_map50,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"epoch_to_90pct_best\": epoch_to_90pct,\n",
    "        \"auc_val_map50\": auc_map50,\n",
    "\n",
    "        \"best_train_loss_at_best_epoch\": best_train_loss,\n",
    "        \"best_val_loss_at_best_epoch\": best_val_loss,\n",
    "        \"gen_gap_val_minus_train_at_best\": gen_gap_at_best,\n",
    "\n",
    "        # Evaluator final (test-side; depends on your evaluation contract)\n",
    "        \"primary_metric_name\": primary_metric_name,\n",
    "        \"primary_metric_test\": primary_metric_val,\n",
    "\n",
    "        \"avg_inference_time_ms\": _safe_get(preds_test, [\"timing\", \"avg_inference_time_ms\"]),\n",
    "        \"num_images\": _safe_get(preds_test, [\"timing\", \"num_images\"]),\n",
    "\n",
    "        \"run_dir\": str(rd),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values([\"model\", \"freeze_id\", \"epochs\"]).reset_index(drop=True)\n",
    "display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plots (E2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "PLOTS_DIR = Path(\"experiments/Experiment_2/runs\") / \"_plots\"\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Reload aggregate if needed\n",
    "import pandas as pd\n",
    "agg_path = Path(\"experiments/Experiment_2/runs/_aggregate_e2.csv\")\n",
    "agg = pd.read_csv(agg_path) if agg_path.exists() else agg\n",
    "\n",
    "def savefig(name: str):\n",
    "    png = PLOTS_DIR / f\"{name}.png\"\n",
    "    pdf = PLOTS_DIR / f\"{name}.pdf\"\n",
    "    plt.savefig(png, bbox_inches=\"tight\", dpi=200)\n",
    "    plt.savefig(pdf, bbox_inches=\"tight\")\n",
    "    print(f\"Saved: {png} and {pdf}\")\n",
    "\n",
    "# Plot 1: score vs epochs (per model)\n",
    "plt.figure(figsize=(7,4))\n",
    "for model, g in agg.groupby(\"model\"):\n",
    "    g = g.sort_values(\"epochs\")\n",
    "    plt.plot(g[\"epochs\"], g[\"score\"], marker=\"o\", label=model)\n",
    "plt.xlabel(\"Epoch budget\")\n",
    "plt.ylabel(\"Performance score (see score_col)\")\n",
    "plt.title(\"E2: Performance vs Epoch Budget (fixed freeze)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "savefig(\"perf_vs_epochs\")\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: wall time vs epochs (per model)\n",
    "plt.figure(figsize=(7,4))\n",
    "for model, g in agg.groupby(\"model\"):\n",
    "    g = g.sort_values(\"epochs\")\n",
    "    plt.plot(g[\"epochs\"], g[\"train_wall_s\"], marker=\"o\", label=model)\n",
    "plt.xlabel(\"Epoch budget\")\n",
    "plt.ylabel(\"Train wall-clock time (s)\")\n",
    "plt.title(\"E2: Training Time vs Epoch Budget\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "savefig(\"time_vs_epochs\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Plots saved under:\", PLOTS_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inspect One Run (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import Image, display\n",
    "\n",
    "runs_root = Path(\"experiments/Experiment_2/runs\")\n",
    "# pick one run dir that has plots\n",
    "cands = list(runs_root.glob(\"**/eval/test/plots/*.png\"))\n",
    "if not cands:\n",
    "    print(\"No plot PNGs found under eval/test/plots (this is ok if plot generation is disabled).\")\n",
    "else:\n",
    "    p = sorted(cands)[0]\n",
    "    print(\"Displaying:\", p)\n",
    "    display(Image(filename=str(p)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "1. Clones the repo and mounts Google Drive.\n",
    "2. Downloads the dataset (Roboflow) and builds evaluation indices.\n",
    "3. Writes an absolute-path `data/processed/data.yaml` for Ultralytics.\n",
    "4. Runs **Experiment 2** epoch-budget sweep via `experiments/Experiment_2/run_experiment2.sh`.\n",
    "5. Verifies expected artifacts and produces E2 plots.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}