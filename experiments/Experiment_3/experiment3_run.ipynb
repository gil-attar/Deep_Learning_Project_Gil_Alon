{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: Internal Masking vs Occlusion Training\n",
    "\n",
    "**Goal:** Test whether internal channel masking during training can improve robustness to occlusions, as an alternative to training on occluded images.\n",
    "\n",
    "**Sessions (6 per model):**\n",
    "- S1: Clean train (baseline)\n",
    "- S2: Occluded train (standard practice)\n",
    "- S3: Clean train + mask backbone_early\n",
    "- S4: Clean train + mask backbone_late\n",
    "- S5: Clean train + mask neck\n",
    "- S6: Clean train + mask head\n",
    "\n",
    "**Evaluation:**\n",
    "- All models tested on both `test_clean` and `test_occluded` (40%)\n",
    "- Using our custom evaluation system (P/R/F1, per-class, counting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Configuration\n",
    "\n",
    "**EDIT THIS CELL to switch between smoke test and full run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# EXPERIMENT 3 CONFIGURATION - EDIT THIS CELL ONLY\n# ============================================================\n\n# EPOCHS: Set to 1 for smoke test, 50 for full experiment\nEPOCHS = 1  # <-- CHANGE THIS: 1 = smoke test, 50 = full run\n\n# Models to run (both for full experiment, one for quick test)\nMODELS = [\"yolov8m\", \"rtdetr-l\"]\n\n# Sessions to run (all 6 for full, fewer for quick test)\nSESSIONS_TO_RUN = [\"S1_clean_train\", \"S2_occ_train\", \"S3_mask_backbone_early\", \n                   \"S4_mask_backbone_late\", \"S5_mask_neck\", \"S6_mask_head\"]\n\n# Masking parameters (fixed for all sessions)\nP_APPLY = 0.5      # Probability of applying masking per batch\nP_CHANNELS = 0.2   # Fraction of channels to zero when masking\n\n# Training parameters\nIMGSZ = 640\nBATCH = -1  # Auto batch size\nPATIENCE = 10  # Early stopping patience\nSEED = 42  # Fixed seed for reproducibility\n\n# Occlusion level for test\nOCCLUSION_LEVEL = \"level_040\"  # 40% occlusion\n\n# ============================================================\n# GOOGLE DRIVE - RUN_ID for resume capability\n# ============================================================\n# IMPORTANT: Smoke test and full run should use DIFFERENT RUN_IDs!\n#\n# Option 1: Automatic (recommended)\n#   - Each run gets a unique timestamp-based ID\n#   - Smoke test won't interfere with full run\n#\n# Option 2: Manual resume\n#   - Set RUN_ID to a specific string to continue a previous run\n#\nimport time\nif EPOCHS <= 1:\n    # Smoke test: use a fixed ID so it's easy to delete/ignore\n    RUN_ID = \"E3_SMOKE_TEST\"\nelse:\n    # Full run: use timestamp for unique ID\n    RUN_ID = time.strftime(\"E3_%Y%m%d_%H%M%S\")\n\n# To RESUME a specific run, uncomment and set:\n# RUN_ID = \"E3_20240115_143022\"  # <-- paste your RUN_ID here\n\n# ============================================================\nprint(f\"Configuration:\")\nprint(f\"  EPOCHS: {EPOCHS} {'(SMOKE TEST)' if EPOCHS <= 1 else '(FULL RUN)'}\")\nprint(f\"  MODELS: {MODELS}\")\nprint(f\"  SESSIONS: {len(SESSIONS_TO_RUN)} sessions\")\nprint(f\"  Masking: p_apply={P_APPLY}, p_channels={P_CHANNELS}\")\nprint(f\"  RUN_ID: {RUN_ID}\")\nprint()\nif EPOCHS <= 1:\n    print(\"NOTE: Smoke test uses RUN_ID='E3_SMOKE_TEST'\")\n    print(\"      This will NOT interfere with your full 50-epoch run.\")\n    print(\"      Full run will get a unique timestamp-based RUN_ID.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Mount Google Drive (FIRST!)\n",
    "\n",
    "**IMPORTANT:** Drive is mounted at the very beginning so you can approve permissions and then leave the computer running overnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if in Colab and mount Drive FIRST\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # MOUNT GOOGLE DRIVE FIRST - This will prompt for permission\n",
    "    # ============================================================\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted successfully!\")\n",
    "    \n",
    "    # Clone repo if not already cloned\n",
    "    if not os.path.exists('/content/Deep_Learning_Gil_Alon'):\n",
    "        !git clone https://github.com/gil-attar/Deep_Learning_Project_Gil_Alon.git Deep_Learning_Gil_Alon\n",
    "    os.chdir('/content/Deep_Learning_Gil_Alon')\n",
    "else:\n",
    "    print(\"Running locally - Drive backup not needed\")\n",
    "    if 'Experiment_3' in os.getcwd():\n",
    "        os.chdir('../..')\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Setup Drive Folder Structure (Same as Experiment 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\n\nPROJECT_NAME = \"Deep_Learning_Project_Gil_Alon\"\n\nif IN_COLAB:\n    # Setup Drive paths (same structure as Experiment 1)\n    DRIVE_ROOT = Path(\"/content/drive/MyDrive/Colab_Outputs\") / PROJECT_NAME / RUN_ID\n    PERSIST_E3_RUNS = DRIVE_ROOT / \"E3_runs\"  # Training outputs go here\n    \n    PERSIST_E3_RUNS.mkdir(parents=True, exist_ok=True)\n    \n    # ============================================================\n    # SAVE RUN_ID TO DRIVE (so you can find it later to resume)\n    # ============================================================\n    run_id_file = DRIVE_ROOT / \"RUN_ID.txt\"\n    run_id_file.write_text(f\"{RUN_ID}\\n\\nTo resume after disconnect:\\n1. Set RUN_ID = \\\"{RUN_ID}\\\" in config cell\\n2. Re-run all cells\\n\")\n    \n    # Also save to a \"latest\" file for easy lookup\n    latest_file = Path(\"/content/drive/MyDrive/Colab_Outputs\") / PROJECT_NAME / \"LATEST_E3_RUN_ID.txt\"\n    latest_file.write_text(f\"{RUN_ID}\")\n    \n    print(f\"Drive root: {DRIVE_ROOT}\")\n    print(f\"E3 runs will be saved to: {PERSIST_E3_RUNS}\")\n    print(f\"\")\n    print(f\"========================================\")\n    print(f\"RUN_ID saved to: {run_id_file}\")\n    print(f\"Latest run ID: {latest_file}\")\n    print(f\"========================================\")\n    \n    # Symlink local runs/ to Drive (so training writes directly to Drive)\n    REPO = Path.cwd()\n    E3_RUNS_IN_REPO = REPO / \"runs\" / \"exp3\"\n    \n    # Remove local dir if exists, then symlink to Drive\n    !rm -rf \"{E3_RUNS_IN_REPO}\"\n    E3_RUNS_IN_REPO.parent.mkdir(parents=True, exist_ok=True)\n    !ln -s \"{PERSIST_E3_RUNS}\" \"{E3_RUNS_IN_REPO}\"\n    \n    print(f\"\\nSymlink created: {E3_RUNS_IN_REPO} -> {PERSIST_E3_RUNS}\")\nelse:\n    PERSIST_E3_RUNS = None\n    print(\"Running locally - no Drive symlink needed\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q ultralytics roboflow pyyaml pillow numpy matplotlib pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "import json\n",
    "import shutil\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from ultralytics import YOLO, RTDETR\n",
    "\n",
    "# Import our experiment modules\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "from experiments.Experiment_3.mask_presets import (\n",
    "    get_mask_prefixes, get_session_config, SESSIONS\n",
    ")\n",
    "from experiments.Experiment_3.channel_masking import MaskingManager\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset if not exists\n",
    "if not Path(\"data/raw/train/images\").exists():\n",
    "    os.environ[\"ROBOFLOW_API_KEY\"] = \"zEF9icmDY2oTcPkaDcQY\"\n",
    "    !python scripts/download_dataset.py --output_dir data/raw\n",
    "else:\n",
    "    print(\"Dataset already exists\")\n",
    "\n",
    "# Verify\n",
    "print(f\"Train images: {len(list(Path('data/raw/train/images').glob('*')))}\")\n",
    "print(f\"Val images: {len(list(Path('data/raw/valid/images').glob('*')))}\")\n",
    "print(f\"Test images: {len(list(Path('data/raw/test/images').glob('*')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Evaluation Indices & Generate Occluded Data\n",
    "\n",
    "**IMPORTANT:** All sessions use the SAME dataset splits and SAME occluded test images (fixed seed=42)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build evaluation indices (same for ALL sessions)\n",
    "if not Path(\"data/processed/evaluation/test_index.json\").exists():\n",
    "    !python scripts/build_evaluation_indices.py \\\n",
    "        --dataset_root data/raw \\\n",
    "        --output_dir data/processed/evaluation\n",
    "else:\n",
    "    print(\"Evaluation indices already exist\")\n",
    "\n",
    "# Verify\n",
    "with open(\"data/processed/evaluation/test_index.json\") as f:\n",
    "    test_index = json.load(f)\n",
    "print(f\"Test set: {test_index['metadata']['num_images']} images, {test_index['metadata']['total_objects']} objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate occluded TRAINING data for S2 (seed=42 for reproducibility)\n",
    "occluded_train_dir = Path(\"data/occluded_train_040/level_040\")\n",
    "\n",
    "if not occluded_train_dir.exists():\n",
    "    print(\"Generating occluded training data (40% occlusion, seed=42)...\")\n",
    "    !python scripts/generate_synthetic_occlusions.py \\\n",
    "        --test_index data/processed/evaluation/train_index.json \\\n",
    "        --images_dir data/raw/train/images \\\n",
    "        --labels_dir data/raw/train/labels \\\n",
    "        --output_dir data/occluded_train_040 \\\n",
    "        --levels 0.4 \\\n",
    "        --seed 42\n",
    "else:\n",
    "    print(f\"Occluded training data already exists at {occluded_train_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate occluded TEST data (seed=42 - SAME for all sessions!)\n",
    "occluded_test_dir = Path(f\"data/synthetic_occlusion/{OCCLUSION_LEVEL}\")\n",
    "\n",
    "if not occluded_test_dir.exists():\n",
    "    print(f\"Generating occluded test data ({OCCLUSION_LEVEL}, seed=42)...\")\n",
    "    !python scripts/generate_synthetic_occlusions.py \\\n",
    "        --test_index data/processed/evaluation/test_index.json \\\n",
    "        --images_dir data/raw/test/images \\\n",
    "        --labels_dir data/raw/test/labels \\\n",
    "        --output_dir data/synthetic_occlusion \\\n",
    "        --levels 0.4 \\\n",
    "        --seed 42\n",
    "else:\n",
    "    print(f\"Occluded test data already exists at {occluded_test_dir}\")\n",
    "\n",
    "print(f\"\\nAll sessions will use:\")\n",
    "print(f\"  - Same train images: data/raw/train/\")\n",
    "print(f\"  - Same occluded train (S2 only): {occluded_train_dir}\")\n",
    "print(f\"  - Same test images: data/raw/test/\")\n",
    "print(f\"  - Same occluded test: {occluded_test_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Data YAML Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data.yaml files for clean and occluded training\n",
    "\n",
    "# Load class names from original data.yaml\n",
    "with open('data/raw/data.yaml', 'r') as f:\n",
    "    original_config = yaml.safe_load(f)\n",
    "\n",
    "# Clean training data.yaml (for S1, S3-S6)\n",
    "clean_config = {\n",
    "    'path': str(Path('data/raw').resolve()),\n",
    "    'train': 'train/images',\n",
    "    'val': 'valid/images',\n",
    "    'test': 'test/images',\n",
    "    'names': original_config['names'],\n",
    "    'nc': len(original_config['names'])\n",
    "}\n",
    "\n",
    "Path('data/processed').mkdir(parents=True, exist_ok=True)\n",
    "with open('data/processed/data_clean.yaml', 'w') as f:\n",
    "    yaml.dump(clean_config, f, default_flow_style=False)\n",
    "print(\"Created data/processed/data_clean.yaml\")\n",
    "\n",
    "# Occluded training data.yaml (for S2 only)\n",
    "occ_train_config = {\n",
    "    'path': str(Path('data').resolve()),\n",
    "    'train': 'occluded_train_040/level_040/images',\n",
    "    'val': 'raw/valid/images',\n",
    "    'test': 'raw/test/images',\n",
    "    'names': original_config['names'],\n",
    "    'nc': len(original_config['names'])\n",
    "}\n",
    "\n",
    "with open('data/processed/data_occ_train.yaml', 'w') as f:\n",
    "    yaml.dump(occ_train_config, f, default_flow_style=False)\n",
    "print(\"Created data/processed/data_occ_train.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name: str):\n",
    "    \"\"\"Load a model by name.\"\"\"\n",
    "    if 'yolo' in model_name.lower():\n",
    "        return YOLO(f\"{model_name}.pt\")\n",
    "    elif 'rtdetr' in model_name.lower():\n",
    "        return RTDETR(f\"{model_name}.pt\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "\n",
    "def get_model_type(model_name: str) -> str:\n",
    "    \"\"\"Get model type for mask presets.\"\"\"\n",
    "    if 'yolo' in model_name.lower():\n",
    "        return 'yolo'\n",
    "    elif 'rtdetr' in model_name.lower():\n",
    "        return 'rtdetr'\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "\n",
    "def is_session_complete(output_dir: Path, model_name: str, session_name: str) -> bool:\n",
    "    \"\"\"Check if a session is already complete (has DONE marker and weights).\"\"\"\n",
    "    run_dir = output_dir / f\"{model_name}__{session_name}\"\n",
    "    done_marker = run_dir / \"DONE\"\n",
    "    weights_file = run_dir / \"weights\" / \"best.pt\"\n",
    "    return done_marker.exists() and weights_file.exists()\n",
    "\n",
    "\n",
    "def train_session(\n",
    "    model_name: str,\n",
    "    session_name: str,\n",
    "    epochs: int,\n",
    "    output_dir: Path,\n",
    "    p_apply: float = 0.5,\n",
    "    p_channels: float = 0.2\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Train a single session with crash recovery.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with training results and paths\n",
    "    \"\"\"\n",
    "    session_config = get_session_config(session_name)\n",
    "    run_name = f\"{model_name}__{session_name}\"\n",
    "    run_dir = output_dir / run_name\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING: {run_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Description: {session_config['description']}\")\n",
    "    print(f\"Epochs: {epochs}\")\n",
    "    \n",
    "    # Check if already completed\n",
    "    if is_session_complete(output_dir, model_name, session_name):\n",
    "        print(f\"Session already completed. Skipping.\")\n",
    "        return {\"status\": \"skipped\", \"run_dir\": str(run_dir)}\n",
    "    \n",
    "    # Select data.yaml based on session\n",
    "    if session_config['train_data'] == 'occluded':\n",
    "        data_yaml = 'data/processed/data_occ_train.yaml'\n",
    "    else:\n",
    "        data_yaml = 'data/processed/data_clean.yaml'\n",
    "    \n",
    "    print(f\"Data: {data_yaml}\")\n",
    "    \n",
    "    # Load model\n",
    "    model = get_model(model_name)\n",
    "    \n",
    "    # Setup masking if needed\n",
    "    masking_manager = None\n",
    "    if session_config['mask_location'] is not None:\n",
    "        mask_location = session_config['mask_location']\n",
    "        model_type = get_model_type(model_name)\n",
    "        layer_prefixes = get_mask_prefixes(model_type, mask_location)\n",
    "        \n",
    "        print(f\"Masking: {mask_location} -> layers {layer_prefixes}\")\n",
    "        print(f\"Masking params: p_apply={p_apply}, p_channels={p_channels}\")\n",
    "        \n",
    "        masking_manager = MaskingManager(model.model, p_apply, p_channels)\n",
    "        num_hooks = masking_manager.add_masking_to_layers(layer_prefixes)\n",
    "        print(f\"Added {num_hooks} masking hooks\")\n",
    "    else:\n",
    "        print(\"Masking: None\")\n",
    "    \n",
    "    # Train\n",
    "    try:\n",
    "        results = model.train(\n",
    "            data=data_yaml,\n",
    "            epochs=epochs,\n",
    "            imgsz=IMGSZ,\n",
    "            batch=BATCH,\n",
    "            patience=PATIENCE,\n",
    "            save=True,\n",
    "            project=str(output_dir),\n",
    "            name=run_name,\n",
    "            exist_ok=True,\n",
    "            pretrained=True,\n",
    "            optimizer='auto',\n",
    "            verbose=True,\n",
    "            seed=SEED\n",
    "        )\n",
    "        \n",
    "        # Mark as done\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "        (run_dir / \"DONE\").touch()\n",
    "        \n",
    "        # Save masking summary if used\n",
    "        if masking_manager:\n",
    "            with open(run_dir / \"masking_summary.json\", 'w') as f:\n",
    "                json.dump(masking_manager.get_summary(), f, indent=2)\n",
    "        \n",
    "        print(f\"\\nTraining complete: {run_name}\")\n",
    "        print(f\"Saved to: {run_dir}\")\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"run_dir\": str(run_dir),\n",
    "            \"weights_path\": str(run_dir / \"weights\" / \"best.pt\")\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nTraining FAILED: {run_name}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        \n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "        (run_dir / \"FAILED\").write_text(str(e))\n",
    "        \n",
    "        return {\"status\": \"failed\", \"error\": str(e), \"run_dir\": str(run_dir)}\n",
    "    \n",
    "    finally:\n",
    "        if masking_manager:\n",
    "            masking_manager.remove_all_hooks()\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run All Training Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all training sessions\n",
    "# Output goes directly to Drive via symlink (crash-safe!)\n",
    "\n",
    "output_dir = Path(\"runs/exp3\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "if IN_COLAB:\n",
    "    print(f\"(Symlinked to Drive: {PERSIST_E3_RUNS})\")\n",
    "\n",
    "# Check for existing completed sessions\n",
    "print(\"\\nChecking for previously completed sessions...\")\n",
    "for model_name in MODELS:\n",
    "    for session_name in SESSIONS_TO_RUN:\n",
    "        if is_session_complete(output_dir, model_name, session_name):\n",
    "            print(f\"  Found: {model_name}__{session_name}\")\n",
    "\n",
    "# Run training\n",
    "training_results = []\n",
    "total_sessions = len(MODELS) * len(SESSIONS_TO_RUN)\n",
    "current = 0\n",
    "\n",
    "for model_name in MODELS:\n",
    "    for session_name in SESSIONS_TO_RUN:\n",
    "        current += 1\n",
    "        print(f\"\\n[{current}/{total_sessions}] {model_name} - {session_name}\")\n",
    "        \n",
    "        result = train_session(\n",
    "            model_name=model_name,\n",
    "            session_name=session_name,\n",
    "            epochs=EPOCHS,\n",
    "            output_dir=output_dir,\n",
    "            p_apply=P_APPLY,\n",
    "            p_channels=P_CHANNELS\n",
    "        )\n",
    "        \n",
    "        result['model'] = model_name\n",
    "        result['session'] = session_name\n",
    "        training_results.append(result)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for r in training_results:\n",
    "    status_icon = \"OK\" if r['status'] == 'success' else \"SKIP\" if r['status'] == 'skipped' else \"FAIL\"\n",
    "    print(f\"[{status_icon}] {r['model']}__{r['session']}\")\n",
    "\n",
    "# Save results\n",
    "with open(output_dir / \"training_results.json\", 'w') as f:\n",
    "    json.dump(training_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {output_dir / 'training_results.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation modules\n",
    "from evaluation.io import load_ground_truth, load_class_names\n",
    "from evaluation.metrics import (\n",
    "    eval_detection_prf_at_iou,\n",
    "    eval_per_class_metrics_and_confusions,\n",
    "    eval_counting_quality\n",
    ")\n",
    "from evaluation.plots import plot_all_metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def generate_predictions(model_path: str, test_images_dir: str, test_index: dict) -> list:\n",
    "    \"\"\"Generate predictions for a trained model on a test set.\"\"\"\n",
    "    # Load model based on path\n",
    "    if 'rtdetr' in model_path.lower():\n",
    "        model = RTDETR(model_path)\n",
    "    else:\n",
    "        model = YOLO(model_path)\n",
    "    \n",
    "    predictions = []\n",
    "    test_images_dir = Path(test_images_dir)\n",
    "    \n",
    "    for img_data in tqdm(test_index['images'], desc=\"Inference\"):\n",
    "        image_path = test_images_dir / img_data['image_filename']\n",
    "        \n",
    "        if not image_path.exists():\n",
    "            continue\n",
    "        \n",
    "        results = model.predict(\n",
    "            source=str(image_path),\n",
    "            conf=0.01,\n",
    "            imgsz=640,\n",
    "            verbose=False\n",
    "        )[0]\n",
    "        \n",
    "        detections = []\n",
    "        if len(results.boxes) > 0:\n",
    "            for i in range(len(results.boxes)):\n",
    "                detections.append({\n",
    "                    \"class_id\": int(results.boxes.cls[i].item()),\n",
    "                    \"class_name\": results.names[int(results.boxes.cls[i].item())],\n",
    "                    \"confidence\": float(results.boxes.conf[i].item()),\n",
    "                    \"bbox\": results.boxes.xyxy[i].tolist(),\n",
    "                    \"bbox_format\": \"xyxy\"\n",
    "                })\n",
    "        \n",
    "        predictions.append({\n",
    "            \"image_id\": img_data['image_id'],\n",
    "            \"detections\": detections\n",
    "        })\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def is_eval_complete(output_dir: Path, model_name: str, session_name: str, test_type: str) -> bool:\n",
    "    \"\"\"Check if evaluation is already complete.\"\"\"\n",
    "    eval_dir = output_dir / \"evaluations\" / f\"{model_name}__{session_name}__test_{test_type}\"\n",
    "    return (eval_dir / \"metrics.json\").exists()\n",
    "\n",
    "\n",
    "def evaluate_session(\n",
    "    model_name: str,\n",
    "    session_name: str,\n",
    "    weights_path: str,\n",
    "    test_type: str,\n",
    "    output_dir: Path\n",
    ") -> dict:\n",
    "    \"\"\"Evaluate a trained model on a test set.\"\"\"\n",
    "    run_name = f\"{model_name}__{session_name}\"\n",
    "    eval_name = f\"{run_name}__test_{test_type}\"\n",
    "    eval_dir = output_dir / \"evaluations\" / eval_name\n",
    "    eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nEvaluating: {eval_name}\")\n",
    "    \n",
    "    # Select test set\n",
    "    if test_type == 'clean':\n",
    "        test_images_dir = \"data/raw/test/images\"\n",
    "    else:\n",
    "        test_images_dir = f\"data/synthetic_occlusion/{OCCLUSION_LEVEL}/images\"\n",
    "    \n",
    "    # Load test index\n",
    "    with open(\"data/processed/evaluation/test_index.json\") as f:\n",
    "        test_index = json.load(f)\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = generate_predictions(weights_path, test_images_dir, test_index)\n",
    "    \n",
    "    # Save predictions (useful for debugging, but don't backup to Drive)\n",
    "    with open(eval_dir / \"predictions.json\", 'w') as f:\n",
    "        json.dump({\"predictions\": predictions}, f)\n",
    "    \n",
    "    # Load ground truth\n",
    "    gts = load_ground_truth(\"data/processed/evaluation/test_index.json\")\n",
    "    class_names = load_class_names(\"data/processed/evaluation/test_index.json\")\n",
    "    \n",
    "    # Run metrics\n",
    "    threshold_sweep = eval_detection_prf_at_iou(\n",
    "        predictions, gts,\n",
    "        iou_threshold=0.5,\n",
    "        conf_thresholds=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "    )\n",
    "    \n",
    "    best_thr = max(threshold_sweep.keys(), key=lambda k: threshold_sweep[k]['f1'])\n",
    "    best_metrics = threshold_sweep[best_thr]\n",
    "    \n",
    "    per_class = eval_per_class_metrics_and_confusions(\n",
    "        predictions, gts,\n",
    "        conf_threshold=float(best_thr),\n",
    "        class_names=class_names\n",
    "    )\n",
    "    \n",
    "    counting = eval_counting_quality(\n",
    "        predictions, gts,\n",
    "        conf_threshold=float(best_thr),\n",
    "        class_names=class_names\n",
    "    )\n",
    "    \n",
    "    # Generate plots (saved locally, not backed up to Drive)\n",
    "    plot_all_metrics(\n",
    "        threshold_sweep=threshold_sweep,\n",
    "        per_class_results=per_class['per_class'],\n",
    "        confusion_data=per_class,\n",
    "        counting_results=counting,\n",
    "        output_dir=str(eval_dir),\n",
    "        run_name=eval_name\n",
    "    )\n",
    "    \n",
    "    # Save metrics (this IS backed up via symlink)\n",
    "    metrics = {\n",
    "        \"run_name\": eval_name,\n",
    "        \"model\": model_name,\n",
    "        \"session\": session_name,\n",
    "        \"test_type\": test_type,\n",
    "        \"best_threshold\": float(best_thr),\n",
    "        \"precision\": best_metrics['precision'],\n",
    "        \"recall\": best_metrics['recall'],\n",
    "        \"f1\": best_metrics['f1'],\n",
    "        \"tp\": best_metrics['tp'],\n",
    "        \"fp\": best_metrics['fp'],\n",
    "        \"fn\": best_metrics['fn'],\n",
    "        \"count_mae_matched\": counting['matched_only']['global_mae'],\n",
    "        \"count_mae_all\": counting['all_predictions']['global_mae']\n",
    "    }\n",
    "    \n",
    "    with open(eval_dir / \"metrics.json\", 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    print(f\"  Best F1: {best_metrics['f1']:.4f} @ conf={best_thr}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"Evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run All Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluations on both clean and occluded test sets\n",
    "all_metrics = []\n",
    "\n",
    "for model_name in MODELS:\n",
    "    for session_name in SESSIONS_TO_RUN:\n",
    "        # Check if training completed\n",
    "        if not is_session_complete(output_dir, model_name, session_name):\n",
    "            print(f\"SKIP (no training): {model_name}__{session_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Find weights\n",
    "        run_dir = output_dir / f\"{model_name}__{session_name}\"\n",
    "        weights_path = run_dir / \"weights\" / \"best.pt\"\n",
    "        \n",
    "        if not weights_path.exists():\n",
    "            print(f\"WARNING: Weights not found for {model_name}__{session_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Evaluate on both test sets\n",
    "        for test_type in ['clean', 'occluded']:\n",
    "            if is_eval_complete(output_dir, model_name, session_name, test_type):\n",
    "                print(f\"SKIP (already done): {model_name}__{session_name}__test_{test_type}\")\n",
    "                # Load existing metrics\n",
    "                eval_dir = output_dir / \"evaluations\" / f\"{model_name}__{session_name}__test_{test_type}\"\n",
    "                with open(eval_dir / \"metrics.json\") as f:\n",
    "                    metrics = json.load(f)\n",
    "            else:\n",
    "                metrics = evaluate_session(\n",
    "                    model_name, session_name, str(weights_path), test_type, output_dir\n",
    "                )\n",
    "            \n",
    "            all_metrics.append(metrics)\n",
    "\n",
    "# Save all metrics\n",
    "with open(output_dir / \"all_metrics.json\", 'w') as f:\n",
    "    json.dump(all_metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\nAll evaluations complete! Results in {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Summary Table & Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "df = pd.DataFrame(all_metrics)\n",
    "\n",
    "if len(df) > 0:\n",
    "    # Pivot for nice display\n",
    "    summary_clean = df[df['test_type'] == 'clean'][['model', 'session', 'f1', 'precision', 'recall']].copy()\n",
    "    summary_clean = summary_clean.rename(columns={'f1': 'F1_clean', 'precision': 'P_clean', 'recall': 'R_clean'})\n",
    "\n",
    "    summary_occ = df[df['test_type'] == 'occluded'][['model', 'session', 'f1', 'precision', 'recall']].copy()\n",
    "    summary_occ = summary_occ.rename(columns={'f1': 'F1_occ', 'precision': 'P_occ', 'recall': 'R_occ'})\n",
    "\n",
    "    summary = pd.merge(summary_clean, summary_occ, on=['model', 'session'])\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPERIMENT 3 RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(summary.to_string(index=False))\n",
    "\n",
    "    # Save CSV\n",
    "    summary.to_csv(output_dir / \"summary_metrics.csv\", index=False)\n",
    "    print(f\"\\nSaved to {output_dir / 'summary_metrics.csv'}\")\n",
    "else:\n",
    "    print(\"No metrics to summarize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comparison bar plots\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if len(df) > 0:\n",
    "    def plot_comparison(df, metric_col, test_type, title, output_path):\n",
    "        data = df[df['test_type'] == test_type]\n",
    "        sessions = data['session'].unique()\n",
    "        models = data['model'].unique()\n",
    "        \n",
    "        x = np.arange(len(sessions))\n",
    "        width = 0.35\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(14, 6))\n",
    "        \n",
    "        for i, model in enumerate(models):\n",
    "            model_data = data[data['model'] == model]\n",
    "            values = [model_data[model_data['session'] == s][metric_col].values[0] \n",
    "                      if len(model_data[model_data['session'] == s]) > 0 else 0 \n",
    "                      for s in sessions]\n",
    "            offset = width * (i - len(models)/2 + 0.5)\n",
    "            bars = ax.bar(x + offset, values, width, label=model)\n",
    "            \n",
    "            for bar, val in zip(bars, values):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                       f'{val:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        ax.set_xlabel('Session')\n",
    "        ax.set_ylabel(metric_col.upper())\n",
    "        ax.set_title(title)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([s.replace('_', '\\n') for s in sessions], fontsize=9)\n",
    "        ax.legend()\n",
    "        ax.set_ylim(0, 1.0)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"Saved: {output_path}\")\n",
    "\n",
    "    plots_dir = output_dir / \"plots\"\n",
    "    plots_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    plot_comparison(df, 'f1', 'clean', 'Experiment 3: F1 Score on Clean Test Set', \n",
    "                    plots_dir / \"comparison_f1_clean.png\")\n",
    "    plot_comparison(df, 'f1', 'occluded', f'Experiment 3: F1 Score on Occluded Test Set ({OCCLUSION_LEVEL})', \n",
    "                    plots_dir / \"comparison_f1_occluded.png\")\n",
    "else:\n",
    "    print(\"No data to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT 3 COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Models: {MODELS}\")\n",
    "print(f\"  Sessions: {SESSIONS_TO_RUN}\")\n",
    "print(f\"  Masking: p_apply={P_APPLY}, p_channels={P_CHANNELS}\")\n",
    "print(f\"  Seed: {SEED}\")\n",
    "\n",
    "print(f\"\\nOutputs saved to: {output_dir}\")\n",
    "if IN_COLAB:\n",
    "    print(f\"  (Backed up to Drive: {PERSIST_E3_RUNS})\")\n",
    "\n",
    "print(f\"\\nKey files:\")\n",
    "print(f\"  - training_results.json\")\n",
    "print(f\"  - all_metrics.json\")\n",
    "print(f\"  - summary_metrics.csv\")\n",
    "print(f\"  - <model>__<session>/weights/best.pt (trained weights)\")\n",
    "print(f\"  - <model>__<session>/DONE (completion marker)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}