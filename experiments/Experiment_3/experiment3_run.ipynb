{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: Internal Masking vs Occlusion Training\n",
    "\n",
    "**Goal:** Test whether internal channel masking during training can improve robustness to occlusions, as an alternative to training on occluded images.\n",
    "\n",
    "**Sessions (6 per model):**\n",
    "- S1: Clean train (baseline)\n",
    "- S2: Occluded train (standard practice)\n",
    "- S3: Clean train + mask backbone_early\n",
    "- S4: Clean train + mask backbone_late\n",
    "- S5: Clean train + mask neck\n",
    "- S6: Clean train + mask head\n",
    "\n",
    "**Evaluation:**\n",
    "- All models tested on both `test_clean` and `test_occluded` (40%)\n",
    "- Using our custom evaluation system (P/R/F1, per-class, counting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Configuration\n",
    "\n",
    "**EDIT THIS CELL to switch between smoke test and full run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPERIMENT 3 CONFIGURATION - EDIT THIS CELL ONLY\n",
    "# ============================================================\n",
    "\n",
    "# EPOCHS: Set to 1 for smoke test, 50 for full experiment\n",
    "EPOCHS = 1  # <-- CHANGE THIS: 1 = smoke test, 50 = full run\n",
    "\n",
    "# Models to run (both for full experiment, one for quick test)\n",
    "MODELS = [\"yolov8n\", \"rtdetr-l\"]\n",
    "\n",
    "# Sessions to run (all 6 for full, fewer for quick test)\n",
    "SESSIONS_TO_RUN = [\"S1_clean_train\", \"S2_occ_train\", \"S3_mask_backbone_early\", \n",
    "                   \"S4_mask_backbone_late\", \"S5_mask_neck\", \"S6_mask_head\"]\n",
    "\n",
    "# Masking parameters (fixed for all sessions)\n",
    "P_APPLY = 0.5      # Probability of applying masking per batch\n",
    "P_CHANNELS = 0.2   # Fraction of channels to zero when masking\n",
    "\n",
    "# Training parameters\n",
    "IMGSZ = 640\n",
    "BATCH = -1  # Auto batch size\n",
    "PATIENCE = 10  # Early stopping patience\n",
    "\n",
    "# Paths\n",
    "DATASET_ROOT = \"data/raw\"\n",
    "OUTPUT_ROOT = \"runs/exp3\"\n",
    "OCCLUSION_LEVEL = \"level_040\"  # 40% occlusion for test\n",
    "\n",
    "# ============================================================\n",
    "# GOOGLE DRIVE BACKUP SETTINGS\n",
    "# ============================================================\n",
    "ENABLE_DRIVE_BACKUP = True  # Set to False to disable backups\n",
    "DRIVE_BACKUP_FOLDER = \"exp3_backups\"  # Folder name in Google Drive\n",
    "\n",
    "# ============================================================\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  EPOCHS: {EPOCHS} {'(SMOKE TEST)' if EPOCHS <= 1 else '(FULL RUN)'}\")\n",
    "print(f\"  MODELS: {MODELS}\")\n",
    "print(f\"  SESSIONS: {len(SESSIONS_TO_RUN)} sessions\")\n",
    "print(f\"  Masking: p_apply={P_APPLY}, p_channels={P_CHANNELS}\")\n",
    "print(f\"  Drive Backup: {'ENABLED' if ENABLE_DRIVE_BACKUP else 'DISABLED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Colab\n",
    "import sys\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab\")\n",
    "    if not os.path.exists('/content/Deep_Learning_Gil_Alon'):\n",
    "        !git clone https://github.com/gil-attar/Deep_Learning_Project_Gil_Alon.git Deep_Learning_Gil_Alon\n",
    "    os.chdir('/content/Deep_Learning_Gil_Alon')\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    # Navigate to project root if needed\n",
    "    if 'Experiment_3' in os.getcwd():\n",
    "        os.chdir('../..')\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# ============================================================\n",
    "# MOUNT GOOGLE DRIVE FOR BACKUPS\n",
    "# ============================================================\n",
    "DRIVE_MOUNTED = False\n",
    "DRIVE_BACKUP_PATH = None\n",
    "\n",
    "if ENABLE_DRIVE_BACKUP and IN_COLAB:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        DRIVE_BACKUP_PATH = Path(f'/content/drive/MyDrive/{DRIVE_BACKUP_FOLDER}')\n",
    "        DRIVE_BACKUP_PATH.mkdir(parents=True, exist_ok=True)\n",
    "        DRIVE_MOUNTED = True\n",
    "        print(f\"Google Drive mounted. Backups will be saved to: {DRIVE_BACKUP_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Could not mount Google Drive: {e}\")\n",
    "        print(\"Backups will be disabled.\")\n",
    "        DRIVE_MOUNTED = False\n",
    "elif ENABLE_DRIVE_BACKUP and not IN_COLAB:\n",
    "    print(\"Not running in Colab - Drive backup disabled (local files are safe)\")\n",
    "    DRIVE_MOUNTED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q ultralytics roboflow pyyaml pillow numpy matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "import json\n",
    "import shutil\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from ultralytics import YOLO, RTDETR\n",
    "\n",
    "# Import our experiment modules\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "from experiments.Experiment_3.mask_presets import (\n",
    "    get_mask_prefixes, get_session_config, SESSIONS\n",
    ")\n",
    "from experiments.Experiment_3.channel_masking import MaskingManager\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "\n",
    "# ============================================================\n",
    "# GOOGLE DRIVE BACKUP FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def backup_session_to_drive(run_dir: Path, run_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    Backup important training files to Google Drive after each session.\n",
    "    \n",
    "    Backs up:\n",
    "    - weights/best.pt (trained model)\n",
    "    - weights/last.pt (last checkpoint)\n",
    "    - results.csv (training metrics)\n",
    "    - args.yaml (training config)\n",
    "    - DONE marker\n",
    "    - masking_summary.json (if exists)\n",
    "    \n",
    "    Returns True if backup successful, False otherwise.\n",
    "    \"\"\"\n",
    "    if not DRIVE_MOUNTED or DRIVE_BACKUP_PATH is None:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Create session backup folder\n",
    "        backup_dir = DRIVE_BACKUP_PATH / run_name\n",
    "        backup_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Files to backup\n",
    "        files_to_backup = [\n",
    "            (\"weights/best.pt\", True),      # (path, required)\n",
    "            (\"weights/last.pt\", False),\n",
    "            (\"results.csv\", False),\n",
    "            (\"args.yaml\", False),\n",
    "            (\"DONE\", True),\n",
    "            (\"masking_summary.json\", False),\n",
    "        ]\n",
    "        \n",
    "        backed_up = []\n",
    "        for file_rel, required in files_to_backup:\n",
    "            src = run_dir / file_rel\n",
    "            if src.exists():\n",
    "                dst = backup_dir / file_rel\n",
    "                dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "                shutil.copy2(src, dst)\n",
    "                backed_up.append(file_rel)\n",
    "            elif required:\n",
    "                print(f\"  WARNING: Required file not found: {src}\")\n",
    "        \n",
    "        # Save backup timestamp\n",
    "        backup_info = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"run_name\": run_name,\n",
    "            \"source_dir\": str(run_dir),\n",
    "            \"files_backed_up\": backed_up\n",
    "        }\n",
    "        with open(backup_dir / \"backup_info.json\", 'w') as f:\n",
    "            json.dump(backup_info, f, indent=2)\n",
    "        \n",
    "        print(f\"  BACKUP OK: {len(backed_up)} files -> {backup_dir}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  BACKUP FAILED: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def backup_evaluation_to_drive(eval_dir: Path, eval_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    Backup evaluation results to Google Drive.\n",
    "    \n",
    "    Backs up:\n",
    "    - metrics.json\n",
    "    - predictions.json\n",
    "    - All plot images\n",
    "    \"\"\"\n",
    "    if not DRIVE_MOUNTED or DRIVE_BACKUP_PATH is None:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        backup_dir = DRIVE_BACKUP_PATH / \"evaluations\" / eval_name\n",
    "        backup_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        backed_up = []\n",
    "        \n",
    "        # Backup JSON files\n",
    "        for json_file in [\"metrics.json\", \"predictions.json\"]:\n",
    "            src = eval_dir / json_file\n",
    "            if src.exists():\n",
    "                shutil.copy2(src, backup_dir / json_file)\n",
    "                backed_up.append(json_file)\n",
    "        \n",
    "        # Backup plot images\n",
    "        for png_file in eval_dir.glob(\"*.png\"):\n",
    "            shutil.copy2(png_file, backup_dir / png_file.name)\n",
    "            backed_up.append(png_file.name)\n",
    "        \n",
    "        print(f\"  BACKUP OK: {len(backed_up)} files -> {backup_dir}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  BACKUP FAILED: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def restore_from_drive(output_dir: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Check Google Drive for existing backups and restore them.\n",
    "    Use this to continue training after Colab disconnection.\n",
    "    \n",
    "    Returns dict with restored session names.\n",
    "    \"\"\"\n",
    "    if not DRIVE_MOUNTED or DRIVE_BACKUP_PATH is None:\n",
    "        return {\"restored\": []}\n",
    "    \n",
    "    restored = []\n",
    "    \n",
    "    try:\n",
    "        # Find all backed up sessions\n",
    "        for backup_dir in DRIVE_BACKUP_PATH.iterdir():\n",
    "            if not backup_dir.is_dir():\n",
    "                continue\n",
    "            if backup_dir.name == \"evaluations\":\n",
    "                continue\n",
    "                \n",
    "            run_name = backup_dir.name\n",
    "            local_dir = output_dir / run_name\n",
    "            done_marker = local_dir / \"DONE\"\n",
    "            \n",
    "            # Skip if already exists locally\n",
    "            if done_marker.exists():\n",
    "                continue\n",
    "            \n",
    "            # Check if backup has DONE marker (completed training)\n",
    "            backup_done = backup_dir / \"DONE\"\n",
    "            if not backup_done.exists():\n",
    "                continue\n",
    "            \n",
    "            # Restore from backup\n",
    "            print(f\"Restoring from Drive: {run_name}\")\n",
    "            local_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Copy all files\n",
    "            for src_file in backup_dir.rglob(\"*\"):\n",
    "                if src_file.is_file():\n",
    "                    rel_path = src_file.relative_to(backup_dir)\n",
    "                    dst_file = local_dir / rel_path\n",
    "                    dst_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    shutil.copy2(src_file, dst_file)\n",
    "            \n",
    "            restored.append(run_name)\n",
    "            print(f\"  Restored: {run_name}\")\n",
    "        \n",
    "        # Also restore evaluations\n",
    "        eval_backup_dir = DRIVE_BACKUP_PATH / \"evaluations\"\n",
    "        if eval_backup_dir.exists():\n",
    "            for eval_dir in eval_backup_dir.iterdir():\n",
    "                if not eval_dir.is_dir():\n",
    "                    continue\n",
    "                \n",
    "                eval_name = eval_dir.name\n",
    "                local_eval_dir = output_dir / \"evaluations\" / eval_name\n",
    "                \n",
    "                if (local_eval_dir / \"metrics.json\").exists():\n",
    "                    continue\n",
    "                \n",
    "                print(f\"Restoring evaluation: {eval_name}\")\n",
    "                local_eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                for src_file in eval_dir.iterdir():\n",
    "                    if src_file.is_file():\n",
    "                        shutil.copy2(src_file, local_eval_dir / src_file.name)\n",
    "                \n",
    "                restored.append(f\"eval:{eval_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Restore error: {e}\")\n",
    "    \n",
    "    return {\"restored\": restored}\n",
    "\n",
    "\n",
    "print(\"Backup functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset if not exists\n",
    "if not Path(\"data/raw/train/images\").exists():\n",
    "    os.environ[\"ROBOFLOW_API_KEY\"] = \"zEF9icmDY2oTcPkaDcQY\"\n",
    "    !python scripts/download_dataset.py --output_dir data/raw\n",
    "else:\n",
    "    print(\"Dataset already exists\")\n",
    "\n",
    "# Verify\n",
    "print(f\"Train images: {len(list(Path('data/raw/train/images').glob('*')))}\")\n",
    "print(f\"Val images: {len(list(Path('data/raw/valid/images').glob('*')))}\")\n",
    "print(f\"Test images: {len(list(Path('data/raw/test/images').glob('*')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Occluded Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate occluded training data for S2\n",
    "# We need 40% occlusion on training images\n",
    "\n",
    "occluded_train_dir = Path(\"data/occluded_train_040\")\n",
    "\n",
    "if not occluded_train_dir.exists():\n",
    "    print(\"Generating occluded training data (40% occlusion)...\")\n",
    "    \n",
    "    # First build evaluation indices if not exists\n",
    "    if not Path(\"data/processed/evaluation/train_index.json\").exists():\n",
    "        !python scripts/build_evaluation_indices.py \\\n",
    "            --dataset_root data/raw \\\n",
    "            --output_dir data/processed/evaluation\n",
    "    \n",
    "    # Generate occluded training images\n",
    "    !python scripts/generate_synthetic_occlusions.py \\\n",
    "        --test_index data/processed/evaluation/train_index.json \\\n",
    "        --images_dir data/raw/train/images \\\n",
    "        --labels_dir data/raw/train/labels \\\n",
    "        --output_dir data/occluded_train_040 \\\n",
    "        --levels 0.4 \\\n",
    "        --seed 42\n",
    "else:\n",
    "    print(f\"Occluded training data already exists at {occluded_train_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate occluded test data if not exists\n",
    "occluded_test_dir = Path(f\"data/synthetic_occlusion/{OCCLUSION_LEVEL}\")\n",
    "\n",
    "if not occluded_test_dir.exists():\n",
    "    print(f\"Generating occluded test data ({OCCLUSION_LEVEL})...\")\n",
    "    \n",
    "    !python scripts/generate_synthetic_occlusions.py \\\n",
    "        --test_index data/processed/evaluation/test_index.json \\\n",
    "        --images_dir data/raw/test/images \\\n",
    "        --labels_dir data/raw/test/labels \\\n",
    "        --output_dir data/synthetic_occlusion \\\n",
    "        --levels 0.4 \\\n",
    "        --seed 42\n",
    "else:\n",
    "    print(f\"Occluded test data already exists at {occluded_test_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Data YAML Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data.yaml files for clean and occluded training\n",
    "\n",
    "# Load class names from original data.yaml\n",
    "with open('data/raw/data.yaml', 'r') as f:\n",
    "    original_config = yaml.safe_load(f)\n",
    "\n",
    "# Clean training data.yaml\n",
    "clean_config = {\n",
    "    'path': str(Path('data/raw').resolve()),\n",
    "    'train': 'train/images',\n",
    "    'val': 'valid/images',\n",
    "    'test': 'test/images',\n",
    "    'names': original_config['names'],\n",
    "    'nc': len(original_config['names'])\n",
    "}\n",
    "\n",
    "Path('data/processed').mkdir(parents=True, exist_ok=True)\n",
    "with open('data/processed/data_clean.yaml', 'w') as f:\n",
    "    yaml.dump(clean_config, f, default_flow_style=False)\n",
    "print(\"Created data/processed/data_clean.yaml\")\n",
    "\n",
    "# Occluded training data.yaml (for S2)\n",
    "occ_train_config = {\n",
    "    'path': str(Path('data').resolve()),\n",
    "    'train': 'occluded_train_040/level_040/images',  # Occluded training images\n",
    "    'val': 'raw/valid/images',  # Keep validation clean\n",
    "    'test': 'raw/test/images',\n",
    "    'names': original_config['names'],\n",
    "    'nc': len(original_config['names'])\n",
    "}\n",
    "\n",
    "with open('data/processed/data_occ_train.yaml', 'w') as f:\n",
    "    yaml.dump(occ_train_config, f, default_flow_style=False)\n",
    "print(\"Created data/processed/data_occ_train.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build Evaluation Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build evaluation indices if not exists\n",
    "if not Path(\"data/processed/evaluation/test_index.json\").exists():\n",
    "    !python scripts/build_evaluation_indices.py \\\n",
    "        --dataset_root data/raw \\\n",
    "        --output_dir data/processed/evaluation\n",
    "else:\n",
    "    print(\"Evaluation indices already exist\")\n",
    "\n",
    "# Verify\n",
    "with open(\"data/processed/evaluation/test_index.json\") as f:\n",
    "    test_index = json.load(f)\n",
    "print(f\"Test set: {test_index['metadata']['num_images']} images, {test_index['metadata']['total_objects']} objects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name: str):\n",
    "    \"\"\"Load a model by name.\"\"\"\n",
    "    if 'yolo' in model_name.lower():\n",
    "        return YOLO(f\"{model_name}.pt\")\n",
    "    elif 'rtdetr' in model_name.lower():\n",
    "        return RTDETR(f\"{model_name}.pt\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "\n",
    "def get_model_type(model_name: str) -> str:\n",
    "    \"\"\"Get model type for mask presets.\"\"\"\n",
    "    if 'yolo' in model_name.lower():\n",
    "        return 'yolo'\n",
    "    elif 'rtdetr' in model_name.lower():\n",
    "        return 'rtdetr'\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "\n",
    "def train_session(\n",
    "    model_name: str,\n",
    "    session_name: str,\n",
    "    epochs: int,\n",
    "    output_dir: Path,\n",
    "    p_apply: float = 0.5,\n",
    "    p_channels: float = 0.2\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Train a single session.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with training results and paths\n",
    "    \"\"\"\n",
    "    session_config = get_session_config(session_name)\n",
    "    run_name = f\"{model_name}__{session_name}\"\n",
    "    run_dir = output_dir / run_name\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING: {run_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Description: {session_config['description']}\")\n",
    "    print(f\"Epochs: {epochs}\")\n",
    "    \n",
    "    # Check if already completed (local or from Drive backup)\n",
    "    done_marker = run_dir / \"DONE\"\n",
    "    if done_marker.exists():\n",
    "        print(f\"Session already completed. Skipping.\")\n",
    "        return {\"status\": \"skipped\", \"run_dir\": str(run_dir)}\n",
    "    \n",
    "    # Select data.yaml based on session\n",
    "    if session_config['train_data'] == 'occluded':\n",
    "        data_yaml = 'data/processed/data_occ_train.yaml'\n",
    "    else:\n",
    "        data_yaml = 'data/processed/data_clean.yaml'\n",
    "    \n",
    "    print(f\"Data: {data_yaml}\")\n",
    "    \n",
    "    # Load model\n",
    "    model = get_model(model_name)\n",
    "    \n",
    "    # Setup masking if needed\n",
    "    masking_manager = None\n",
    "    if session_config['mask_location'] is not None:\n",
    "        mask_location = session_config['mask_location']\n",
    "        model_type = get_model_type(model_name)\n",
    "        layer_prefixes = get_mask_prefixes(model_type, mask_location)\n",
    "        \n",
    "        print(f\"Masking: {mask_location} -> layers {layer_prefixes}\")\n",
    "        print(f\"Masking params: p_apply={p_apply}, p_channels={p_channels}\")\n",
    "        \n",
    "        # Add masking hooks\n",
    "        masking_manager = MaskingManager(model.model, p_apply, p_channels)\n",
    "        num_hooks = masking_manager.add_masking_to_layers(layer_prefixes)\n",
    "        print(f\"Added {num_hooks} masking hooks\")\n",
    "    else:\n",
    "        print(\"Masking: None\")\n",
    "    \n",
    "    # Train\n",
    "    try:\n",
    "        results = model.train(\n",
    "            data=data_yaml,\n",
    "            epochs=epochs,\n",
    "            imgsz=IMGSZ,\n",
    "            batch=BATCH,\n",
    "            patience=PATIENCE,\n",
    "            save=True,\n",
    "            project=str(output_dir),\n",
    "            name=run_name,\n",
    "            exist_ok=True,\n",
    "            pretrained=True,\n",
    "            optimizer='auto',\n",
    "            verbose=True,\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        # Mark as done\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "        done_marker.touch()\n",
    "        \n",
    "        # Save masking summary if used\n",
    "        if masking_manager:\n",
    "            masking_summary = masking_manager.get_summary()\n",
    "            with open(run_dir / \"masking_summary.json\", 'w') as f:\n",
    "                json.dump(masking_summary, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n Training complete: {run_name}\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # BACKUP TO GOOGLE DRIVE IMMEDIATELY AFTER TRAINING\n",
    "        # ============================================================\n",
    "        if DRIVE_MOUNTED:\n",
    "            print(\"Backing up to Google Drive...\")\n",
    "            backup_session_to_drive(run_dir, run_name)\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"run_dir\": str(run_dir),\n",
    "            \"weights_path\": str(run_dir / \"weights\" / \"best.pt\")\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n Training FAILED: {run_name}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        \n",
    "        # Mark as failed\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "        (run_dir / \"FAILED\").write_text(str(e))\n",
    "        \n",
    "        return {\"status\": \"failed\", \"error\": str(e), \"run_dir\": str(run_dir)}\n",
    "    \n",
    "    finally:\n",
    "        # Clean up masking hooks\n",
    "        if masking_manager:\n",
    "            masking_manager.remove_all_hooks()\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run All Training Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all training sessions\n",
    "output_dir = Path(OUTPUT_ROOT)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# RESTORE FROM GOOGLE DRIVE IF AVAILABLE\n",
    "# ============================================================\n",
    "# This allows continuing after Colab disconnection\n",
    "if DRIVE_MOUNTED:\n",
    "    print(\"Checking Google Drive for previous backups...\")\n",
    "    restore_result = restore_from_drive(output_dir)\n",
    "    if restore_result['restored']:\n",
    "        print(f\"Restored {len(restore_result['restored'])} sessions from Drive:\")\n",
    "        for name in restore_result['restored']:\n",
    "            print(f\"  - {name}\")\n",
    "    else:\n",
    "        print(\"No previous backups found (starting fresh)\")\n",
    "    print()\n",
    "\n",
    "# ============================================================\n",
    "# RUN TRAINING\n",
    "# ============================================================\n",
    "training_results = []\n",
    "\n",
    "total_sessions = len(MODELS) * len(SESSIONS_TO_RUN)\n",
    "current = 0\n",
    "\n",
    "for model_name in MODELS:\n",
    "    for session_name in SESSIONS_TO_RUN:\n",
    "        current += 1\n",
    "        print(f\"\\n[{current}/{total_sessions}] Starting {model_name} - {session_name}\")\n",
    "        \n",
    "        result = train_session(\n",
    "            model_name=model_name,\n",
    "            session_name=session_name,\n",
    "            epochs=EPOCHS,\n",
    "            output_dir=output_dir,\n",
    "            p_apply=P_APPLY,\n",
    "            p_channels=P_CHANNELS\n",
    "        )\n",
    "        \n",
    "        result['model'] = model_name\n",
    "        result['session'] = session_name\n",
    "        training_results.append(result)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for r in training_results:\n",
    "    status_icon = \"\" if r['status'] == 'success' else \"\" if r['status'] == 'skipped' else \"\"\n",
    "    print(f\"{status_icon} {r['model']}__{r['session']}: {r['status']}\")\n",
    "\n",
    "# Save results\n",
    "with open(output_dir / \"training_results.json\", 'w') as f:\n",
    "    json.dump(training_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {output_dir / 'training_results.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation modules\n",
    "from evaluation.io import load_predictions, load_ground_truth, load_class_names\n",
    "from evaluation.metrics import (\n",
    "    eval_detection_prf_at_iou,\n",
    "    eval_per_class_metrics_and_confusions,\n",
    "    eval_counting_quality\n",
    ")\n",
    "from evaluation.plots import plot_all_metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def generate_predictions(model_path: str, test_images_dir: str, test_index: dict) -> list:\n",
    "    \"\"\"\n",
    "    Generate predictions for a trained model on a test set.\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    if 'rtdetr' in model_path.lower():\n",
    "        model = RTDETR(model_path)\n",
    "    else:\n",
    "        model = YOLO(model_path)\n",
    "    \n",
    "    predictions = []\n",
    "    test_images_dir = Path(test_images_dir)\n",
    "    \n",
    "    for img_data in tqdm(test_index['images'], desc=\"Inference\"):\n",
    "        image_path = test_images_dir / img_data['image_filename']\n",
    "        \n",
    "        if not image_path.exists():\n",
    "            continue\n",
    "        \n",
    "        results = model.predict(\n",
    "            source=str(image_path),\n",
    "            conf=0.01,\n",
    "            imgsz=640,\n",
    "            verbose=False\n",
    "        )[0]\n",
    "        \n",
    "        detections = []\n",
    "        if len(results.boxes) > 0:\n",
    "            for i in range(len(results.boxes)):\n",
    "                detections.append({\n",
    "                    \"class_id\": int(results.boxes.cls[i].item()),\n",
    "                    \"class_name\": results.names[int(results.boxes.cls[i].item())],\n",
    "                    \"confidence\": float(results.boxes.conf[i].item()),\n",
    "                    \"bbox\": results.boxes.xyxy[i].tolist(),\n",
    "                    \"bbox_format\": \"xyxy\"\n",
    "                })\n",
    "        \n",
    "        predictions.append({\n",
    "            \"image_id\": img_data['image_id'],\n",
    "            \"detections\": detections\n",
    "        })\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def evaluate_session(\n",
    "    model_name: str,\n",
    "    session_name: str,\n",
    "    weights_path: str,\n",
    "    test_type: str,  # 'clean' or 'occluded'\n",
    "    output_dir: Path\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate a trained model on a test set.\n",
    "    \"\"\"\n",
    "    run_name = f\"{model_name}__{session_name}\"\n",
    "    eval_name = f\"{run_name}__test_{test_type}\"\n",
    "    eval_dir = output_dir / \"evaluations\" / eval_name\n",
    "    eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nEvaluating: {eval_name}\")\n",
    "    \n",
    "    # Select test set\n",
    "    if test_type == 'clean':\n",
    "        test_images_dir = \"data/raw/test/images\"\n",
    "    else:\n",
    "        test_images_dir = f\"data/synthetic_occlusion/{OCCLUSION_LEVEL}/images\"\n",
    "    \n",
    "    # Load test index\n",
    "    with open(\"data/processed/evaluation/test_index.json\") as f:\n",
    "        test_index = json.load(f)\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = generate_predictions(weights_path, test_images_dir, test_index)\n",
    "    \n",
    "    # Save predictions\n",
    "    pred_json = {\n",
    "        \"run_id\": eval_name,\n",
    "        \"split\": \"test\",\n",
    "        \"test_type\": test_type,\n",
    "        \"model_family\": get_model_type(model_name),\n",
    "        \"predictions\": predictions\n",
    "    }\n",
    "    pred_path = eval_dir / \"predictions.json\"\n",
    "    with open(pred_path, 'w') as f:\n",
    "        json.dump(pred_json, f)\n",
    "    \n",
    "    # Load for evaluation\n",
    "    preds = predictions\n",
    "    gts = load_ground_truth(\"data/processed/evaluation/test_index.json\")\n",
    "    class_names = load_class_names(\"data/processed/evaluation/test_index.json\")\n",
    "    \n",
    "    # Run metrics\n",
    "    threshold_sweep = eval_detection_prf_at_iou(\n",
    "        preds, gts,\n",
    "        iou_threshold=0.5,\n",
    "        conf_thresholds=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "    )\n",
    "    \n",
    "    # Find best threshold\n",
    "    best_thr = max(threshold_sweep.keys(), key=lambda k: threshold_sweep[k]['f1'])\n",
    "    best_metrics = threshold_sweep[best_thr]\n",
    "    \n",
    "    per_class = eval_per_class_metrics_and_confusions(\n",
    "        preds, gts,\n",
    "        conf_threshold=float(best_thr),\n",
    "        class_names=class_names\n",
    "    )\n",
    "    \n",
    "    counting = eval_counting_quality(\n",
    "        preds, gts,\n",
    "        conf_threshold=float(best_thr),\n",
    "        class_names=class_names\n",
    "    )\n",
    "    \n",
    "    # Generate plots\n",
    "    plot_all_metrics(\n",
    "        threshold_sweep=threshold_sweep,\n",
    "        per_class_results=per_class['per_class'],\n",
    "        confusion_data=per_class,\n",
    "        counting_results=counting,\n",
    "        output_dir=str(eval_dir),\n",
    "        run_name=eval_name\n",
    "    )\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics = {\n",
    "        \"run_name\": eval_name,\n",
    "        \"model\": model_name,\n",
    "        \"session\": session_name,\n",
    "        \"test_type\": test_type,\n",
    "        \"best_threshold\": float(best_thr),\n",
    "        \"precision\": best_metrics['precision'],\n",
    "        \"recall\": best_metrics['recall'],\n",
    "        \"f1\": best_metrics['f1'],\n",
    "        \"tp\": best_metrics['tp'],\n",
    "        \"fp\": best_metrics['fp'],\n",
    "        \"fn\": best_metrics['fn'],\n",
    "        \"count_mae_matched\": counting['matched_only']['global_mae'],\n",
    "        \"count_mae_all\": counting['all_predictions']['global_mae']\n",
    "    }\n",
    "    \n",
    "    with open(eval_dir / \"metrics.json\", 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    print(f\"  Best F1: {best_metrics['f1']:.4f} @ conf={best_thr}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"Evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run All Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluations on both clean and occluded test sets\n",
    "# With crash recovery: skip already-completed evaluations\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "# First, load any existing metrics from previous runs\n",
    "existing_metrics_file = output_dir / \"all_metrics.json\"\n",
    "if existing_metrics_file.exists():\n",
    "    with open(existing_metrics_file) as f:\n",
    "        existing_data = json.load(f)\n",
    "    print(f\"Found {len(existing_data)} existing evaluation results\")\n",
    "\n",
    "\n",
    "def is_eval_done(model_name, session_name, test_type):\n",
    "    \"\"\"Check if evaluation already completed.\"\"\"\n",
    "    eval_dir = output_dir / \"evaluations\" / f\"{model_name}__{session_name}__test_{test_type}\"\n",
    "    return (eval_dir / \"metrics.json\").exists()\n",
    "\n",
    "\n",
    "def load_existing_eval(model_name, session_name, test_type):\n",
    "    \"\"\"Load existing evaluation metrics.\"\"\"\n",
    "    eval_dir = output_dir / \"evaluations\" / f\"{model_name}__{session_name}__test_{test_type}\"\n",
    "    with open(eval_dir / \"metrics.json\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "# Scan for all completed training sessions (not just from this run)\n",
    "for model_name in MODELS:\n",
    "    for session_name in SESSIONS_TO_RUN:\n",
    "        run_dir = output_dir / f\"{model_name}__{session_name}\"\n",
    "        done_marker = run_dir / \"DONE\"\n",
    "        \n",
    "        if not done_marker.exists():\n",
    "            print(f\"SKIP (no training): {model_name}__{session_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Find weights\n",
    "        weights_candidates = [\n",
    "            run_dir / \"weights\" / \"best.pt\",\n",
    "            output_dir / f\"{model_name}__{session_name}\" / \"weights\" / \"best.pt\"\n",
    "        ]\n",
    "        \n",
    "        weights_path = None\n",
    "        for candidate in weights_candidates:\n",
    "            if candidate.exists():\n",
    "                weights_path = str(candidate)\n",
    "                break\n",
    "        \n",
    "        if not weights_path:\n",
    "            found = list(output_dir.rglob(f\"*{model_name}__{session_name}*/weights/best.pt\"))\n",
    "            if found:\n",
    "                weights_path = str(found[0])\n",
    "        \n",
    "        if not weights_path:\n",
    "            print(f\"WARNING: Weights not found for {model_name}__{session_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Evaluate on both test sets\n",
    "        for test_type in ['clean', 'occluded']:\n",
    "            eval_name = f\"{model_name}__{session_name}__test_{test_type}\"\n",
    "            \n",
    "            if is_eval_done(model_name, session_name, test_type):\n",
    "                print(f\"SKIP (already done): {eval_name}\")\n",
    "                metrics = load_existing_eval(model_name, session_name, test_type)\n",
    "                all_metrics.append(metrics)\n",
    "            else:\n",
    "                metrics = evaluate_session(\n",
    "                    model_name, session_name, weights_path, test_type, output_dir\n",
    "                )\n",
    "                all_metrics.append(metrics)\n",
    "                \n",
    "                # Backup evaluation to Google Drive\n",
    "                if DRIVE_MOUNTED:\n",
    "                    eval_dir = output_dir / \"evaluations\" / eval_name\n",
    "                    backup_evaluation_to_drive(eval_dir, eval_name)\n",
    "                \n",
    "                # Save after each evaluation for crash recovery\n",
    "                with open(output_dir / \"all_metrics.json\", 'w') as f:\n",
    "                    json.dump(all_metrics, f, indent=2)\n",
    "\n",
    "# Final save\n",
    "with open(output_dir / \"all_metrics.json\", 'w') as f:\n",
    "    json.dump(all_metrics, f, indent=2)\n",
    "\n",
    "# Backup final summary to Drive\n",
    "if DRIVE_MOUNTED:\n",
    "    try:\n",
    "        shutil.copy2(output_dir / \"all_metrics.json\", DRIVE_BACKUP_PATH / \"all_metrics.json\")\n",
    "        print(\"Backed up all_metrics.json to Google Drive\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not backup all_metrics.json: {e}\")\n",
    "\n",
    "print(f\"\\n All evaluations complete! Results in {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Summary Table & Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "df = pd.DataFrame(all_metrics)\n",
    "\n",
    "# Pivot for nice display\n",
    "summary_clean = df[df['test_type'] == 'clean'][['model', 'session', 'f1', 'precision', 'recall']]\n",
    "summary_clean = summary_clean.rename(columns={'f1': 'F1_clean', 'precision': 'P_clean', 'recall': 'R_clean'})\n",
    "\n",
    "summary_occ = df[df['test_type'] == 'occluded'][['model', 'session', 'f1', 'precision', 'recall']]\n",
    "summary_occ = summary_occ.rename(columns={'f1': 'F1_occ', 'precision': 'P_occ', 'recall': 'R_occ'})\n",
    "\n",
    "# Merge\n",
    "summary = pd.merge(summary_clean, summary_occ, on=['model', 'session'])\n",
    "\n",
    "# Display\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT 3 RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "# Save CSV\n",
    "summary.to_csv(output_dir / \"summary_metrics.csv\", index=False)\n",
    "print(f\"\\nSaved to {output_dir / 'summary_metrics.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comparison bar plots\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_comparison(df, metric_col, test_type, title, output_path):\n",
    "    \"\"\"Create grouped bar chart comparing models across sessions.\"\"\"\n",
    "    data = df[df['test_type'] == test_type]\n",
    "    \n",
    "    sessions = data['session'].unique()\n",
    "    models = data['model'].unique()\n",
    "    \n",
    "    x = np.arange(len(sessions))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        model_data = data[data['model'] == model]\n",
    "        values = [model_data[model_data['session'] == s][metric_col].values[0] \n",
    "                  if len(model_data[model_data['session'] == s]) > 0 else 0 \n",
    "                  for s in sessions]\n",
    "        offset = width * (i - len(models)/2 + 0.5)\n",
    "        bars = ax.bar(x + offset, values, width, label=model)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, val in zip(bars, values):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                   f'{val:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    ax.set_xlabel('Session')\n",
    "    ax.set_ylabel(metric_col.upper())\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([s.replace('_', '\\n') for s in sessions], fontsize=9)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1.0)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {output_path}\")\n",
    "\n",
    "# Create plots directory\n",
    "plots_dir = output_dir / \"plots\"\n",
    "plots_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# F1 on Clean Test Set\n",
    "plot_comparison(\n",
    "    df, 'f1', 'clean',\n",
    "    'Experiment 3: F1 Score on Clean Test Set',\n",
    "    plots_dir / \"comparison_f1_clean.png\"\n",
    ")\n",
    "\n",
    "# F1 on Occluded Test Set\n",
    "plot_comparison(\n",
    "    df, 'f1', 'occluded',\n",
    "    f'Experiment 3: F1 Score on Occluded Test Set ({OCCLUSION_LEVEL})',\n",
    "    plots_dir / \"comparison_f1_occluded.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT 3 COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Models: {MODELS}\")\n",
    "print(f\"  Sessions: {SESSIONS_TO_RUN}\")\n",
    "print(f\"  Masking: p_apply={P_APPLY}, p_channels={P_CHANNELS}\")\n",
    "\n",
    "print(f\"\\nOutputs saved to: {output_dir}\")\n",
    "print(f\"  - training_results.json\")\n",
    "print(f\"  - all_metrics.json\")\n",
    "print(f\"  - summary_metrics.csv\")\n",
    "print(f\"  - plots/comparison_f1_clean.png\")\n",
    "print(f\"  - plots/comparison_f1_occluded.png\")\n",
    "print(f\"  - evaluations/*/metrics.json (per-session details)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
